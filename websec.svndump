SVN-fs-dump-format-version: 2

UUID: 45cc574e-f6bb-0310-b799-ead349d68e0b

Revision-number: 0
Prop-content-length: 56
Content-length: 56

K 8
svn:date
V 27
2003-04-23T09:51:20.867475Z
PROPS-END

Revision-number: 1
Prop-content-length: 144
Content-length: 144

K 7
svn:log
V 43
Create initial structure of svn repository

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-04-23T09:53:49.939573Z
PROPS-END

Node-path: branches
Node-kind: dir
Node-action: add
Prop-content-length: 10
Content-length: 10

PROPS-END


Node-path: branches/websec
Node-kind: dir
Node-action: add
Prop-content-length: 10
Content-length: 10

PROPS-END


Node-path: tags
Node-kind: dir
Node-action: add
Prop-content-length: 10
Content-length: 10

PROPS-END


Node-path: tags/websec
Node-kind: dir
Node-action: add
Prop-content-length: 10
Content-length: 10

PROPS-END


Node-path: trunk
Node-kind: dir
Node-action: add
Prop-content-length: 10
Content-length: 10

PROPS-END


Node-path: trunk/websec
Node-kind: dir
Node-action: add
Prop-content-length: 10
Content-length: 10

PROPS-END


Revision-number: 2
Prop-content-length: 138
Content-length: 138

K 7
svn:log
V 37
Import version 1.3.4 into repository

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-04-23T09:55:25.496441Z
PROPS-END

Node-path: trunk/websec/COPYING
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 18007
Text-content-md5: 0636e73ff0215e8d672dc4c32c317bb3
Content-length: 18017

PROPS-END
		    GNU GENERAL PUBLIC LICENSE
		       Version 2, June 1991

 Copyright (C) 1989, 1991 Free Software Foundation, Inc.
                       59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

			    Preamble

  The licenses for most software are designed to take away your
freedom to share and change it.  By contrast, the GNU General Public
License is intended to guarantee your freedom to share and change free
software--to make sure the software is free for all its users.  This
General Public License applies to most of the Free Software
Foundation's software and to any other program whose authors commit to
using it.  (Some other Free Software Foundation software is covered by
the GNU Library General Public License instead.)  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
this service if you wish), that you receive source code or can get it
if you want it, that you can change the software or use pieces of it
in new free programs; and that you know you can do these things.

  To protect your rights, we need to make restrictions that forbid
anyone to deny you these rights or to ask you to surrender the rights.
These restrictions translate to certain responsibilities for you if you
distribute copies of the software, or if you modify it.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must give the recipients all the rights that
you have.  You must make sure that they, too, receive or can get the
source code.  And you must show them these terms so they know their
rights.

  We protect your rights with two steps: (1) copyright the software, and
(2) offer you this license which gives you legal permission to copy,
distribute and/or modify the software.

  Also, for each author's protection and ours, we want to make certain
that everyone understands that there is no warranty for this free
software.  If the software is modified by someone else and passed on, we
want its recipients to know that what they have is not the original, so
that any problems introduced by others will not reflect on the original
authors' reputations.

  Finally, any free program is threatened constantly by software
patents.  We wish to avoid the danger that redistributors of a free
program will individually obtain patent licenses, in effect making the
program proprietary.  To prevent this, we have made it clear that any
patent must be licensed for everyone's free use or not licensed at all.

  The precise terms and conditions for copying, distribution and
modification follow.

		    GNU GENERAL PUBLIC LICENSE
   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION

  0. This License applies to any program or other work which contains
a notice placed by the copyright holder saying it may be distributed
under the terms of this General Public License.  The "Program", below,
refers to any such program or work, and a "work based on the Program"
means either the Program or any derivative work under copyright law:
that is to say, a work containing the Program or a portion of it,
either verbatim or with modifications and/or translated into another
language.  (Hereinafter, translation is included without limitation in
the term "modification".)  Each licensee is addressed as "you".

Activities other than copying, distribution and modification are not
covered by this License; they are outside its scope.  The act of
running the Program is not restricted, and the output from the Program
is covered only if its contents constitute a work based on the
Program (independent of having been made by running the Program).
Whether that is true depends on what the Program does.

  1. You may copy and distribute verbatim copies of the Program's
source code as you receive it, in any medium, provided that you
conspicuously and appropriately publish on each copy an appropriate
copyright notice and disclaimer of warranty; keep intact all the
notices that refer to this License and to the absence of any warranty;
and give any other recipients of the Program a copy of this License
along with the Program.

You may charge a fee for the physical act of transferring a copy, and
you may at your option offer warranty protection in exchange for a fee.

  2. You may modify your copy or copies of the Program or any portion
of it, thus forming a work based on the Program, and copy and
distribute such modifications or work under the terms of Section 1
above, provided that you also meet all of these conditions:

    a) You must cause the modified files to carry prominent notices
    stating that you changed the files and the date of any change.

    b) You must cause any work that you distribute or publish, that in
    whole or in part contains or is derived from the Program or any
    part thereof, to be licensed as a whole at no charge to all third
    parties under the terms of this License.

    c) If the modified program normally reads commands interactively
    when run, you must cause it, when started running for such
    interactive use in the most ordinary way, to print or display an
    announcement including an appropriate copyright notice and a
    notice that there is no warranty (or else, saying that you provide
    a warranty) and that users may redistribute the program under
    these conditions, and telling the user how to view a copy of this
    License.  (Exception: if the Program itself is interactive but
    does not normally print such an announcement, your work based on
    the Program is not required to print an announcement.)

These requirements apply to the modified work as a whole.  If
identifiable sections of that work are not derived from the Program,
and can be reasonably considered independent and separate works in
themselves, then this License, and its terms, do not apply to those
sections when you distribute them as separate works.  But when you
distribute the same sections as part of a whole which is a work based
on the Program, the distribution of the whole must be on the terms of
this License, whose permissions for other licensees extend to the
entire whole, and thus to each and every part regardless of who wrote it.

Thus, it is not the intent of this section to claim rights or contest
your rights to work written entirely by you; rather, the intent is to
exercise the right to control the distribution of derivative or
collective works based on the Program.

In addition, mere aggregation of another work not based on the Program
with the Program (or with a work based on the Program) on a volume of
a storage or distribution medium does not bring the other work under
the scope of this License.

  3. You may copy and distribute the Program (or a work based on it,
under Section 2) in object code or executable form under the terms of
Sections 1 and 2 above provided that you also do one of the following:

    a) Accompany it with the complete corresponding machine-readable
    source code, which must be distributed under the terms of Sections
    1 and 2 above on a medium customarily used for software interchange; or,

    b) Accompany it with a written offer, valid for at least three
    years, to give any third party, for a charge no more than your
    cost of physically performing source distribution, a complete
    machine-readable copy of the corresponding source code, to be
    distributed under the terms of Sections 1 and 2 above on a medium
    customarily used for software interchange; or,

    c) Accompany it with the information you received as to the offer
    to distribute corresponding source code.  (This alternative is
    allowed only for noncommercial distribution and only if you
    received the program in object code or executable form with such
    an offer, in accord with Subsection b above.)

The source code for a work means the preferred form of the work for
making modifications to it.  For an executable work, complete source
code means all the source code for all modules it contains, plus any
associated interface definition files, plus the scripts used to
control compilation and installation of the executable.  However, as a
special exception, the source code distributed need not include
anything that is normally distributed (in either source or binary
form) with the major components (compiler, kernel, and so on) of the
operating system on which the executable runs, unless that component
itself accompanies the executable.

If distribution of executable or object code is made by offering
access to copy from a designated place, then offering equivalent
access to copy the source code from the same place counts as
distribution of the source code, even though third parties are not
compelled to copy the source along with the object code.

  4. You may not copy, modify, sublicense, or distribute the Program
except as expressly provided under this License.  Any attempt
otherwise to copy, modify, sublicense or distribute the Program is
void, and will automatically terminate your rights under this License.
However, parties who have received copies, or rights, from you under
this License will not have their licenses terminated so long as such
parties remain in full compliance.

  5. You are not required to accept this License, since you have not
signed it.  However, nothing else grants you permission to modify or
distribute the Program or its derivative works.  These actions are
prohibited by law if you do not accept this License.  Therefore, by
modifying or distributing the Program (or any work based on the
Program), you indicate your acceptance of this License to do so, and
all its terms and conditions for copying, distributing or modifying
the Program or works based on it.

  6. Each time you redistribute the Program (or any work based on the
Program), the recipient automatically receives a license from the
original licensor to copy, distribute or modify the Program subject to
these terms and conditions.  You may not impose any further
restrictions on the recipients' exercise of the rights granted herein.
You are not responsible for enforcing compliance by third parties to
this License.

  7. If, as a consequence of a court judgment or allegation of patent
infringement or for any other reason (not limited to patent issues),
conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot
distribute so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you
may not distribute the Program at all.  For example, if a patent
license would not permit royalty-free redistribution of the Program by
all those who receive copies directly or indirectly through you, then
the only way you could satisfy both it and this License would be to
refrain entirely from distribution of the Program.

If any portion of this section is held invalid or unenforceable under
any particular circumstance, the balance of the section is intended to
apply and the section as a whole is intended to apply in other
circumstances.

It is not the purpose of this section to induce you to infringe any
patents or other property right claims or to contest validity of any
such claims; this section has the sole purpose of protecting the
integrity of the free software distribution system, which is
implemented by public license practices.  Many people have made
generous contributions to the wide range of software distributed
through that system in reliance on consistent application of that
system; it is up to the author/donor to decide if he or she is willing
to distribute software through any other system and a licensee cannot
impose that choice.

This section is intended to make thoroughly clear what is believed to
be a consequence of the rest of this License.

  8. If the distribution and/or use of the Program is restricted in
certain countries either by patents or by copyrighted interfaces, the
original copyright holder who places the Program under this License
may add an explicit geographical distribution limitation excluding
those countries, so that distribution is permitted only in or among
countries not thus excluded.  In such case, this License incorporates
the limitation as if written in the body of this License.

  9. The Free Software Foundation may publish revised and/or new versions
of the General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

Each version is given a distinguishing version number.  If the Program
specifies a version number of this License which applies to it and "any
later version", you have the option of following the terms and conditions
either of that version or of any later version published by the Free
Software Foundation.  If the Program does not specify a version number of
this License, you may choose any version ever published by the Free Software
Foundation.

  10. If you wish to incorporate parts of the Program into other free
programs whose distribution conditions are different, write to the author
to ask for permission.  For software which is copyrighted by the Free
Software Foundation, write to the Free Software Foundation; we sometimes
make exceptions for this.  Our decision will be guided by the two goals
of preserving the free status of all derivatives of our free software and
of promoting the sharing and reuse of software generally.

			    NO WARRANTY

  11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY
FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN
OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES
PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED
OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS
TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE
PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,
REPAIR OR CORRECTION.

  12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR
REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,
INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING
OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED
TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY
YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER
PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE
POSSIBILITY OF SUCH DAMAGES.

		     END OF TERMS AND CONDITIONS

	    How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
convey the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) 19yy  <name of author>

    This program is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 2 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program; if not, write to the Free Software
    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA


Also add information on how to contact you by electronic and paper mail.

If the program is interactive, make it output a short notice like this
when it starts in an interactive mode:

    Gnomovision version 69, Copyright (C) 19yy name of author
    Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, the commands you use may
be called something other than `show w' and `show c'; they could even be
mouse-clicks or menu items--whatever suits your program.

You should also get your employer (if you work as a programmer) or your
school, if any, to sign a "copyright disclaimer" for the program, if
necessary.  Here is a sample; alter the names:

  Yoyodyne, Inc., hereby disclaims all copyright interest in the program
  `Gnomovision' (which makes passes at compilers) written by James Hacker.

  <signature of Ty Coon>, 1 April 1989
  Ty Coon, President of Vice

This General Public License does not permit incorporating your program into
proprietary programs.  If your program is a subroutine library, you may
consider it more useful to permit linking proprietary applications with the
library.  If this is what you want to do, use the GNU Library General
Public License instead of this License.


Node-path: trunk/websec/README
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 16357
Text-content-md5: 9a73326e6f891b222040057812a05a38
Content-length: 16367

PROPS-END
WEB SECRETARY Version 1.3.4


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec <URL list>'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://homemade.hypermart.net/websec/
                           or:   http://homemade.virtualave.net/websec/


5. URL LIST

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


6. IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


7. IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


8. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


9. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


10. HISTORY

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


11. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Node-path: trunk/websec/ignore.list
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 835
Text-content-md5: 4905b1346598fba8d95ac8c6bf7f9848
Content-length: 845

PROPS-END
[General]
all rights reserved
an error occurred
click here
comments
copyright
daily articles for
details
discussion forum
downloads
in issues
last modified
last updated
maintained
posted
posted at
previous cartoon
search by
special offer
the current week
total votes
visits
votes

[Date_Time]
January \d+
February \d+
March \d+
April \d+
May \d+
June \d+
July \d+
August \d+
September \d+
October \d+
November \d+
December \d+
Jan \d+
Feb \d+
Mar \d+
Apr \d+
Jun \d+
Jul \d+
Aug \d+
Sep \d+
Oct \d+
Nov \d+
Dec \d+
\d+ January
\d+ February
\d+ March
\d+ April
\d+ May
\d+ June
\d+ July
\d+ August
\d+ September
\d+ October
\d+ November
\d+ December
\d+ Jan
\d+ Feb
\d+ Mar
\d+ Apr
\d+ Jun
\d+ Jul
\d+ Aug
\d+ Sep
\d+ Oct
\d+ Nov
\d+ Dec
\d+\/\d+\/\d+

[Adverts]
http://www.news.com/cgi-bin/acc_clickthru
http://ads2.zdnet.com/adverts/


Node-path: trunk/websec/rollback
Node-kind: file
Node-action: add
Prop-content-length: 36
Text-content-length: 363
Text-content-md5: bd86690187200f910f88332f114c67be
Content-length: 399

K 14
svn:executable
V 1
*
PROPS-END
################################################################
# Rollback all files in archive/ directory by one day.
################################################################
 
foreach oldfile (archive/*.old.html)
    set tmpfile = $oldfile:r
    set newfile = $tmpfile:r.html
    echo "Processing $oldfile => $newfile ..."
    cp $oldfile $newfile
end


Node-path: trunk/websec/url.list
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 3321
Text-content-md5: a38e73c108b63e06dd0c3fd3e43c8c2a
Content-length: 3331

PROPS-END
#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General.Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = your_email@domain.net
EmailLink = your_email@domain.net

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sciam.com/currentissue.html
Name = Scientic American
Prefix = sci-american

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://segfault.org/
Name = Segfault.org
Prefix = segfault
Hicolor = grey

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://www.linuxtoday.com/opinions/
Name = Linux Today
Prefix = linux-today

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news


Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: add
Prop-content-length: 36
Text-content-length: 12733
Text-content-md5: b978036290e76dc13c8d85f6cba97ba1
Content-length: 12769

K 14
svn:executable
V 1
*
PROPS-END
#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.3.4
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

# Print help if no arguments passed
if ($#ARGV <= 0)
{
    print "Webdiff Ver 1.3.4\n";
    print "By Chew Wei Yih Copyleft (c) 1998\n\n";
    print "Options:\n";
    print "  -archive   <pathname>  Archive HTML file\n";
    print "  -current   <pathname>  Current HTML file\n";
    print "  -out       <pathname>  Output HTML file (with highlighting)\n";
    print "  -hicolor   <color>     Highlight color (Def: blue, yellow, pink, grey or #rrggbb)\n";
    print "  -ignore    <filelist>  Comma-delimited list of named sections containing ignore keywords\n";
    print "  -ignoreurl <filelist>  Comma-delimited list of named sections containing ignore urls\n";
    print "  -tmin      <number>    Don't check if token contains <= given no. of words\n";
    print "  -tmax      <number>    Don't ignore if token contains >= given no. of words\n";
    print "  -debug     <boolean>   Set to 'true' to output debug messages\n\n";
    exit -1;
}

# Parse arguments to get option values
%optionList =
(
    -hicolor    => "blue",
    -ignore     => "none",
    -ignoreurl  => "none",
    -tmin       => 0,
    -tmax       => 99999,
    -debug      => "false",
);
ParseArguments(@ARGV);

# Store option values in easy-to-access variables
$oldpage    = $optionList{-archive};
$curpage    = $optionList{-current};
$outpage    = $optionList{-out};
$hicolor    = $optionList{-hicolor};
$ignore     = $optionList{-ignore};
$ignoreurl  = $optionList{-ignoreurl};
$tmin       = $optionList{-tmin};
$tmax       = $optionList{-tmax};
$debug      = $optionList{-debug};
$ignoreFile = "ignore.list";

# Get base directory
($basedir = $0) =~ s:[^/]+$::;

# Choose highlighting color
%colorList = ( yellow => "#ffff99", blue => "#66ccff", pink => "#ffcccc", grey => "#4c4c4c" );
if (defined $colorList{$hicolor}) { $hicolor = $colorList{$hicolor}; }
if ($hicolor eq "") { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags = ( "CODE", "B", "I", "U", "TT", "EM", "FONT*", "SUP", "SUB", "SMALL", "STRIKE", "STRONG", "CAPTION*", "A*" );

# Read ignore keywords
if ($ignore ne "none")
{
    $ignore = "," . $ignore . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir$ignoreFile") or die "Cannot open $basedir$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignore =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignore = split/[\r\n]/, $ignorelist;
}
if ($debug eq "true") { foreach (@ignore) { print "Ignore: $_\n"; } }
close(IGNORE);

# Read ignore urls
if ($ignoreurl ne "none")
{
    $ignoreurl = "," . $ignoreurl . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir$ignoreFile") or die "Cannot open $basedir$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignoreurl =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignoreurl = split/[\r\n]/, $ignorelist;
}
if ($debug eq "true") { foreach (@ignoreurl) { print "IgnoreURL: $_\n"; } }

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open(OLDPAGE, "< $oldpage") or die "Cannot open $oldpage: $!\n";
open(CURPAGE, "< $curpage") or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;                                                # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;                                    # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;     # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags)
{
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ($tag =~ s/\*/ /)
    {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
TokenizePage($oldpage); @oldtokens = @tokens; $#tokens = -1;
if ($debug eq "true") { foreach (@oldtokens) { print ">>>> $_\n"; } }
TokenizePage($newpage); @newtokens = @tokens; $#tokens = -1;

# Parse new page
PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens)
{
    $token =~ s:\@\@\@\@&nbsp;~~~~:&nbsp;:sig;
    foreach $tag (@tags) { $token =~ s:~~~~(/*.*?)\@\@\@\@:<$1>:sig; }
}

# Open output file for writing
open(OUTPAGE, "> $outpage") or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if (!$changeStatus)
{
    if ($debug eq "true") { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Parse arguments
sub ParseArguments()
{
    while(1)
    {
        if (!defined($option = shift)) { last; }
        if ($option ne "-archive" && $option ne "-current" && $option ne "-out" &&
            $option ne "-hicolor" && $option ne "-ignore" && $option ne "-debug" &&
            $option ne "-tmin" && $option ne "-tmax" && $option ne "-ignoreurl")
        {
            print "Unrecognized option: $option.\n";
            exit -1;
        }

        if (!defined($value = shift))
        {
            print "No value supplied for option: $option.\n";
            exit -1;
        }
        $optionList{$option} = $value;
    }

    # Make sure some essential option values are supplied
    if ($optionList{-archive} eq "")
    {
        print "You did not supply the archive HTML file via the -archive option.\n";
        exit -1;
    }
    if ($optionList{-current} eq "")
    {
        print "You did not supply the current HTML file via the -current option.\n";
        exit -1;
    }
    if ($optionList{-out} eq "")
    {
        print "You did not supply the output HTML file via the -out option.\n";
        exit -1;
    }
}

# Convert page to tokens
sub TokenizePage()
{
    my $page = shift(@_);
    @tokens = split/(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff()
{
    my $commentOn = 0;
    my $scriptOn = 0;
    my $styleOn = 0;
    my $titleOn = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens)
    {
        if ($token eq "") { next; }
        if ($debug eq "true") { print "<<<< $token\n"; }

        if ($token =~ m|^.*?<!-.*?$|) { $commentOn = 1; }
        if ($token =~ m|^.*?->.*?|) { $commentOn = 0; next; }

        if ($token =~ m|^.*?<TITLE.*?>$|i) { $titleOn = 1; }
        if ($token =~ m|^.*?</TITLE.*?>$|i) { $titleOn = 0; next; }

        if ($token =~ m|^.*?<SCRIPT.*?>$|i) { $scriptOn = 1; }
        if ($token =~ m|^.*?</SCRIPT.*?>$|i) { $scriptOn = 0; next; }

        if ($token =~ m|^.*?<STYLE.*?>$|i) { $styleOn = 1; }
        if ($token =~ m|^.*?</STYLE.*?>$|i) { $styleOn = 0; next; }

        if (TokenContainsIgnoreURL($token)) { $ignoreUrlOn = 1; }
        if ($ignoreUrlOn && TokenContainsHlinkEnd($token)) { $ignoreUrlOn = 0; next; }

        if ($commentOn)
        {
            if ($debug eq "true") { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn)
        {
            if ($debug eq "true") { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn)
        {
            if ($debug eq "true") { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn)
        {
            if ($debug eq "true") { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn)
        {
            if ($debug eq "true") { print "#### Token contains ignore URL - $lastIgnoreURL\n"; }
        }
        elsif ($token =~ m/<.*?>/sig)
        {
            if ($debug eq "true") { print "#### Token is a HTML tag.\n"; }
        }
        elsif (TokenIsMangledHTMLTag($token))
        {
            if ($debug eq "true") { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif (TokenContainsIgnoreKeyword($token))
        {
            if ($debug eq "true") { print "#### Token contains ignore keyword - $lastIgnoreKeyword\n"; }
        }
        elsif (TokenExists($token))
        {
            if ($debug eq "true") { print "#### Token exists in old page.\n"; }
        }
        else
        {
            if ($debug eq "true" ) { print "#### Token has been highlighted!\n"; }
            $token = "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>" .
                $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag()
{
    my $token = shift(@_);

    while($token ne "")
    {
        if ($token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i)
        {
            $token = $2;
            if (!$1 =~ m|^\s*$|) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split/\s/, $tokdup;
    if ($debug eq "true") { print "#### C".($#words+1).": $tokdup\n"; }
    if ($#words+1 > $tmax) { return 0; }

    foreach $keyword (@ignore)
    {
        if ($token  =~ m:^.*?(\b$keyword\b).*?$:i ||
            $tokdup =~ m:^.*?(\b$keyword\b).*?$:i)
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split/\s/, $tokdup;
    if ($#words+1 <= $tmin) { return 1; }

    foreach $oldtok (@oldtokens)
    {
        $oldtok =~ s/\s{2,}/ /sig;
        if ($token eq $oldtok) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl)
    {
        if ($token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i)
        {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}


Node-path: trunk/websec/websec
Node-kind: file
Node-action: add
Prop-content-length: 36
Text-content-length: 9779
Text-content-md5: 0604cea9a5c8e15e22eae2d28b7c4e12
Content-length: 9815

K 14
svn:executable
V 1
*
PROPS-END
#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.3.4
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);

# Print introduction
print "Web Secretary Ver 1.3.4\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
($base = $0) =~ s:[^/]+$::;
$archive = "archive/";
$outgoing = $base . "index.html";
$page_current = $base . "retrieve.html";

# prepare digest
@digest = ();

# Set default values
%defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Tmin      => 0,
    Tmax      => 99999,
    Digest    => "false",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"*(.*?)\"*\s*$/$1/;

    # Check if valid keyword
    if ($keyword ne "URL" && $keyword ne "Auth" && $keyword ne "Name"
	&& $keyword ne "Prefix"
	&& $keyword ne "Diff" && $keyword ne "Hicolor" && $keyword ne "Ignore"
	&& $keyword ne "Email" && $keyword ne "EmailLink"
	&& $keyword ne "Tmin" && $keyword ne "Tmax" && $keyword ne "Proxy"
	&& $keyword ne "IgnoreURL" && $keyword ne "ProxyAuth"
	&& $keyword ne "Digest")
    {
        print qq(Unrecognized keyword in line $.: "$_".\n);
        return -1;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $digest    = $siteinfo{Digest};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $page_previous = $base . $archive . $prefix . ".html";
    $page_archive = $base . $archive . $prefix . ".old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "[websec] $name - $today";
    $webdiff =
        $base .
        "webdiff -archive $page_previous -current $page_current -out $outgoing " .
        "-hicolor $hicolor -ignore $ignore -ignoreurl $ignoreurl -tmin $tmin -tmax $tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent("websec/1.0");
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to 3 times to download URL
    for(1..3)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}


Revision-number: 3
Prop-content-length: 119
Content-length: 119

K 7
svn:log
V 18
Tag version 1.3.4

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-04-23T09:57:07.969469Z
PROPS-END

Node-path: tags/websec/version-1.3.4
Node-kind: dir
Node-action: add
Node-copyfrom-rev: 2
Node-copyfrom-path: trunk/websec


Revision-number: 4
Prop-content-length: 122
Content-length: 122

K 7
svn:log
V 21
Import version 1.4.0

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-04-23T09:59:05.476997Z
PROPS-END

Node-path: trunk/websec/Makefile
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 374
Text-content-md5: bff5b217e10d32662ed95b84abf20fd8
Content-length: 384

PROPS-END
install:
	install websec $(DESTDIR)/usr/bin/websec
	install webdiff $(DESTDIR)/usr/bin/webdiff

	install websec.1 $(DESTDIR)/usr/share/man/man1/
	install webdiff.1 $(DESTDIR)/usr/share/man/man1/

	install -m 0644 url.list $(DESTDIR)/usr/share/doc/websec/examples
	install -m 0644 ignore.list $(DESTDIR)/usr/share/doc/websec/examples
clean:
	@echo '- clean: nothing to do -'


Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 16443
Text-content-md5: 08e3b9cdd8da433edc782c036bceb6f8
Content-length: 16443

WEB SECRETARY Version 1.4.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec <URL list>'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://homemade.hypermart.net/websec/
                           or:   http://homemade.virtualave.net/websec/


5. URL LIST

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


6. IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


7. IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


8. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


9. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


10. HISTORY

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


11. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12743
Text-content-md5: 9d88dbd457e6eeb28ef7a11fb890f3b6
Content-length: 12743

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.4.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

# Print help if no arguments passed
if ($#ARGV <= 0)
{
    print "Webdiff Ver 1.4.0\n";
    print "By Chew Wei Yih Copyleft (c) 1998\n\n";
    print "Options:\n";
    print "  -archive   <pathname>  Archive HTML file\n";
    print "  -current   <pathname>  Current HTML file\n";
    print "  -out       <pathname>  Output HTML file (with highlighting)\n";
    print "  -hicolor   <color>     Highlight color (Def: blue, yellow, pink, grey or #rrggbb)\n";
    print "  -ignore    <filelist>  Comma-delimited list of named sections containing ignore keywords\n";
    print "  -ignoreurl <filelist>  Comma-delimited list of named sections containing ignore urls\n";
    print "  -tmin      <number>    Don't check if token contains <= given no. of words\n";
    print "  -tmax      <number>    Don't ignore if token contains >= given no. of words\n";
    print "  -debug     <boolean>   Set to 'true' to output debug messages\n\n";
    exit -1;
}

# Parse arguments to get option values
%optionList =
(
    -hicolor    => "blue",
    -ignore     => "none",
    -ignoreurl  => "none",
    -tmin       => 0,
    -tmax       => 99999,
    -debug      => "false",
);
&ParseArguments(@ARGV);

# Store option values in easy-to-access variables
$oldpage    = $optionList{-archive};
$curpage    = $optionList{-current};
$outpage    = $optionList{-out};
$hicolor    = $optionList{-hicolor};
$ignore     = $optionList{-ignore};
$ignoreurl  = $optionList{-ignoreurl};
$tmin       = $optionList{-tmin};
$tmax       = $optionList{-tmax};
$debug      = $optionList{-debug};
$ignoreFile = "ignore.list";

# Get base directory
$basedir = $ENV{HOME} . "/.websec/";

# Choose highlighting color
%colorList = ( yellow => "#ffff99", blue => "#66ccff", pink => "#ffcccc", grey => "#4c4c4c" );
if (defined $colorList{$hicolor}) { $hicolor = $colorList{$hicolor}; }
if ($hicolor eq "") { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags = ( "CODE", "B", "I", "U", "TT", "EM", "FONT*", "SUP", "SUB", "SMALL", "STRIKE", "STRONG", "CAPTION*", "A*" );

# Read ignore keywords
if ($ignore ne "none")
{
    $ignore = "," . $ignore . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir$ignoreFile") or die "Cannot open $basedir$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignore =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignore = split/[\r\n]/, $ignorelist;
}
if ($debug eq "true") { foreach (@ignore) { print "Ignore: $_\n"; } }
close(IGNORE);

# Read ignore urls
if ($ignoreurl ne "none")
{
    $ignoreurl = "," . $ignoreurl . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir$ignoreFile") or die "Cannot open $basedir$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignoreurl =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignoreurl = split/[\r\n]/, $ignorelist;
}
if ($debug eq "true") { foreach (@ignoreurl) { print "IgnoreURL: $_\n"; } }

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open(OLDPAGE, "< $oldpage") or die "Cannot open $oldpage: $!\n";
open(CURPAGE, "< $curpage") or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;                                                # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;                                    # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;     # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags)
{
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ($tag =~ s/\*/ /)
    {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage); @oldtokens = @tokens; $#tokens = -1;
if ($debug eq "true") { foreach (@oldtokens) { print ">>>> $_\n"; } }
&TokenizePage($newpage); @newtokens = @tokens; $#tokens = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens)
{
    $token =~ s:\@\@\@\@&nbsp;~~~~:&nbsp;:sig;
    foreach $tag (@tags) { $token =~ s:~~~~(/*.*?)\@\@\@\@:<$1>:sig; }
}

# Open output file for writing
open(OUTPAGE, "> $outpage") or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if (!$changeStatus)
{
    if ($debug eq "true") { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Parse arguments
sub ParseArguments()
{
    while(1)
    {
        if (!defined($option = shift)) { last; }
        if ($option ne "-archive" && $option ne "-current" && $option ne "-out" &&
            $option ne "-hicolor" && $option ne "-ignore" && $option ne "-debug" &&
            $option ne "-tmin" && $option ne "-tmax" && $option ne "-ignoreurl")
        {
            print "Unrecognized option: $option.\n";
            exit -1;
        }

        if (!defined($value = shift))
        {
            print "No value supplied for option: $option.\n";
            exit -1;
        }
        $optionList{$option} = $value;
    }

    # Make sure some essential option values are supplied
    if ($optionList{-archive} eq "")
    {
        print "You did not supply the archive HTML file via the -archive option.\n";
        exit -1;
    }
    if ($optionList{-current} eq "")
    {
        print "You did not supply the current HTML file via the -current option.\n";
        exit -1;
    }
    if ($optionList{-out} eq "")
    {
        print "You did not supply the output HTML file via the -out option.\n";
        exit -1;
    }
}

# Convert page to tokens
sub TokenizePage()
{
    my $page = shift(@_);
    @tokens = split/(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff()
{
    my $commentOn = 0;
    my $scriptOn = 0;
    my $styleOn = 0;
    my $titleOn = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens)
    {
        if ($token eq "") { next; }
        if ($debug eq "true") { print "<<<< $token\n"; }

        if ($token =~ m|^.*?<!-.*?$|) { $commentOn = 1; }
        if ($token =~ m|^.*?->.*?|) { $commentOn = 0; next; }

        if ($token =~ m|^.*?<TITLE.*?>$|i) { $titleOn = 1; }
        if ($token =~ m|^.*?</TITLE.*?>$|i) { $titleOn = 0; next; }

        if ($token =~ m|^.*?<SCRIPT.*?>$|i) { $scriptOn = 1; }
        if ($token =~ m|^.*?</SCRIPT.*?>$|i) { $scriptOn = 0; next; }

        if ($token =~ m|^.*?<STYLE.*?>$|i) { $styleOn = 1; }
        if ($token =~ m|^.*?</STYLE.*?>$|i) { $styleOn = 0; next; }

        if (TokenContainsIgnoreURL($token)) { $ignoreUrlOn = 1; }
        if ($ignoreUrlOn && TokenContainsHlinkEnd($token)) { $ignoreUrlOn = 0; next; }

        if ($commentOn)
        {
            if ($debug eq "true") { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn)
        {
            if ($debug eq "true") { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn)
        {
            if ($debug eq "true") { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn)
        {
            if ($debug eq "true") { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn)
        {
            if ($debug eq "true") { print "#### Token contains ignore URL - $lastIgnoreURL\n"; }
        }
        elsif ($token =~ m/<.*?>/sig)
        {
            if ($debug eq "true") { print "#### Token is a HTML tag.\n"; }
        }
        elsif (TokenIsMangledHTMLTag($token))
        {
            if ($debug eq "true") { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif (TokenContainsIgnoreKeyword($token))
        {
            if ($debug eq "true") { print "#### Token contains ignore keyword - $lastIgnoreKeyword\n"; }
        }
        elsif (TokenExists($token))
        {
            if ($debug eq "true") { print "#### Token exists in old page.\n"; }
        }
        else
        {
            if ($debug eq "true" ) { print "#### Token has been highlighted!\n"; }
            $token = "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>" .
                $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag()
{
    my $token = shift(@_);

    while($token ne "")
    {
        if ($token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i)
        {
            $token = $2;
            if (!$1 =~ m|^\s*$|) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split/\s/, $tokdup;
    if ($debug eq "true") { print "#### C".($#words+1).": $tokdup\n"; }
    if ($#words+1 > $tmax) { return 0; }

    foreach $keyword (@ignore)
    {
        if ($token  =~ m:^.*?(\b$keyword\b).*?$:i ||
            $tokdup =~ m:^.*?(\b$keyword\b).*?$:i)
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split/\s/, $tokdup;
    if ($#words+1 <= $tmin) { return 1; }

    foreach $oldtok (@oldtokens)
    {
        $oldtok =~ s/\s{2,}/ /sig;
        if ($token eq $oldtok) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl)
    {
        if ($token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i)
        {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}


Node-path: trunk/websec/webdiff.1
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 338
Text-content-md5: ea3b9d071d8f6d4241c7dfd8aa062c90
Content-length: 348

PROPS-END
.TH WEBDIFF 1
.SH NAME
webdiff \- web secretary utility
.SH DESCRIPTION
.PP
.B Webdiff 
is part of the websec package. It is used internally by websec.

.SH SEE ALSO
/usr/share/doc/websec/README.

.SH AUTHOR
This manual page was written by Joop Stakenborg <pa3aba@debian.org>,
for the Debian GNU/Linux system (but may be used by others).


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 10582
Text-content-md5: 38b1dbdbbb9a388285124be77b1983f2
Content-length: 10582

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.4.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);

# Print introduction
print "Web Secretary Ver 1.4.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$base= $ENV{HOME} . "/.websec/";
$archive = "archive/";
mkdir $base,0750 if !-d $base;
mkdir $base . $archive,0750 if !-d $base . $archive;
$outgoing = $base . "index.html";
$page_current = $base . "retrieve.html";

# prepare digest
@digest = ();

# Set default values
%defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, "$base/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"*(.*?)\"*\s*$/$1/;

    # Check if valid keyword
    if ($keyword ne "URL" && $keyword ne "Auth" && $keyword ne "Name"
	&& $keyword ne "Prefix" && $keyword ne "Timeout"
	&& $keyword ne "Retry" && $keyword ne "Retrywait"
	&& $keyword ne "Randomwait"
	&& $keyword ne "Diff" && $keyword ne "Hicolor" && $keyword ne "Ignore"
	&& $keyword ne "Email" && $keyword ne "EmailLink"
	&& $keyword ne "Tmin" && $keyword ne "Tmax" && $keyword ne "Proxy"
	&& $keyword ne "IgnoreURL" && $keyword ne "ProxyAuth"
	&& $keyword ne "Digest" && $keyword ne "AddSubject")
    {
        print qq(Unrecognized keyword in line $.: "$_".\n);
        return -1;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $page_previous = $base . $archive . $prefix . ".html";
    $page_archive = $base . $archive . $prefix . ".old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "webdiff -archive $page_previous -current $page_current -out $outgoing " .
        "-hicolor $hicolor -ignore $ignore -ignoreurl $ignoreurl -tmin $tmin -tmax $tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent("websec/1.0");
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}


Node-path: trunk/websec/websec.1
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 1420
Text-content-md5: bbc070d4584d8eeaedd6dce99aef5c8f
Content-length: 1430

PROPS-END
.\" 
.TH "WEBSEC" "1" "1.4.0" "" ""
.SH "NAME"
websec \- web secretary

.SH "SYNOPSIS"
\fBwebsec\fR <$HOME/.websec/url.list>

.SH "DESCRIPTION"
\fBWebsec\fR is a web page monitoring software. 
It will send you a changed web page with the contents
highlighted. The debian version of websec has been
changed, so the web\-pages archive will be stored in
\fI$HOME/.websec\fR, this is also the directory where
you should keep your url.list.


When called without an argument, \fBwebsec\fR will try to
open \fI$HOME/.websec/url.list\fR, but you can also 
provide a filename as the first argument.


You can add a line like \fIAddSubject = [websec]\fR to
url.list, websec will add \fI[websec]\fR to every subject
as a first word when mail is sent. You can then easily detect
this line by a mail filter.


The keywords \fIRetry\fR, \fIRetrywait\fR, and
\fITimeout\fR in url.list lets you specify the number
of times to retry, time to wait between retries, and a
timeout setting.


\fBWebsec\fR waits for a random number of seconds between
retries up to the value specified by the \fIRandomwait\fR
keyword. This is to prevent websec from being blocked by
websites that perform log analysis to find time similarities
between requests.



.SH "SEE ALSO"
/usr/share/doc/websec/README.gz.

.SH "AUTHOR"
This manual page was written by Joop Stakenborg
<pa3aba@debian.org>,
for the Debian GNU/Linux system (but may be used by others).


Revision-number: 5
Prop-content-length: 119
Content-length: 119

K 7
svn:log
V 18
Tag version 1.4.0

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-04-23T09:59:20.279379Z
PROPS-END

Node-path: tags/websec/version-1.4.0
Node-kind: dir
Node-action: add
Node-copyfrom-rev: 4
Node-copyfrom-path: trunk/websec


Revision-number: 6
Prop-content-length: 152
Content-length: 152

K 7
svn:log
V 51
Import utilities I use to update the files section

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-04-23T12:27:27.669033Z
PROPS-END

Node-path: trunk/utils
Node-kind: dir
Node-action: add
Prop-content-length: 10
Content-length: 10

PROPS-END


Node-path: trunk/utils/release
Node-kind: file
Node-action: add
Prop-content-length: 36
Text-content-length: 469
Text-content-md5: 0c69700dd3c6480242cea625084ee65e
Content-length: 505

K 14
svn:executable
V 1
*
PROPS-END
#!/bin/sh

VERSION=$1
FILE=websec-$VERSION.tar.gz
DIR=files/websec.pkg/$VERSION/
BASEDIR=websec-$VERSION/
TAG=version-$VERSION

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

svn export file:///home/svn/websec/tags/websec/$TAG $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	exit 1
fi

tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

svnadmin dump /home/svn/websec | gzip -9 > files/svn/websec.svn-dump.gz


Node-path: trunk/utils/upload
Node-kind: file
Node-action: add
Prop-content-length: 36
Text-content-length: 92
Text-content-md5: c52d63b8d0d60441e7edbc0f40773d0d
Content-length: 128

K 14
svn:executable
V 1
*
PROPS-END
#!/bin/sh

rsync -alvzC --delete -e ssh files/ baruch@freesoftware.fsf.org:/upload/websec/



Revision-number: 7
Prop-content-length: 284
Content-length: 284

K 7
svn:log
V 182
Start using Getopt::Long for parameters.
This changed parameter format for webdiff.
Added --base parameter for websec to specify base dir so that it won't be forced to be ~/.websec/

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-04-26T12:56:58.805422Z
PROPS-END

Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 11761
Text-content-md5: 31e24fa017fd2e6c4b5212b7e6fb176c
Content-length: 11761

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.4.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;

# Print help if no arguments passed
if ($#ARGV <= 0)
{
    print "Webdiff Ver 1.4.0\n";
    print "By Chew Wei Yih Copyleft (c) 1998\n\n";
    print "Options:\n";
    print "  --archive=<pathname>	Archive HTML file\n";
    print "  --current=<pathname>	Current HTML file\n";
    print "  --out=<pathname>		Output HTML file (with highlighting)\n";
    print "  --basedir=<pathname>	Base directory for files\n";
    print "  --hicolor=<color>		Highlight color (Def: blue, yellow, pink, grey or #rrggbb)\n";
    print "  --ignore=<filelist>	Comma-delimited list of named sections containing ignore keywords\n";
    print "  --ignoreurl=<filelist>	Comma-delimited list of named sections containing ignore urls\n";
    print "  --tmin=<number>		Don't check if token contains <= given no. of words\n";
    print "  --tmax=<number>		Don't ignore if token contains >= given no. of words\n";
    print "  --debug                Debug messages\n\n";
    exit -1;
}

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir	= $ENV{HOME} . "/.websec/";

# Parse options
GetOptions(
		"basedir=s"		=> \$basedir,
		"archive=s"		=> \$oldpage,
		"current=s"		=> \$curpage,
		"out=s"			=> \$outpage,
		"hicolor=s"		=> \$hicolor,
		"ignore=s" 		=> \$ignore,
		"ignoreurl=s"	=> \$ignoreurl,
		"tmin=i"		=> \$tmin,
		"tmax=i"		=> \$tmax,
		"debug"			=> \$debug,
		"ignorefile=s"	=> \$ignoreFile);

# Make sure some essential option values are supplied
if ($oldpage eq "")
{
	print "You did not supply the archive HTML file via the --archive option.\n";
	exit -1;
}
if ($curpage eq "")
{
	print "You did not supply the current HTML file via the --current option.\n";
	exit -1;
}
if ($outpage eq "")
{
	print "You did not supply the output HTML file via the --out option.\n";
	exit -1;
}

# Choose highlighting color
%colorList = ( yellow => "#ffff99", blue => "#66ccff", pink => "#ffcccc", grey => "#4c4c4c" );
if (defined $colorList{$hicolor}) { $hicolor = $colorList{$hicolor}; }
if ($hicolor eq "") { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags = ( "CODE", "B", "I", "U", "TT", "EM", "FONT*", "SUP", "SUB", "SMALL", "STRIKE", "STRONG", "CAPTION*", "A*" );

# Read ignore keywords
if ($ignore ne "none")
{
    $ignore = "," . $ignore . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir$ignoreFile") or die "Cannot open $basedir$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignore =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignore = split/[\r\n]/, $ignorelist;
}
if ($debug) { foreach (@ignore) { print "Ignore: $_\n"; } }
close(IGNORE);

# Read ignore urls
if ($ignoreurl ne "none")
{
    $ignoreurl = "," . $ignoreurl . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir$ignoreFile") or die "Cannot open $basedir$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignoreurl =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignoreurl = split/[\r\n]/, $ignorelist;
}
if ($debug) { foreach (@ignoreurl) { print "IgnoreURL: $_\n"; } }

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open(OLDPAGE, "< $oldpage") or die "Cannot open $oldpage: $!\n";
open(CURPAGE, "< $curpage") or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;                                                # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;                                    # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;     # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags)
{
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ($tag =~ s/\*/ /)
    {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage); @oldtokens = @tokens; $#tokens = -1;
if ($debug) { foreach (@oldtokens) { print ">>>> $_\n"; } }
&TokenizePage($newpage); @newtokens = @tokens; $#tokens = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens)
{
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open(OUTPAGE, "> $outpage") or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if (!$changeStatus)
{
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage()
{
    my $page = shift(@_);
    @tokens = split/(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff()
{
    my $commentOn = 0;
    my $scriptOn = 0;
    my $styleOn = 0;
    my $titleOn = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens)
    {
        if ($token eq "") { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ($token =~ m|^.*?<!-.*?$|) { $commentOn = 1; }
        if ($token =~ m|^.*?->.*?|) { $commentOn = 0; next; }

        if ($token =~ m|^.*?<TITLE.*?>$|i) { $titleOn = 1; }
        if ($token =~ m|^.*?</TITLE.*?>$|i) { $titleOn = 0; next; }

        if ($token =~ m|^.*?<SCRIPT.*?>$|i) { $scriptOn = 1; }
        if ($token =~ m|^.*?</SCRIPT.*?>$|i) { $scriptOn = 0; next; }

        if ($token =~ m|^.*?<STYLE.*?>$|i) { $styleOn = 1; }
        if ($token =~ m|^.*?</STYLE.*?>$|i) { $styleOn = 0; next; }

        if (TokenContainsIgnoreURL($token)) { $ignoreUrlOn = 1; }
        if ($ignoreUrlOn && TokenContainsHlinkEnd($token)) { $ignoreUrlOn = 0; next; }

        if ($commentOn)
        {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn)
        {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn)
        {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn)
        {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn)
        {
            if ($debug) { print "#### Token contains ignore URL - $lastIgnoreURL\n"; }
        }
        elsif ($token =~ m/<.*?>/sig)
        {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif (TokenIsMangledHTMLTag($token))
        {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif (TokenContainsIgnoreKeyword($token))
        {
            if ($debug) { print "#### Token contains ignore keyword - $lastIgnoreKeyword\n"; }
        }
        elsif (TokenExists($token))
        {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else
        {
            if ($debug ) { print "#### Token has been highlighted!\n"; }
            $token = "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>" .
                $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag()
{
    my $token = shift(@_);

    while($token ne "")
    {
        if ($token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i)
        {
            $token = $2;
            if (!$1 =~ m|^\s*$|) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C".($#words+1).": $tokdup\n"; }
    if ($#words+1 > $tmax) { return 0; }

    foreach $keyword (@ignore)
    {
        if ($token  =~ m:^.*?(\b$keyword\b).*?$:i ||
            $tokdup =~ m:^.*?(\b$keyword\b).*?$:i)
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split/\s/, $tokdup;
    if ($#words+1 <= $tmin) { return 1; }

    foreach $oldtok (@oldtokens)
    {
        $oldtok =~ s/\s{2,}/ /sig;
        if ($token eq $oldtok) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl)
    {
        if ($token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i)
        {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 10814
Text-content-md5: 2ad3d14713c1bdad36a7726449175749
Content-length: 10814

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.4.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;

# Print introduction
print "Web Secretary Ver 1.4.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
$base= $ENV{HOME} . "/.websec/";

# Parse command line options
$result = GetOptions("base=s"	=> \$base);

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "archive/";
mkdir $base,0750 if !-d $base;
mkdir $base . $archive,0750 if !-d $base . $archive;
$outgoing = $base . "index.html";
$page_current = $base . "retrieve.html";

# prepare digest
@digest = ();

# Set default values
%defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, "$base/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ($keyword ne "URL" && $keyword ne "Auth" && $keyword ne "Name"
	&& $keyword ne "Prefix" && $keyword ne "Timeout"
	&& $keyword ne "Retry" && $keyword ne "Retrywait"
	&& $keyword ne "Randomwait"
	&& $keyword ne "Diff" && $keyword ne "Hicolor" && $keyword ne "Ignore"
	&& $keyword ne "Email" && $keyword ne "EmailLink"
	&& $keyword ne "Tmin" && $keyword ne "Tmax" && $keyword ne "Proxy"
	&& $keyword ne "IgnoreURL" && $keyword ne "ProxyAuth"
	&& $keyword ne "Digest" && $keyword ne "AddSubject")
    {
        print qq(Unrecognized keyword in line $.: "$_".\n);
        return -1;
    }

	# Allow values from the environment
	if ($value =~ m/^\$/)
	{
		$value = $ENV{$value};
	}

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $page_previous = $base . $archive . $prefix . ".html";
    $page_archive = $base . $archive . $prefix . ".old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "webdiff --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing " .
        "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent("websec/1.0");
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}


Node-path: trunk/websec/websec.1
Node-kind: file
Node-action: change
Text-content-length: 1283
Text-content-md5: 1d0609e31333fccac8be52623fc56093
Content-length: 1283

.\" 
.TH "WEBSEC" "1" "1.4.0" "" ""
.SH "NAME"
websec \- web secretary

.SH "SYNOPSIS"
\fBwebsec\fR 

.SH "DESCRIPTION"
\fBWebsec\fR is a web page monitoring software. 
It will send you a changed web page with the contents
highlighted.

When called without an argument, \fBwebsec\fR will try to
open \fI$HOME/.websec/url.list\fR, but you can provide the
--base argument to provide a different directory to read the
config file from and keep the data in.


You can add a line like \fIAddSubject = [websec]\fR to
url.list, websec will add \fI[websec]\fR to every subject
as a first word when mail is sent. You can then easily detect
this line by a mail filter.


The keywords \fIRetry\fR, \fIRetrywait\fR, and
\fITimeout\fR in url.list lets you specify the number
of times to retry, time to wait between retries, and a
timeout setting.


\fBWebsec\fR waits for a random number of seconds between
retries up to the value specified by the \fIRandomwait\fR
keyword. This is to prevent websec from being blocked by
websites that perform log analysis to find time similarities
between requests.



.SH "SEE ALSO"
/usr/share/doc/websec/README.gz.

.SH "AUTHOR"
This manual page was written by Joop Stakenborg
<pa3aba@debian.org>,
for the Debian GNU/Linux system (but may be used by others).


Revision-number: 8
Prop-content-length: 225
Content-length: 225

K 7
svn:log
V 123
Allow environment variables in config file, they should be in the format
${VARNAME}.

For example:
Email = ${USER}@wavelet

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-04-26T13:03:07.043013Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 10819
Text-content-md5: 4435f4a7f980f8cb0936e4c71b666312
Content-length: 10819

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.4.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;

# Print introduction
print "Web Secretary Ver 1.4.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
$base= $ENV{HOME} . "/.websec/";

# Parse command line options
$result = GetOptions("base=s"	=> \$base);

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "archive/";
mkdir $base,0750 if !-d $base;
mkdir $base . $archive,0750 if !-d $base . $archive;
$outgoing = $base . "index.html";
$page_current = $base . "retrieve.html";

# prepare digest
@digest = ();

# Set default values
%defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, "$base/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ($keyword ne "URL" && $keyword ne "Auth" && $keyword ne "Name"
	&& $keyword ne "Prefix" && $keyword ne "Timeout"
	&& $keyword ne "Retry" && $keyword ne "Retrywait"
	&& $keyword ne "Randomwait"
	&& $keyword ne "Diff" && $keyword ne "Hicolor" && $keyword ne "Ignore"
	&& $keyword ne "Email" && $keyword ne "EmailLink"
	&& $keyword ne "Tmin" && $keyword ne "Tmax" && $keyword ne "Proxy"
	&& $keyword ne "IgnoreURL" && $keyword ne "ProxyAuth"
	&& $keyword ne "Digest" && $keyword ne "AddSubject")
    {
        print qq(Unrecognized keyword in line $.: "$_".\n);
        return -1;
    }

	# Allow values from the environment
	if ($value =~ m/^\${([^}]+)}/)
	{
		$value = $ENV{$1};
	}

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $page_previous = $base . $archive . $prefix . ".html";
    $page_archive = $base . $archive . $prefix . ".old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "webdiff --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing " .
        "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent("websec/1.0");
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}


Revision-number: 9
Prop-content-length: 190
Content-length: 190

K 7
svn:log
V 89
Make sure all concatenations of $base add a /, the user might not provide it in his path

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-04-26T13:07:37.707371Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 10825
Text-content-md5: d7942968dc2a97d8004d4c194fc6616b
Content-length: 10825

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.4.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;

# Print introduction
print "Web Secretary Ver 1.4.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
$base= $ENV{HOME} . "/.websec/";

# Parse command line options
$result = GetOptions("base=s"	=> \$base);

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "/archive/";
mkdir $base,0750 if !-d $base;
mkdir $base . $archive,0750 if !-d $base . $archive;
$outgoing = $base . "/index.html";
$page_current = $base . "/retrieve.html";

# prepare digest
@digest = ();

# Set default values
%defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ($keyword ne "URL" && $keyword ne "Auth" && $keyword ne "Name"
	&& $keyword ne "Prefix" && $keyword ne "Timeout"
	&& $keyword ne "Retry" && $keyword ne "Retrywait"
	&& $keyword ne "Randomwait"
	&& $keyword ne "Diff" && $keyword ne "Hicolor" && $keyword ne "Ignore"
	&& $keyword ne "Email" && $keyword ne "EmailLink"
	&& $keyword ne "Tmin" && $keyword ne "Tmax" && $keyword ne "Proxy"
	&& $keyword ne "IgnoreURL" && $keyword ne "ProxyAuth"
	&& $keyword ne "Digest" && $keyword ne "AddSubject")
    {
        print qq(Unrecognized keyword in line $.: "$_".\n);
        return -1;
    }

	# Allow values from the environment
	if ($value =~ m/^\${([^}]+)}/)
	{
		$value = $ENV{$1};
	}

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $page_previous = $base . $archive . $prefix . ".html";
    $page_archive = $base . $archive . $prefix . ".old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "webdiff --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing " .
        "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent("websec/1.0");
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}


Revision-number: 10
Prop-content-length: 234
Content-length: 234

K 7
svn:log
V 132
Remove trailing slash from $base so as not to have double slashes in the path names.
Use a simplex syntax to create the path names.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-04-26T13:15:03.642785Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 10897
Text-content-md5: 5d7d6bdf7883082ff5077b6f559a2248
Content-length: 10897

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.4.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;

# Print introduction
print "Web Secretary Ver 1.4.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
$base= $ENV{HOME} . "/.websec/";

# Parse command line options
$result = GetOptions("base=s"	=> \$base);

# Remove trailing slash from base, we will add it ourself everywhere needed
$base = s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,0750 if !-d $base;
mkdir $archive,0750 if !-d $archive;
$outgoing = "$base/index.html";
$page_current = "$base/retrieve.html";

# prepare digest
@digest = ();

# Set default values
%defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ($keyword ne "URL" && $keyword ne "Auth" && $keyword ne "Name"
	&& $keyword ne "Prefix" && $keyword ne "Timeout"
	&& $keyword ne "Retry" && $keyword ne "Retrywait"
	&& $keyword ne "Randomwait"
	&& $keyword ne "Diff" && $keyword ne "Hicolor" && $keyword ne "Ignore"
	&& $keyword ne "Email" && $keyword ne "EmailLink"
	&& $keyword ne "Tmin" && $keyword ne "Tmax" && $keyword ne "Proxy"
	&& $keyword ne "IgnoreURL" && $keyword ne "ProxyAuth"
	&& $keyword ne "Digest" && $keyword ne "AddSubject")
    {
        print qq(Unrecognized keyword in line $.: "$_".\n);
        return -1;
    }

	# Allow values from the environment
	if ($value =~ m/^\${([^}]+)}/)
	{
		$value = $ENV{$1};
	}

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase = "$archive/$prefix";
    $page_previous = "$pagebase.html";
    $page_archive = "$pagebase.old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "webdiff --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing " .
        "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent("websec/1.0");
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}


Revision-number: 11
Prop-content-length: 151
Content-length: 151

K 7
svn:log
V 50
Savannah bug 2914, Ignore list is comma seperated

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T12:03:09.903987Z
PROPS-END

Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 3321
Text-content-md5: 8be87f3fa3941ed06bd1ca7e852fe585
Content-length: 3321

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = your_email@domain.net
EmailLink = your_email@domain.net

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sciam.com/currentissue.html
Name = Scientic American
Prefix = sci-american

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://segfault.org/
Name = Segfault.org
Prefix = segfault
Hicolor = grey

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://www.linuxtoday.com/opinions/
Name = Linux Today
Prefix = linux-today

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news


Revision-number: 12
Prop-content-length: 135
Content-length: 135

K 7
svn:log
V 34
Fix translation of $base variable

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T14:24:39.408573Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 10898
Text-content-md5: 0ad81d664beee1c736b4f2175ddf038e
Content-length: 10898

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.4.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;

# Print introduction
print "Web Secretary Ver 1.4.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
$base= $ENV{HOME} . "/.websec/";

# Parse command line options
$result = GetOptions("base=s"	=> \$base);

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,0750 if !-d $base;
mkdir $archive,0750 if !-d $archive;
$outgoing = "$base/index.html";
$page_current = "$base/retrieve.html";

# prepare digest
@digest = ();

# Set default values
%defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ($keyword ne "URL" && $keyword ne "Auth" && $keyword ne "Name"
	&& $keyword ne "Prefix" && $keyword ne "Timeout"
	&& $keyword ne "Retry" && $keyword ne "Retrywait"
	&& $keyword ne "Randomwait"
	&& $keyword ne "Diff" && $keyword ne "Hicolor" && $keyword ne "Ignore"
	&& $keyword ne "Email" && $keyword ne "EmailLink"
	&& $keyword ne "Tmin" && $keyword ne "Tmax" && $keyword ne "Proxy"
	&& $keyword ne "IgnoreURL" && $keyword ne "ProxyAuth"
	&& $keyword ne "Digest" && $keyword ne "AddSubject")
    {
        print qq(Unrecognized keyword in line $.: "$_".\n);
        return -1;
    }

	# Allow values from the environment
	if ($value =~ m/^\${([^}]+)}/)
	{
		$value = $ENV{$1};
	}

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase = "$archive/$prefix";
    $page_previous = "$pagebase.html";
    $page_archive = "$pagebase.old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "webdiff --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing " .
        "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent("websec/1.0");
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}


Revision-number: 13
Prop-content-length: 295
Content-length: 295

K 7
svn:log
V 193
Insert websec manpage into websec, this allows using the --man option in websec to display the manpage, or --help to display the help, and both come from the same text to eliminate redundancy.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T14:26:07.335887Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 12548
Text-content-md5: 3c0b5d3a41595de4ef990fb3308983f7
Content-length: 12548

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.4.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.4.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
$base= $ENV{HOME} . "/.websec/";
$help = 0;
$man = 0;

# Parse command line options
GetOptions("help|?"	=> \$help,
           "man"	=> \$man,
           "base=s"	=> \$base);
pod2usage(1) if ($help);
pod2usage(-exitstatus => 0, -verbose => 2) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,0750 if !-d $base;
mkdir $archive,0750 if !-d $archive;
$outgoing = "$base/index.html";
$page_current = "$base/retrieve.html";

# prepare digest
@digest = ();

# Set default values
%defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ($keyword ne "URL" && $keyword ne "Auth" && $keyword ne "Name"
	&& $keyword ne "Prefix" && $keyword ne "Timeout"
	&& $keyword ne "Retry" && $keyword ne "Retrywait"
	&& $keyword ne "Randomwait"
	&& $keyword ne "Diff" && $keyword ne "Hicolor" && $keyword ne "Ignore"
	&& $keyword ne "Email" && $keyword ne "EmailLink"
	&& $keyword ne "Tmin" && $keyword ne "Tmax" && $keyword ne "Proxy"
	&& $keyword ne "IgnoreURL" && $keyword ne "ProxyAuth"
	&& $keyword ne "Digest" && $keyword ne "AddSubject")
    {
        print qq(Unrecognized keyword in line $.: "$_".\n);
        return -1;
    }

	# Allow values from the environment
	if ($value =~ m/^\${([^}]+)}/)
	{
		$value = $ENV{$1};
	}

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase = "$archive/$prefix";
    $page_previous = "$pagebase.html";
    $page_archive = "$pagebase.old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "webdiff --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing " .
        "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent("websec/1.0");
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<Websec> is a web page monitoring software. 
It will send you a changed web page with the contents
highlighted.

When called without an argument, B<websec> will try to
open I<$HOME/.websec/url.list>, but you can provide the
--base argument to provide a different directory to read the
config file from and keep the data in.


You can add a line like I<AddSubject = [websec]> to
url.list, websec will add I<[websec]> to every subject
as a first word when mail is sent. You can then easily detect
this line by a mail filter.


The keywords I<Retry>, I<Retrywait>, and
I<Timeout> in url.list lets you specify the number
of times to retry, time to wait between retries, and a
timeout setting.


B<Websec> waits for a random number of seconds between
retries up to the value specified by the I<Randomwait>
keyword. This is to prevent websec from being blocked by
websites that perform log analysis to find time similarities
between requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut


Node-path: trunk/websec/websec.1
Node-action: delete


Revision-number: 14
Prop-content-length: 248
Content-length: 248

K 7
svn:log
V 146
Run webdiff from the same location of websec if available, allows to simply unpack the package and use it without the need for an actual install.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T14:49:43.926444Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 12802
Text-content-md5: 9c437f8e6d9927afe5febaeb3df646a6
Content-length: 12802

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.4.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.4.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
$base= $ENV{HOME} . "/.websec/";
$help = 0;
$man = 0;

# Parse command line options
GetOptions("help|?"	=> \$help,
           "man"	=> \$man,
           "base=s"	=> \$base);
pod2usage(1) if ($help);
pod2usage(-exitstatus => 0, -verbose => 2) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,0750 if !-d $base;
mkdir $archive,0750 if !-d $archive;
$outgoing = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if (! -e $webdiffbin) {
	$webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
%defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ($keyword ne "URL" && $keyword ne "Auth" && $keyword ne "Name"
	&& $keyword ne "Prefix" && $keyword ne "Timeout"
	&& $keyword ne "Retry" && $keyword ne "Retrywait"
	&& $keyword ne "Randomwait"
	&& $keyword ne "Diff" && $keyword ne "Hicolor" && $keyword ne "Ignore"
	&& $keyword ne "Email" && $keyword ne "EmailLink"
	&& $keyword ne "Tmin" && $keyword ne "Tmax" && $keyword ne "Proxy"
	&& $keyword ne "IgnoreURL" && $keyword ne "ProxyAuth"
	&& $keyword ne "Digest" && $keyword ne "AddSubject")
    {
        print qq(Unrecognized keyword in line $.: "$_".\n);
        return -1;
    }

	# Allow values from the environment
	if ($value =~ m/^\${([^}]+)}/)
	{
		$value = $ENV{$1};
	}

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase = "$archive/$prefix";
    $page_previous = "$pagebase.html";
    $page_archive = "$pagebase.old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing " .
        "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent("websec/1.0");
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<Websec> is a web page monitoring software. 
It will send you a changed web page with the contents
highlighted.

When called without an argument, B<websec> will try to
open I<$HOME/.websec/url.list>, but you can provide the
--base argument to provide a different directory to read the
config file from and keep the data in.


You can add a line like I<AddSubject = [websec]> to
url.list, websec will add I<[websec]> to every subject
as a first word when mail is sent. You can then easily detect
this line by a mail filter.


The keywords I<Retry>, I<Retrywait>, and
I<Timeout> in url.list lets you specify the number
of times to retry, time to wait between retries, and a
timeout setting.


B<Websec> waits for a random number of seconds between
retries up to the value specified by the I<Randomwait>
keyword. This is to prevent websec from being blocked by
websites that perform log analysis to find time similarities
between requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut


Revision-number: 15
Prop-content-length: 176
Content-length: 176

K 7
svn:log
V 75
Attempt to use the current directory as the base if it has url.list in it.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T14:56:30.330061Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 12989
Text-content-md5: 7a05a1d751a1b7a28baddd2ec128253d
Content-length: 12989

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.4.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.4.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if (-e url.list) {
	$base = ".";
} else {
	$base= $ENV{HOME} . "/.websec";
}
$help = 0;
$man = 0;

# Parse command line options
GetOptions("help|?"	=> \$help,
           "man"	=> \$man,
           "base=s"	=> \$base);
pod2usage(1) if ($help);
pod2usage(-exitstatus => 0, -verbose => 2) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,0750 if !-d $base;
mkdir $archive,0750 if !-d $archive;
$outgoing = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if (! -e $webdiffbin) {
	$webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
%defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ($keyword ne "URL" && $keyword ne "Auth" && $keyword ne "Name"
	&& $keyword ne "Prefix" && $keyword ne "Timeout"
	&& $keyword ne "Retry" && $keyword ne "Retrywait"
	&& $keyword ne "Randomwait"
	&& $keyword ne "Diff" && $keyword ne "Hicolor" && $keyword ne "Ignore"
	&& $keyword ne "Email" && $keyword ne "EmailLink"
	&& $keyword ne "Tmin" && $keyword ne "Tmax" && $keyword ne "Proxy"
	&& $keyword ne "IgnoreURL" && $keyword ne "ProxyAuth"
	&& $keyword ne "Digest" && $keyword ne "AddSubject")
    {
        print qq(Unrecognized keyword in line $.: "$_".\n);
        return -1;
    }

	# Allow values from the environment
	if ($value =~ m/^\${([^}]+)}/)
	{
		$value = $ENV{$1};
	}

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase = "$archive/$prefix";
    $page_previous = "$pagebase.html";
    $page_archive = "$pagebase.old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing " .
        "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent("websec/1.0");
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut


Revision-number: 16
Prop-content-length: 152
Content-length: 152

K 7
svn:log
V 51
Correct usage of basedir, enforce a / in dir names

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T15:00:10.523842Z
PROPS-END

Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 11866
Text-content-md5: e6459a51c52dc7e677cbac8f0e46aac2
Content-length: 11866

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.4.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;

# Print help if no arguments passed
if ($#ARGV <= 0)
{
    print "Webdiff Ver 1.4.0\n";
    print "By Chew Wei Yih Copyleft (c) 1998\n\n";
    print "Options:\n";
    print "  --archive=<pathname>	Archive HTML file\n";
    print "  --current=<pathname>	Current HTML file\n";
    print "  --out=<pathname>		Output HTML file (with highlighting)\n";
    print "  --basedir=<pathname>	Base directory for files\n";
    print "  --hicolor=<color>		Highlight color (Def: blue, yellow, pink, grey or #rrggbb)\n";
    print "  --ignore=<filelist>	Comma-delimited list of named sections containing ignore keywords\n";
    print "  --ignoreurl=<filelist>	Comma-delimited list of named sections containing ignore urls\n";
    print "  --tmin=<number>		Don't check if token contains <= given no. of words\n";
    print "  --tmax=<number>		Don't ignore if token contains >= given no. of words\n";
    print "  --debug                Debug messages\n\n";
    exit -1;
}

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir	= $ENV{HOME} . "/.websec/";

# Parse options
GetOptions(
		"basedir=s"		=> \$basedir,
		"archive=s"		=> \$oldpage,
		"current=s"		=> \$curpage,
		"out=s"			=> \$outpage,
		"hicolor=s"		=> \$hicolor,
		"ignore=s" 		=> \$ignore,
		"ignoreurl=s"	=> \$ignoreurl,
		"tmin=i"		=> \$tmin,
		"tmax=i"		=> \$tmax,
		"debug"			=> \$debug,
		"ignorefile=s"	=> \$ignoreFile);

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ($oldpage eq "")
{
	print "You did not supply the archive HTML file via the --archive option.\n";
	exit -1;
}
if ($curpage eq "")
{
	print "You did not supply the current HTML file via the --current option.\n";
	exit -1;
}
if ($outpage eq "")
{
	print "You did not supply the output HTML file via the --out option.\n";
	exit -1;
}

# Choose highlighting color
%colorList = ( yellow => "#ffff99", blue => "#66ccff", pink => "#ffcccc", grey => "#4c4c4c" );
if (defined $colorList{$hicolor}) { $hicolor = $colorList{$hicolor}; }
if ($hicolor eq "") { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags = ( "CODE", "B", "I", "U", "TT", "EM", "FONT*", "SUP", "SUB", "SMALL", "STRIKE", "STRONG", "CAPTION*", "A*" );

# Read ignore keywords
if ($ignore ne "none")
{
    $ignore = "," . $ignore . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir/$ignoreFile") or die "Cannot open $basedir/$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignore =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignore = split/[\r\n]/, $ignorelist;
}
if ($debug) { foreach (@ignore) { print "Ignore: $_\n"; } }
close(IGNORE);

# Read ignore urls
if ($ignoreurl ne "none")
{
    $ignoreurl = "," . $ignoreurl . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir/$ignoreFile") or die "Cannot open $basedir/$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignoreurl =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignoreurl = split/[\r\n]/, $ignorelist;
}
if ($debug) { foreach (@ignoreurl) { print "IgnoreURL: $_\n"; } }

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open(OLDPAGE, "< $oldpage") or die "Cannot open $oldpage: $!\n";
open(CURPAGE, "< $curpage") or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;                                                # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;                                    # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;     # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags)
{
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ($tag =~ s/\*/ /)
    {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage); @oldtokens = @tokens; $#tokens = -1;
if ($debug) { foreach (@oldtokens) { print ">>>> $_\n"; } }
&TokenizePage($newpage); @newtokens = @tokens; $#tokens = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens)
{
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open(OUTPAGE, "> $outpage") or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if (!$changeStatus)
{
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage()
{
    my $page = shift(@_);
    @tokens = split/(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff()
{
    my $commentOn = 0;
    my $scriptOn = 0;
    my $styleOn = 0;
    my $titleOn = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens)
    {
        if ($token eq "") { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ($token =~ m|^.*?<!-.*?$|) { $commentOn = 1; }
        if ($token =~ m|^.*?->.*?|) { $commentOn = 0; next; }

        if ($token =~ m|^.*?<TITLE.*?>$|i) { $titleOn = 1; }
        if ($token =~ m|^.*?</TITLE.*?>$|i) { $titleOn = 0; next; }

        if ($token =~ m|^.*?<SCRIPT.*?>$|i) { $scriptOn = 1; }
        if ($token =~ m|^.*?</SCRIPT.*?>$|i) { $scriptOn = 0; next; }

        if ($token =~ m|^.*?<STYLE.*?>$|i) { $styleOn = 1; }
        if ($token =~ m|^.*?</STYLE.*?>$|i) { $styleOn = 0; next; }

        if (TokenContainsIgnoreURL($token)) { $ignoreUrlOn = 1; }
        if ($ignoreUrlOn && TokenContainsHlinkEnd($token)) { $ignoreUrlOn = 0; next; }

        if ($commentOn)
        {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn)
        {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn)
        {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn)
        {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn)
        {
            if ($debug) { print "#### Token contains ignore URL - $lastIgnoreURL\n"; }
        }
        elsif ($token =~ m/<.*?>/sig)
        {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif (TokenIsMangledHTMLTag($token))
        {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif (TokenContainsIgnoreKeyword($token))
        {
            if ($debug) { print "#### Token contains ignore keyword - $lastIgnoreKeyword\n"; }
        }
        elsif (TokenExists($token))
        {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else
        {
            if ($debug ) { print "#### Token has been highlighted!\n"; }
            $token = "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>" .
                $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag()
{
    my $token = shift(@_);

    while($token ne "")
    {
        if ($token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i)
        {
            $token = $2;
            if (!$1 =~ m|^\s*$|) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C".($#words+1).": $tokdup\n"; }
    if ($#words+1 > $tmax) { return 0; }

    foreach $keyword (@ignore)
    {
        if ($token  =~ m:^.*?(\b$keyword\b).*?$:i ||
            $tokdup =~ m:^.*?(\b$keyword\b).*?$:i)
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split/\s/, $tokdup;
    if ($#words+1 <= $tmin) { return 1; }

    foreach $oldtok (@oldtokens)
    {
        $oldtok =~ s/\s{2,}/ /sig;
        if ($token eq $oldtok) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl)
    {
        if ($token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i)
        {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}


Revision-number: 17
Prop-content-length: 198
Content-length: 198

K 7
svn:log
V 97
Add an option to set the UserAgent of the "Browser", to enable access to browser limiting sites.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T15:00:58.094447Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 12626
Text-content-md5: 750b3702b8a8e662337f7a11cec2a7a0
Content-length: 12626

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.4.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.4.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if (-e url.list) {
	$base = ".";
} else {
	$base= $ENV{HOME} . "/.websec";
}
$help = 0;
$man = 0;

# Parse command line options
GetOptions("help|?"	=> \$help,
           "man"	=> \$man,
           "base=s"	=> \$base);
pod2usage(1) if ($help);
pod2usage(-exitstatus => 0, -verbose => 2) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,0750 if !-d $base;
mkdir $archive,0750 if !-d $archive;
$outgoing = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if (! -e $webdiffbin) {
	$webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
    UserAgent => "WebSec/1.4.0",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if (not defined $defaults{$keyword})
    {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

	# Allow values from the environment
	if ($value =~ m/^\${([^}]+)}/)
	{
		$value = $ENV{$1};
	}

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};
    $useragent = $siteinfo{UserAgent};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase = "$archive/$prefix";
    $page_previous = "$pagebase.html";
    $page_archive = "$pagebase.old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing " .
        "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut


Revision-number: 18
Prop-content-length: 117
Content-length: 117

K 7
svn:log
V 16
Quote file name

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T15:15:17.315203Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 12628
Text-content-md5: 5756c178aca2be80d8be625798a2ede9
Content-length: 12628

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.4.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.4.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if (-e "url.list") {
	$base = ".";
} else {
	$base= $ENV{HOME} . "/.websec";
}
$help = 0;
$man = 0;

# Parse command line options
GetOptions("help|?"	=> \$help,
           "man"	=> \$man,
           "base=s"	=> \$base);
pod2usage(1) if ($help);
pod2usage(-exitstatus => 0, -verbose => 2) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,0750 if !-d $base;
mkdir $archive,0750 if !-d $archive;
$outgoing = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if (! -e $webdiffbin) {
	$webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
    UserAgent => "WebSec/1.4.0",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if (not defined $defaults{$keyword})
    {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

	# Allow values from the environment
	if ($value =~ m/^\${([^}]+)}/)
	{
		$value = $ENV{$1};
	}

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};
    $useragent = $siteinfo{UserAgent};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase = "$archive/$prefix";
    $page_previous = "$pagebase.html";
    $page_archive = "$pagebase.old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing " .
        "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut


Revision-number: 19
Prop-content-length: 219
Content-length: 219

K 7
svn:log
V 117
Change example e-mail addresses to ones that are reserved for examples.
Remove non-existing links from the examples.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T15:16:52.970569Z
PROPS-END

Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 3139
Text-content-md5: 39af88e04469d37aa54d9923afebd800
Content-length: 3139

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = myself@example.com
EmailLink = myself@example.com

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://segfault.org/
Name = Segfault.org
Prefix = segfault
Hicolor = grey

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news


Revision-number: 20
Prop-content-length: 172
Content-length: 172

K 7
svn:log
V 71
Move webdiff manpage into the code, for the same reasons as for websec

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T15:53:54.959652Z
PROPS-END

Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12335
Text-content-md5: 114c1ca77b05a1668e02d722b758317d
Content-length: 12335

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.4.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir	= $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man = 0;
GetOptions(
		"help|?"		=> \$help,
		"man"			=> \$man,
		"basedir=s"		=> \$basedir,
		"archive=s"		=> \$oldpage,
		"current=s"		=> \$curpage,
		"out=s"			=> \$outpage,
		"hicolor=s"		=> \$hicolor,
		"ignore=s" 		=> \$ignore,
		"ignoreurl=s"	=> \$ignoreurl,
		"tmin=i"		=> \$tmin,
		"tmax=i"		=> \$tmax,
		"debug"			=> \$debug,
		"ignorefile=s"	=> \$ignoreFile)
	or pod2usage(0);

pod2usage(1) if ($help);
pod2usage(-exitstatus => 0, -verbose => 2) if $man;


# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ($oldpage eq "")
{
	print "You did not supply the archive HTML file via the --archive option.\n";
	exit -1;
}
if ($curpage eq "")
{
	print "You did not supply the current HTML file via the --current option.\n";
	exit -1;
}
if ($outpage eq "")
{
	print "You did not supply the output HTML file via the --out option.\n";
	exit -1;
}

# Choose highlighting color
%colorList = ( yellow => "#ffff99", blue => "#66ccff", pink => "#ffcccc", grey => "#4c4c4c" );
if (defined $colorList{$hicolor}) { $hicolor = $colorList{$hicolor}; }
if ($hicolor eq "") { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags = ( "CODE", "B", "I", "U", "TT", "EM", "FONT*", "SUP", "SUB", "SMALL", "STRIKE", "STRONG", "CAPTION*", "A*" );

# Read ignore keywords
if ($ignore ne "none")
{
    $ignore = "," . $ignore . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir/$ignoreFile") or die "Cannot open $basedir/$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignore =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignore = split/[\r\n]/, $ignorelist;
}
if ($debug) { foreach (@ignore) { print "Ignore: $_\n"; } }
close(IGNORE);

# Read ignore urls
if ($ignoreurl ne "none")
{
    $ignoreurl = "," . $ignoreurl . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir/$ignoreFile") or die "Cannot open $basedir/$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignoreurl =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignoreurl = split/[\r\n]/, $ignorelist;
}
if ($debug) { foreach (@ignoreurl) { print "IgnoreURL: $_\n"; } }

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open(OLDPAGE, "< $oldpage") or die "Cannot open $oldpage: $!\n";
open(CURPAGE, "< $curpage") or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;                                                # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;                                    # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;     # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags)
{
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ($tag =~ s/\*/ /)
    {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage); @oldtokens = @tokens; $#tokens = -1;
if ($debug) { foreach (@oldtokens) { print ">>>> $_\n"; } }
&TokenizePage($newpage); @newtokens = @tokens; $#tokens = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens)
{
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open(OUTPAGE, "> $outpage") or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if (!$changeStatus)
{
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage()
{
    my $page = shift(@_);
    @tokens = split/(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff()
{
    my $commentOn = 0;
    my $scriptOn = 0;
    my $styleOn = 0;
    my $titleOn = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens)
    {
        if ($token eq "") { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ($token =~ m|^.*?<!-.*?$|) { $commentOn = 1; }
        if ($token =~ m|^.*?->.*?|) { $commentOn = 0; next; }

        if ($token =~ m|^.*?<TITLE.*?>$|i) { $titleOn = 1; }
        if ($token =~ m|^.*?</TITLE.*?>$|i) { $titleOn = 0; next; }

        if ($token =~ m|^.*?<SCRIPT.*?>$|i) { $scriptOn = 1; }
        if ($token =~ m|^.*?</SCRIPT.*?>$|i) { $scriptOn = 0; next; }

        if ($token =~ m|^.*?<STYLE.*?>$|i) { $styleOn = 1; }
        if ($token =~ m|^.*?</STYLE.*?>$|i) { $styleOn = 0; next; }

        if (TokenContainsIgnoreURL($token)) { $ignoreUrlOn = 1; }
        if ($ignoreUrlOn && TokenContainsHlinkEnd($token)) { $ignoreUrlOn = 0; next; }

        if ($commentOn)
        {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn)
        {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn)
        {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn)
        {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn)
        {
            if ($debug) { print "#### Token contains ignore URL - $lastIgnoreURL\n"; }
        }
        elsif ($token =~ m/<.*?>/sig)
        {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif (TokenIsMangledHTMLTag($token))
        {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif (TokenContainsIgnoreKeyword($token))
        {
            if ($debug) { print "#### Token contains ignore keyword - $lastIgnoreKeyword\n"; }
        }
        elsif (TokenExists($token))
        {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else
        {
            if ($debug ) { print "#### Token has been highlighted!\n"; }
            $token = "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>" .
                $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag()
{
    my $token = shift(@_);

    while($token ne "")
    {
        if ($token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i)
        {
            $token = $2;
            if (!$1 =~ m|^\s*$|) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C".($#words+1).": $tokdup\n"; }
    if ($#words+1 > $tmax) { return 0; }

    foreach $keyword (@ignore)
    {
        if ($token  =~ m:^.*?(\b$keyword\b).*?$:i ||
            $tokdup =~ m:^.*?(\b$keyword\b).*?$:i)
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split/\s/, $tokdup;
    if ($#words+1 <= $tmin) { return 1; }

    foreach $oldtok (@oldtokens)
    {
        $oldtok =~ s/\s{2,}/ /sig;
        if ($token eq $oldtok) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl)
    {
        if ($token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i)
        {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.



Node-path: trunk/websec/webdiff.1
Node-action: delete


Revision-number: 21
Prop-content-length: 202
Content-length: 202

K 7
svn:log
V 100
Redo Makefile:
 - Use variables for directories
 - Create man pages from executables during install

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T16:08:29.092560Z
PROPS-END

Node-path: trunk/websec/Makefile
Node-kind: file
Node-action: change
Text-content-length: 574
Text-content-md5: df33c6ea85b2d7fbdd6b6169a9e81885
Content-length: 574

PREFIX=/usr/local
BINDIR=$(DESTDIR)$(PREFIX)/bin
MANDIR=$(DESTDIR)$(PREFIX)/share/man
DOCDIR=$(DESTDIR)$(PREFIX)/share/doc/websec

all: websec.1 webdiff.1

install:
	install -d $(BINDIR)
	install websec $(BINDIR)/
	install webdiff $(BINDIR)/

	install -d $(MANDIR)/man1/
	install websec.1 $(MANDIR)/man1/
	install webdiff.1 $(MANDIR)/man1/

	install -d $(DOCDIR)
	install -m 0644 url.list $(DOCDIR)/examples/
	install -m 0644 ignore.list $(DOCDIR)/examples/
	install -m 0644 README $(DOCDIR)/

clean:
	rm -f websec.1 webdiff.1

websec.1 webdiff.1 : %.1 : %
	pod2man $< > $@


Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 16965
Text-content-md5: 3eb97dc7a045289b50d92c3307a233eb
Content-length: 16965

WEB SECRETARY Version 1.5.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec <URL list>'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://homemade.hypermart.net/websec/
                           or:   http://homemade.virtualave.net/websec/


5. URL LIST

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


6. IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


7. IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


8. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


9. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


10. HISTORY

1.5.0 - Release on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.
* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.
* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


11. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12335
Text-content-md5: 9f3a9c95a769932785221537da1e45be
Content-length: 12335

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.5.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir	= $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man = 0;
GetOptions(
		"help|?"		=> \$help,
		"man"			=> \$man,
		"basedir=s"		=> \$basedir,
		"archive=s"		=> \$oldpage,
		"current=s"		=> \$curpage,
		"out=s"			=> \$outpage,
		"hicolor=s"		=> \$hicolor,
		"ignore=s" 		=> \$ignore,
		"ignoreurl=s"	=> \$ignoreurl,
		"tmin=i"		=> \$tmin,
		"tmax=i"		=> \$tmax,
		"debug"			=> \$debug,
		"ignorefile=s"	=> \$ignoreFile)
	or pod2usage(0);

pod2usage(1) if ($help);
pod2usage(-exitstatus => 0, -verbose => 2) if $man;


# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ($oldpage eq "")
{
	print "You did not supply the archive HTML file via the --archive option.\n";
	exit -1;
}
if ($curpage eq "")
{
	print "You did not supply the current HTML file via the --current option.\n";
	exit -1;
}
if ($outpage eq "")
{
	print "You did not supply the output HTML file via the --out option.\n";
	exit -1;
}

# Choose highlighting color
%colorList = ( yellow => "#ffff99", blue => "#66ccff", pink => "#ffcccc", grey => "#4c4c4c" );
if (defined $colorList{$hicolor}) { $hicolor = $colorList{$hicolor}; }
if ($hicolor eq "") { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags = ( "CODE", "B", "I", "U", "TT", "EM", "FONT*", "SUP", "SUB", "SMALL", "STRIKE", "STRONG", "CAPTION*", "A*" );

# Read ignore keywords
if ($ignore ne "none")
{
    $ignore = "," . $ignore . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir/$ignoreFile") or die "Cannot open $basedir/$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignore =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignore = split/[\r\n]/, $ignorelist;
}
if ($debug) { foreach (@ignore) { print "Ignore: $_\n"; } }
close(IGNORE);

# Read ignore urls
if ($ignoreurl ne "none")
{
    $ignoreurl = "," . $ignoreurl . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir/$ignoreFile") or die "Cannot open $basedir/$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignoreurl =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignoreurl = split/[\r\n]/, $ignorelist;
}
if ($debug) { foreach (@ignoreurl) { print "IgnoreURL: $_\n"; } }

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open(OLDPAGE, "< $oldpage") or die "Cannot open $oldpage: $!\n";
open(CURPAGE, "< $curpage") or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;                                                # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;                                    # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;     # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags)
{
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ($tag =~ s/\*/ /)
    {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage); @oldtokens = @tokens; $#tokens = -1;
if ($debug) { foreach (@oldtokens) { print ">>>> $_\n"; } }
&TokenizePage($newpage); @newtokens = @tokens; $#tokens = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens)
{
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open(OUTPAGE, "> $outpage") or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if (!$changeStatus)
{
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage()
{
    my $page = shift(@_);
    @tokens = split/(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff()
{
    my $commentOn = 0;
    my $scriptOn = 0;
    my $styleOn = 0;
    my $titleOn = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens)
    {
        if ($token eq "") { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ($token =~ m|^.*?<!-.*?$|) { $commentOn = 1; }
        if ($token =~ m|^.*?->.*?|) { $commentOn = 0; next; }

        if ($token =~ m|^.*?<TITLE.*?>$|i) { $titleOn = 1; }
        if ($token =~ m|^.*?</TITLE.*?>$|i) { $titleOn = 0; next; }

        if ($token =~ m|^.*?<SCRIPT.*?>$|i) { $scriptOn = 1; }
        if ($token =~ m|^.*?</SCRIPT.*?>$|i) { $scriptOn = 0; next; }

        if ($token =~ m|^.*?<STYLE.*?>$|i) { $styleOn = 1; }
        if ($token =~ m|^.*?</STYLE.*?>$|i) { $styleOn = 0; next; }

        if (TokenContainsIgnoreURL($token)) { $ignoreUrlOn = 1; }
        if ($ignoreUrlOn && TokenContainsHlinkEnd($token)) { $ignoreUrlOn = 0; next; }

        if ($commentOn)
        {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn)
        {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn)
        {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn)
        {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn)
        {
            if ($debug) { print "#### Token contains ignore URL - $lastIgnoreURL\n"; }
        }
        elsif ($token =~ m/<.*?>/sig)
        {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif (TokenIsMangledHTMLTag($token))
        {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif (TokenContainsIgnoreKeyword($token))
        {
            if ($debug) { print "#### Token contains ignore keyword - $lastIgnoreKeyword\n"; }
        }
        elsif (TokenExists($token))
        {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else
        {
            if ($debug ) { print "#### Token has been highlighted!\n"; }
            $token = "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>" .
                $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag()
{
    my $token = shift(@_);

    while($token ne "")
    {
        if ($token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i)
        {
            $token = $2;
            if (!$1 =~ m|^\s*$|) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C".($#words+1).": $tokdup\n"; }
    if ($#words+1 > $tmax) { return 0; }

    foreach $keyword (@ignore)
    {
        if ($token  =~ m:^.*?(\b$keyword\b).*?$:i ||
            $tokdup =~ m:^.*?(\b$keyword\b).*?$:i)
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split/\s/, $tokdup;
    if ($#words+1 <= $tmin) { return 1; }

    foreach $oldtok (@oldtokens)
    {
        $oldtok =~ s/\s{2,}/ /sig;
        if ($token eq $oldtok) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl)
    {
        if ($token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i)
        {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.



Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 12628
Text-content-md5: 3f21a85f9367e9ba841179ca37f282f7
Content-length: 12628

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.5.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.5.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if (-e "url.list") {
	$base = ".";
} else {
	$base= $ENV{HOME} . "/.websec";
}
$help = 0;
$man = 0;

# Parse command line options
GetOptions("help|?"	=> \$help,
           "man"	=> \$man,
           "base=s"	=> \$base);
pod2usage(1) if ($help);
pod2usage(-exitstatus => 0, -verbose => 2) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,0750 if !-d $base;
mkdir $archive,0750 if !-d $archive;
$outgoing = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if (! -e $webdiffbin) {
	$webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults =
(
    URL       => "",
    Auth      => "none",
    Name      => "",
    Prefix    => "",
    Diff      => "webdiff",
    Hicolor   => "blue",
    Ignore    => "none",
    IgnoreURL => "none",
    Email     => "",
    EmailLink => "",
    Proxy     => "",
    ProxyAuth => "none",
    Randomwait=> 0,
    Retry     => 3,
    Retrywait => 0,
    Timeout   => 20,
    Tmin      => 0,
    Tmax      => 99999,
    AddSubject=> "",
    Digest    => "false",
    UserAgent => "WebSec/1.5.0",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while(<>)
{
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }
    
    # Handle non-empty lines
    if (length != 0)
    {
        $rc = &HandleInput();
        if ($rc != 0) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ($rc != 0) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
    if ($rc == 0 && $siteinfo{URL} ne "") { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest)
{
    $linkmsg = "The contents of the following URLs have changed:\n\n" . join("\n",@digest) . "\n";
    $subj = "[websec] - $today";
    &MailMessage($linkmsg, $subj, $digestEmail);
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput()
{
    # Get keyword, value pair
    ($keyword, $value) = split(/=/, $_, 2);
    $keyword   =~ s/^\s*(.*?)\s*$/$1/;
    $value     =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if (not defined $defaults{$keyword})
    {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

	# Allow values from the environment
	if ($value =~ m/^\${([^}]+)}/)
	{
		$value = $ENV{$1};
	}

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite()
{
    # Get parameter values for this page
    $url       = $siteinfo{URL};
    $auth      = $siteinfo{Auth};
    $name      = $siteinfo{Name};
    $prefix    = $siteinfo{Prefix};
    $diff      = $siteinfo{Diff};
    $hicolor   = $siteinfo{Hicolor};
    $ignore    = $siteinfo{Ignore};
    $ignoreurl = $siteinfo{IgnoreURL};
    $email     = $siteinfo{Email};
    $emailLink = $siteinfo{EmailLink};
    $proxy     = $siteinfo{Proxy};
    $proxyAuth = $siteinfo{ProxyAuth};
    $randomwait= $siteinfo{Randomwait};
    $retry     = $siteinfo{Retry};
    $retrywait = $siteinfo{Retrywait};
    $timeout   = $siteinfo{Timeout};
    $tmin      = $siteinfo{Tmin};
    $tmax      = $siteinfo{Tmax};
    $addsubject= $siteinfo{AddSubject};
    $digest    = $siteinfo{Digest};
    $useragent = $siteinfo{UserAgent};

    # If block without URL, assume parameter setting block and update default values
    if ($url eq "")
    {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ($name eq "" || $prefix eq "" || ($email eq "" && $emailLink eq ""))
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase = "$archive/$prefix";
    $page_previous = "$pagebase.html";
    $page_archive = "$pagebase.old.html";
    $page_previousExists = 1;
    open(FILE, $page_previous) or $page_previousExists = 0;
    close(FILE);
    $subj = "$addsubject $name - $today";
    $webdiff =
        "$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing " .
        "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ($proxy ne "") { $ua->proxy(http => $proxy); }
    $req = new HTTP::Request('GET', $url);
    if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
    if ($proxyAuth ne "none") {
	$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));
    }

    # Try up to '$retry' times to download URL
    $counter=0;
    srand;
    while($counter < $retry)
    {
        $resp = $ua->request($req);
        if ($resp->is_success) { last; }
        else
	    {
	    $counter++;
	    if ($randomwait > 0) {
		$random = int(rand $randomwait) + 1;
		sleep $random;
	    }
	    else { sleep $retrywait; }
	    }
    }

    # If URL is successfully downloaded
    if ($resp->is_success)
    {
        open (HTML_FILE, ">$page_current");
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base. "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ($diff eq "webdiff")
        {
            if ($page_previousExists == 1)
            {
                print "Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ($rc != 0)
                {
                    print "Sending highlighted page to $email ...\n";
                    if ($email ne "")
                    {
                        MailDocument($outgoing, $subj, $email);
                    }
                    if ($emailLink ne "")
                    {
            			if (($digest ne "no") && ($digest ne "false"))
                        {
            			    push @digest,$url;
			                ($digestEmail) or ($digestEmail=$emailLink);
            			}
                        else
                        {
            			    $linkmsg = "The contents of the following URL has changed:\n\n$url\n";
            			    MailMessage($linkmsg, $subj, $emailLink);
			            }
                    }
                }
                else
                {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current, $page_previous;
            }
            else
            {
                print "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else
        {
            if ($email ne "") { MailDocument($page_current, $subj, $email); }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }
    # If unable to download URL
    else
    {
        print "Unable to retrieve page.\n";
        $errmsg = 
            "Unable to retrieve $name ($url).\n\n" .
            "Detailed error as follows:\n" . $resp->error_as_HTML;
        if ($email ne "") { MailMessage($errmsg, $subj, $email); }
        if ($emailLink ne "") {
	    if (($digest ne "no") && ($digest ne "false")) {
		push @digest,"Unable to retrieve: $url";
		($digestEmail) or ($digestEmail=$emailLink);
	    } else {
		MailMessage($errmsg, $subj, $emailLink);
	    }
	}
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage()
{
    my $message = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/plain; charset=us-ascii");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument()
{
    my $filename = shift(@_);
    my $subject = shift(@_);
    my @recipients = split/,/, shift(@_);
    my $tmpstr = $/;

    undef $/;
    open(FILE, "$filename") or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients)
    {
        $req = HTTP::Request->new(POST => "mailto:" . $email);
        $req->header("Subject", $subject);
        $req->header("Content-type", "text/html");
        $req->header("Content-Transfer-Encoding", "7bit");
        $req->header("MIME-Version", "1.0");
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut


Revision-number: 22
Prop-content-length: 153
Content-length: 153

K 7
svn:log
V 52
Update README for 1.5.0
Update examples in url.list

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T16:10:54.738894Z
PROPS-END

Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 17073
Text-content-md5: b69c118408ae26ed6f8575f9a6a3faa1
Content-length: 17073

WEB SECRETARY Version 1.5.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec <URL list>'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://homemade.hypermart.net/websec/
                           or:   http://homemade.virtualave.net/websec/


5. URL LIST

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


6. IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


7. IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


8. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


9. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


10. HISTORY

1.5.0 - Release on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.
* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.
* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com
* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


11. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 3141
Text-content-md5: 48d6528b26142a24c0fbeb2a820279fd
Content-length: 3141

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://segfault.org/
Name = Segfault.org
Prefix = segfault
Hicolor = grey

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news


Revision-number: 23
Prop-content-length: 119
Content-length: 119

K 7
svn:log
V 18
Tag version 1.5.0

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T16:12:03.880912Z
PROPS-END

Node-path: tags/websec/version-1.5.0
Node-kind: dir
Node-action: add
Node-copyfrom-rev: 22
Node-copyfrom-path: trunk/websec


Revision-number: 24
Prop-content-length: 147
Content-length: 147

K 7
svn:log
V 46
Fix installation, add creation of example dir

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T16:15:40.990153Z
PROPS-END

Node-path: trunk/websec/Makefile
Node-kind: file
Node-action: change
Text-content-length: 605
Text-content-md5: a793daaeaa3da0e8a420c14a04428d17
Content-length: 605

PREFIX=/usr/local
BINDIR=$(DESTDIR)$(PREFIX)/bin
MANDIR=$(DESTDIR)$(PREFIX)/share/man
DOCDIR=$(DESTDIR)$(PREFIX)/share/doc/websec

all: websec.1 webdiff.1

install:
	install -d $(BINDIR)
	install websec $(BINDIR)/
	install webdiff $(BINDIR)/

	install -d $(MANDIR)/man1/
	install websec.1 $(MANDIR)/man1/
	install webdiff.1 $(MANDIR)/man1/

	install -d $(DOCDIR)
	install -d $(DOCDIR)/examples
	install -m 0644 url.list $(DOCDIR)/examples/
	install -m 0644 ignore.list $(DOCDIR)/examples/
	install -m 0644 README $(DOCDIR)/

clean:
	rm -f websec.1 webdiff.1

websec.1 webdiff.1 : %.1 : %
	pod2man $< > $@


Revision-number: 25
Prop-content-length: 165
Content-length: 165

K 7
svn:log
V 64
Remove tag of 1.5.0, it doesn't install and was never released.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T16:17:22.683286Z
PROPS-END

Node-path: tags/websec/version-1.5.0
Node-action: delete


Revision-number: 26
Prop-content-length: 119
Content-length: 119

K 7
svn:log
V 18
New tag for 1.5.0

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T16:17:32.956463Z
PROPS-END

Node-path: tags/websec/version-1.5.0
Node-kind: dir
Node-action: add
Node-copyfrom-rev: 25
Node-copyfrom-path: trunk/websec


Revision-number: 27
Prop-content-length: 118
Content-length: 118

K 7
svn:log
V 17
cut the pod data

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T20:10:19.820565Z
PROPS-END

Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12341
Text-content-md5: 3df81472589137e03f77cb79fa46e45f
Content-length: 12341

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.5.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir	= $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man = 0;
GetOptions(
		"help|?"		=> \$help,
		"man"			=> \$man,
		"basedir=s"		=> \$basedir,
		"archive=s"		=> \$oldpage,
		"current=s"		=> \$curpage,
		"out=s"			=> \$outpage,
		"hicolor=s"		=> \$hicolor,
		"ignore=s" 		=> \$ignore,
		"ignoreurl=s"	=> \$ignoreurl,
		"tmin=i"		=> \$tmin,
		"tmax=i"		=> \$tmax,
		"debug"			=> \$debug,
		"ignorefile=s"	=> \$ignoreFile)
	or pod2usage(0);

pod2usage(1) if ($help);
pod2usage(-exitstatus => 0, -verbose => 2) if $man;


# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ($oldpage eq "")
{
	print "You did not supply the archive HTML file via the --archive option.\n";
	exit -1;
}
if ($curpage eq "")
{
	print "You did not supply the current HTML file via the --current option.\n";
	exit -1;
}
if ($outpage eq "")
{
	print "You did not supply the output HTML file via the --out option.\n";
	exit -1;
}

# Choose highlighting color
%colorList = ( yellow => "#ffff99", blue => "#66ccff", pink => "#ffcccc", grey => "#4c4c4c" );
if (defined $colorList{$hicolor}) { $hicolor = $colorList{$hicolor}; }
if ($hicolor eq "") { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags = ( "CODE", "B", "I", "U", "TT", "EM", "FONT*", "SUP", "SUB", "SMALL", "STRIKE", "STRONG", "CAPTION*", "A*" );

# Read ignore keywords
if ($ignore ne "none")
{
    $ignore = "," . $ignore . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir/$ignoreFile") or die "Cannot open $basedir/$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignore =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignore = split/[\r\n]/, $ignorelist;
}
if ($debug) { foreach (@ignore) { print "Ignore: $_\n"; } }
close(IGNORE);

# Read ignore urls
if ($ignoreurl ne "none")
{
    $ignoreurl = "," . $ignoreurl . ",";
    $ignorelist = "";
    $ignoreStartRead = 0;
    open(IGNORE, "< $basedir/$ignoreFile") or die "Cannot open $basedir/$ignoreFile: $!\n";
    while(<IGNORE>)
    {
        chomp;
        if ($ignoreStartRead && $_ eq "") { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ($section = $_) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ($ignoreurl =~ m:,$section,:i) { $ignoreStartRead = 1; }
    }
    @ignoreurl = split/[\r\n]/, $ignorelist;
}
if ($debug) { foreach (@ignoreurl) { print "IgnoreURL: $_\n"; } }

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open(OLDPAGE, "< $oldpage") or die "Cannot open $oldpage: $!\n";
open(CURPAGE, "< $curpage") or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;                                                # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;                                    # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;     # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags)
{
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ($tag =~ s/\*/ /)
    {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage); @oldtokens = @tokens; $#tokens = -1;
if ($debug) { foreach (@oldtokens) { print ">>>> $_\n"; } }
&TokenizePage($newpage); @newtokens = @tokens; $#tokens = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens)
{
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open(OUTPAGE, "> $outpage") or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if (!$changeStatus)
{
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage()
{
    my $page = shift(@_);
    @tokens = split/(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff()
{
    my $commentOn = 0;
    my $scriptOn = 0;
    my $styleOn = 0;
    my $titleOn = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens)
    {
        if ($token eq "") { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ($token =~ m|^.*?<!-.*?$|) { $commentOn = 1; }
        if ($token =~ m|^.*?->.*?|) { $commentOn = 0; next; }

        if ($token =~ m|^.*?<TITLE.*?>$|i) { $titleOn = 1; }
        if ($token =~ m|^.*?</TITLE.*?>$|i) { $titleOn = 0; next; }

        if ($token =~ m|^.*?<SCRIPT.*?>$|i) { $scriptOn = 1; }
        if ($token =~ m|^.*?</SCRIPT.*?>$|i) { $scriptOn = 0; next; }

        if ($token =~ m|^.*?<STYLE.*?>$|i) { $styleOn = 1; }
        if ($token =~ m|^.*?</STYLE.*?>$|i) { $styleOn = 0; next; }

        if (TokenContainsIgnoreURL($token)) { $ignoreUrlOn = 1; }
        if ($ignoreUrlOn && TokenContainsHlinkEnd($token)) { $ignoreUrlOn = 0; next; }

        if ($commentOn)
        {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn)
        {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn)
        {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn)
        {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn)
        {
            if ($debug) { print "#### Token contains ignore URL - $lastIgnoreURL\n"; }
        }
        elsif ($token =~ m/<.*?>/sig)
        {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif (TokenIsMangledHTMLTag($token))
        {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif (TokenContainsIgnoreKeyword($token))
        {
            if ($debug) { print "#### Token contains ignore keyword - $lastIgnoreKeyword\n"; }
        }
        elsif (TokenExists($token))
        {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else
        {
            if ($debug ) { print "#### Token has been highlighted!\n"; }
            $token = "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>" .
                $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag()
{
    my $token = shift(@_);

    while($token ne "")
    {
        if ($token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i)
        {
            $token = $2;
            if (!$1 =~ m|^\s*$|) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C".($#words+1).": $tokdup\n"; }
    if ($#words+1 > $tmax) { return 0; }

    foreach $keyword (@ignore)
    {
        if ($token  =~ m:^.*?(\b$keyword\b).*?$:i ||
            $tokdup =~ m:^.*?(\b$keyword\b).*?$:i)
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split/\s/, $tokdup;
    if ($#words+1 <= $tmin) { return 1; }

    foreach $oldtok (@oldtokens)
    {
        $oldtok =~ s/\s{2,}/ /sig;
        if ($token eq $oldtok) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl)
    {
        if ($token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i)
        {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd()
{
    my $token = shift(@_);
    $token  =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut



Revision-number: 28
Prop-content-length: 162
Content-length: 162

K 7
svn:log
V 61
PerlTidy the code to make it consistent with some formatting

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T20:11:13.237124Z
PROPS-END

Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12602
Text-content-md5: 6f2afff5fc995e458ef593d1ff29475c
Content-length: 12602

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.5.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}
close(IGNORE);

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;    # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;   # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig
  ;    # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags) {
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ( $tag =~ s/\*/ / ) {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            $token =
"<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>"
              . $token
              . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i ) {
            $token = $2;
            if ( !$1 =~ m|^\s*$| ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m:^.*?(\b$keyword\b).*?$:i
            || $tokdup =~ m:^.*?(\b$keyword\b).*?$:i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@.&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut




Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 13022
Text-content-md5: 381efe31ed8244f45348793c77b1a4b8
Content-length: 13022

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.5.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.5.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.5.0",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "[websec] - $today";
    &MailMessage( $linkmsg, $subj, $digestEmail );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};

# If block without URL, assume parameter setting block and update default values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq "" || $prefix eq "" || ( $email eq "" && $emailLink eq "" ) )
    {
        print "Name, prefix or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name - $today";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    print "Sending highlighted page to $email ...\n";
                    if ( $email ne "" ) {
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            $linkmsg =
"The contents of the following URL has changed:\n\n$url\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;
        if ( $email ne "" ) { MailMessage( $errmsg, $subj, $email ); }
        if ( $emailLink ne "" ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut



Revision-number: 29
Prop-content-length: 122
Content-length: 122

K 7
svn:log
V 21
Update documentation

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-02T20:17:12.488345Z
PROPS-END

Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 17196
Text-content-md5: 5f6bf0fc19eff500ffd961f877ed8734
Content-length: 17196

WEB SECRETARY Version 1.5.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux. Users report that it works on Windows too.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://www.nongnu.org/websec/


5. URL LIST

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
	             be used to bypass servers that prevent access based on the user
				 agent.
	

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


6. IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


7. IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


8. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


9. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


10. HISTORY

1.5.0 - Release on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.
* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.
* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com
* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


11. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Revision-number: 30
Prop-content-length: 236
Content-length: 236

K 7
svn:log
V 134
Templetize the script, instead of mentioning websec everywhere use a parameter so the script can easily be adapted for other projects

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-03T09:27:33.736789Z
PROPS-END

Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 474
Text-content-md5: 76e1c89b82081f50a532de8f1b4df139
Content-length: 474

#!/bin/sh

PROJ=websec
VERSION=$1
FILE=$PROJ-$VERSION.tar.gz
DIR=files/$PROJ.pkg/$VERSION/
BASEDIR=$PROJ-$VERSION/
TAG=version-$VERSION

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

svn export file:///home/svn/$PROJ/tags/$PROJ/$TAG $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	exit 1
fi

tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

svnadmin dump /home/svn/$PROJ | gzip -9 > files/svn/$PROJ.svn-dump.gz


Revision-number: 31
Prop-content-length: 181
Content-length: 181

K 7
svn:log
V 80
Ensure that install has the man pages generated (Patch 1449 by Dominik Stadler)

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T07:56:01.846977Z
PROPS-END

Node-path: trunk/websec/Makefile
Node-kind: file
Node-action: change
Text-content-length: 609
Text-content-md5: b5cb99adc86ca4d9e9d432bd53230fff
Content-length: 609

PREFIX=/usr/local
BINDIR=$(DESTDIR)$(PREFIX)/bin
MANDIR=$(DESTDIR)$(PREFIX)/share/man
DOCDIR=$(DESTDIR)$(PREFIX)/share/doc/websec

all: websec.1 webdiff.1

install: all
	install -d $(BINDIR)
	install websec $(BINDIR)/
	install webdiff $(BINDIR)/

	install -d $(MANDIR)/man1/
	install websec.1 $(MANDIR)/man1/
	install webdiff.1 $(MANDIR)/man1/

	install -d $(DOCDIR)
	install -d $(DOCDIR)/examples
	install -m 0644 url.list $(DOCDIR)/examples/
	install -m 0644 ignore.list $(DOCDIR)/examples/
	install -m 0644 README $(DOCDIR)/

clean:
	rm -f websec.1 webdiff.1

websec.1 webdiff.1 : %.1 : %
	pod2man $< > $@


Revision-number: 32
Prop-content-length: 223
Content-length: 223

K 7
svn:log
V 121
Apply rest of Patch 1449 by Dominik Stadler.
This patch adds the Program directive to run a program when a page changes.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T08:00:36.198964Z
PROPS-END

Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 17384
Text-content-md5: e1dec156511a3dadd7b8d63722ecd05c
Content-length: 17384

WEB SECRETARY Version 1.5.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux. Users report that it works on Windows too.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://www.nongnu.org/websec/


5. URL LIST

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
	             be used to bypass servers that prevent access based on the user
				 agent.
	

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


6. IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


7. IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


8. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


9. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


10. HISTORY

1.5.0 - Release on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.
* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.
* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com
* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


11. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 3431
Text-content-md5: 71f6de723dfbffe972f51d916b7920b0
Content-length: 3431

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://segfault.org/
Name = Segfault.org
Prefix = segfault
Hicolor = grey

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage



Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 14364
Text-content-md5: fd4b050b3eff6342f7690d4bc207f5fd
Content-length: 14364

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.5.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.5.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.5.0",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "[websec] - $today";
    &MailMessage( $linkmsg, $subj, $digestEmail );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};

# If block without URL, assume parameter setting block and update default values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name - $today";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    print "Sending highlighted page to $email ...\n";
                    if ( $email ne "" ) {
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            $linkmsg =
"The contents of the following URL has changed:\n\n$url\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;
        if ( $email ne "" ) { MailMessage( $errmsg, $subj, $email ); }
        if ( $emailLink ne "" ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut




Revision-number: 33
Prop-content-length: 176
Content-length: 176

K 7
svn:log
V 75
Use $addsubject instead of hardcoded "[websec]" (spotted by Trevor Boicey)

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T08:14:07.079492Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 14426
Text-content-md5: 6dcfd2fff76a72aac147291e46466dd5
Content-length: 14426

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.5.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.5.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.5.0",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
	if (! $addsubject eq "") {
		$subj = "$today";
	} else {
		$subj = "$addsubject - $today";
	}
    &MailMessage( $linkmsg, $subj, $digestEmail );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};

# If block without URL, assume parameter setting block and update default values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name - $today";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor =$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    print "Sending highlighted page to $email ...\n";
                    if ( $email ne "" ) {
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            $linkmsg =
"The contents of the following URL has changed:\n\n$url\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;
        if ( $email ne "" ) { MailMessage( $errmsg, $subj, $email ); }
        if ( $emailLink ne "" ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut




Revision-number: 34
Prop-content-length: 180
Content-length: 180

K 7
svn:log
V 79
Fix erroneous coloring of changes, the equal sign was added to the color name!

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T08:23:12.330696Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 14425
Text-content-md5: 7211c84e425464b4ca54e6d1f7649d4b
Content-length: 14425

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.5.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.5.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.5.0",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
	if (! $addsubject eq "") {
		$subj = "$today";
	} else {
		$subj = "$addsubject - $today";
	}
    &MailMessage( $linkmsg, $subj, $digestEmail );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};

# If block without URL, assume parameter setting block and update default values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name - $today";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    print "Sending highlighted page to $email ...\n";
                    if ( $email ne "" ) {
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            $linkmsg =
"The contents of the following URL has changed:\n\n$url\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;
        if ( $email ne "" ) { MailMessage( $errmsg, $subj, $email ); }
        if ( $emailLink ne "" ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut




Revision-number: 35
Prop-content-length: 123
Content-length: 123

K 7
svn:log
V 22
Add snapshot versions

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T08:25:23.624268Z
PROPS-END

Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 946
Text-content-md5: 3fbea436a2ab129b8a22af40d94786a7
Content-length: 946

#!/bin/sh

PROJ=chktex
VERSION=$1
DIR=files/$PROJ.pkg/$VERSION/
TAG=version-$VERSION
TAGFILE=file:///home/svn/$PROJ/tags/$PROJ/$TAG

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

if [ "$VERSION" = "snapshot" ]
then
	echo "Doing snapshot"
	VERSION=snapshot-`date +%Y.%m.%d`
	TAGFILE=file:///home/svn/$PROJ/trunk/$PROJ
	DIR=files/$PROJ.pkg/snapshot
	rm -f $DIR/*
fi

FILE=$PROJ-$VERSION.tar.gz
BASEDIR=$PROJ-$VERSION/

# Export clean copy of the repository
svn export $TAGFILE $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	rm -rf $BASEDIR
	exit 1
fi

# Prepare version for release
if [ -f $BASEDIR/autogen.sh ]
then
	pushd $BASEDIR
	./autogen.sh
	popd
fi

# Create tar.gz file
tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

svnadmin dump /home/svn/$PROJ | gzip -9 > files/svn/$PROJ.svn-dump.gz

echo "Now we GPG sign the release:"
pushd $DIR
gpg --detach --armor --sign $FILE
popd


Revision-number: 36
Prop-content-length: 216
Content-length: 216

K 7
svn:log
V 114
Add initial version of WebML site, this is a null generation version, there is nothing changed from the old site.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T09:05:05.022935Z
PROPS-END

Node-path: trunk/site
Node-kind: dir
Node-action: add
Prop-content-length: 10
Content-length: 10

PROPS-END


Node-path: trunk/site/Makefile
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 123
Text-content-md5: a2e6319c7854e2a5f164184810a30061
Content-length: 133

PROPS-END
SRC=index.wml
OUT=$(SRC:%.wml=%.html)

all:
	wmk

clean:
	-rm -f $(OUT)

install: all
	install -m 0644 $(OUT) ../realsite/


Node-path: trunk/site/indexl.html
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 2580
Text-content-md5: 5aa666a75b9191b1aafe9a47bf39ee81
Content-length: 2590

PROPS-END
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<title>WebSec - A Web Secretary</title>
</head>
<body>
<h1>WebSec - A Web Secretary</h1>
<h2>What is WebSec?</h2>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>
<h2>Where to Download?</h2>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
The latest version is
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/1.5.0/websec-1.5.0.tar.gz">1.5.0</a>
</p>
<p>A snapshot release with bug fixes and minor features is available as:
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/websec-snapshot-2003.05.04.tar.gz">websec snapshot 2003.05.04</a>
-
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/websec-snapshot-2003.05.04.tar.gz.asc">GPG Signature</a>.
</p>
<h2>Are there any dependencies?</h2>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
<h2>How to install?</h2>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
<h2>How to use?</h2>
<p>
Just run the program and will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
<h2>How do I get help? How do I help?</h2>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>
<h2>Who is the Author?</h2>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
<h2>Links</h2>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a>
<li><a href="https://savannah.nongnu.org/files/?group=websec">Download Websec</a>
</ul>
</body>
</html>


Node-path: trunk/site/indexl.wml
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 2590
Text-content-md5: ec382285787cd6e09be49c334cba1733
Content-length: 2600

PROPS-END
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<title>WebSec - A Web Secretary</title>
</head>
<body>
<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>

<h2>Where to Download?</h2>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
The latest version is
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/1.5.0/websec-1.5.0.tar.gz">1.5.0</a>
</p>
<p>A snapshot release with bug fixes and minor features is available as:
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/websec-snapshot-2003.05.04.tar.gz">websec snapshot 2003.05.04</a>
- 
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/websec-snapshot-2003.05.04.tar.gz.asc">GPG Signature</a>.
</p>

<h2>Are there any dependencies?</h2>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h2>How to install?</h2>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h2>How to use?</h2>
<p>
Just run the program and will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>

<h2>How do I get help? How do I help?</h2>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<h2>Who is the Author?</h2>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>

<h2>Links</h2>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a>
<li><a href="https://savannah.nongnu.org/files/?group=websec">Download Websec</a>
</ul>
</body>
</html>


Revision-number: 37
Prop-content-length: 158
Content-length: 158

K 7
svn:log
V 57
Rename indexl to index, remove HTML file it's generated!

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T09:07:05.926152Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: add
Node-copyfrom-rev: 36
Node-copyfrom-path: trunk/site/indexl.wml


Node-path: trunk/site/indexl.wml
Node-action: delete


Node-path: trunk/site/indexl.html
Node-action: delete


Revision-number: 38
Prop-content-length: 142
Content-length: 142

K 7
svn:log
V 41
Ignore *.html files, they are generated!

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T09:07:46.559839Z
PROPS-END

Node-path: trunk/site
Node-kind: dir
Node-action: change
Prop-content-length: 38
Content-length: 38

K 10
svn:ignore
V 7
*.html

PROPS-END


Revision-number: 39
Prop-content-length: 204
Content-length: 204

K 7
svn:log
V 102
First step toward automatic generation, create a function to generate the url from the version number

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T09:37:39.284728Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 2647
Text-content-md5: e090c626793f5e4ea8be8b57311baff5
Content-length: 2647

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<title>WebSec - A Web Secretary</title>
</head>
<body>
<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>

<h2>Where to Download?</h2>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

<:
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	sub dlurl
	{
		my $ver = shift(@_);
		my $dir = $ver;
		if ($ver eq "snapshot")
		{
			$ver = "snapshot-2003.05.04";
		}
		return "$dlbase/$dir/websec-$ver$archive";
	}
:>

The latest version is
<a href="<:=dlurl('1.5.0'):>">1.5.0</a>
</p>
<p>A snapshot release with bug fixes and minor features is available as:
<a href="<:=dlurl('snapshot'):>">websec snapshot 2003.05.04</a>
- 
<a href="<:=dlurl('snapshot'):>.asc">GPG Signature</a>.
</p>

<h2>Are there any dependencies?</h2>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h2>How to install?</h2>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h2>How to use?</h2>
<p>
Just run the program and will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>

<h2>How do I get help? How do I help?</h2>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<h2>Who is the Author?</h2>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>

<h2>Links</h2>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a>
<li><a href="https://savannah.nongnu.org/files/?group=websec">Download Websec</a>
</ul>
</body>
</html>


Revision-number: 40
Prop-content-length: 140
Content-length: 140

K 7
svn:log
V 39
True automatic generation of file list

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T10:13:49.853049Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 2775
Text-content-md5: e05d8a96003507fa7f22538d045bb74c
Content-length: 2775

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<title>WebSec - A Web Secretary</title>
</head>
<body>
<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>

<h2>Where to Download?</h2>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	sub dlurl
	{
		my $ver = shift(@_);
		my $dir = $ver;
		if ($ver eq "snapshot")
		{
			$ver = `ls $dirbase/snapshot/*.gz`;
			$ver =~ s/$archive//;
			$ver =~ s/.*\/websec-//;
		}
		return "$dlbase/$dir/websec-$ver$archive";
	}

	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) { print '<li><a href="', dlurl($ver), '">', $ver, '</a></li>'; }
	}
:>

<p>
The latest versions are:
<ul>
<:listversions():>
</ul>

<h2>Are there any dependencies?</h2>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h2>How to install?</h2>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h2>How to use?</h2>
<p>
Just run the program and will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>

<h2>How do I get help? How do I help?</h2>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<h2>Who is the Author?</h2>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>

<h2>Links</h2>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a>
<li><a href="https://savannah.nongnu.org/files/?group=websec">Download Websec</a>
</ul>
</body>
</html>


Revision-number: 41
Prop-content-length: 155
Content-length: 155

K 7
svn:log
V 54
Add more info depending on file such as GPG signature

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T10:35:33.083417Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 3474
Text-content-md5: fe22ed170f4f5e85101aaf5e90179c86
Content-length: 3474

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<title>WebSec - A Web Secretary</title>
</head>
<body>
<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>

<h2>Where to Download?</h2>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, '</a>';
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);

			print '<li>';
			print '<a href="', dlurl($ver,$file), '">', $ver, '</a>';
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print '</li>';
		}
	}
:>

<p>
The latest versions are:
<ul>
<:listversions():>
</ul>

<h2>Are there any dependencies?</h2>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h2>How to install?</h2>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h2>How to use?</h2>
<p>
Just run the program and will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>

<h2>How do I get help? How do I help?</h2>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<h2>Who is the Author?</h2>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>

<h2>Links</h2>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a>
<li><a href="https://savannah.nongnu.org/files/?group=websec">Download Websec</a>
</ul>
</body>
</html>


Revision-number: 42
Prop-content-length: 139
Content-length: 139

K 7
svn:log
V 38
Correct project after previous commit

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T17:19:50.398884Z
PROPS-END

Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 946
Text-content-md5: 6eb562c254ca58a1686acd54f991af2a
Content-length: 946

#!/bin/sh

PROJ=websec
VERSION=$1
DIR=files/$PROJ.pkg/$VERSION/
TAG=version-$VERSION
TAGFILE=file:///home/svn/$PROJ/tags/$PROJ/$TAG

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

if [ "$VERSION" = "snapshot" ]
then
	echo "Doing snapshot"
	VERSION=snapshot-`date +%Y.%m.%d`
	TAGFILE=file:///home/svn/$PROJ/trunk/$PROJ
	DIR=files/$PROJ.pkg/snapshot
	rm -f $DIR/*
fi

FILE=$PROJ-$VERSION.tar.gz
BASEDIR=$PROJ-$VERSION/

# Export clean copy of the repository
svn export $TAGFILE $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	rm -rf $BASEDIR
	exit 1
fi

# Prepare version for release
if [ -f $BASEDIR/autogen.sh ]
then
	pushd $BASEDIR
	./autogen.sh
	popd
fi

# Create tar.gz file
tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

svnadmin dump /home/svn/$PROJ | gzip -9 > files/svn/$PROJ.svn-dump.gz

echo "Now we GPG sign the release:"
pushd $DIR
gpg --detach --armor --sign $FILE
popd


Revision-number: 43
Prop-content-length: 154
Content-length: 154

K 7
svn:log
V 53
Add spaces to lines, so that diffs will look cleaner

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T17:20:32.023914Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 3482
Text-content-md5: 02147bdd3cffc10075562d6e24757e54
Content-length: 3482

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<title>WebSec - A Web Secretary</title>
</head>
<body>
<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>

<h2>Where to Download?</h2>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $ver, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
<ul>
<:listversions():>
</ul>

<h2>Are there any dependencies?</h2>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h2>How to install?</h2>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h2>How to use?</h2>
<p>
Just run the program and will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>

<h2>How do I get help? How do I help?</h2>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<h2>Who is the Author?</h2>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>

<h2>Links</h2>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a>
<li><a href="https://savannah.nongnu.org/files/?group=websec">Download Websec</a>
</ul>
</body>
</html>


Revision-number: 44
Prop-content-length: 156
Content-length: 156

K 7
svn:log
V 55
Make sure to recompile whenever the files list changes

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-04T17:20:50.336267Z
PROPS-END

Node-path: trunk/site/Makefile
Node-kind: file
Node-action: change
Text-content-length: 497
Text-content-md5: 8783509951f9f391fe668243aa84acf8
Content-length: 497

SRC=index.wml
OUT=$(SRC:%.wml=%.html)

all: files_changed run install

run:
	wmk

files_changed:
	@ls -R ../files | cat - Makefile | md5sum > md5sum.new
	@if [ ! -e "md5sum.old" ]; then \
		echo "No old md5sum file, recompiling everything."; \
		make clean; \
	else \
		if [ "`cat md5sum.old`" != "`cat md5sum.new`" ]; then \
			echo "File list changed, recompiling!"; \
			make clean; \
		fi ;\
	fi
	@mv md5sum.new md5sum.old

clean:
	-rm -f $(OUT)

install:
	install -m 0644 $(OUT) ../realsite/


Revision-number: 45
Prop-content-length: 195
Content-length: 195

K 7
svn:log
V 94
Add fixes from Alain to close IGNORE file and handle &nbsp; correctly.
(Bug 2913 on Savannah)

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-05T11:29:58.256692Z
PROPS-END

Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12627
Text-content-md5: f9900c16027173a82a124a0036f4d5e3
Content-length: 12627

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.5.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;    # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;   # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig
  ;    # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags) {
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ( $tag =~ s/\*/ / ) {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            $token =
"<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>"
              . $token
              . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i ) {
            $token = $2;
            if ( !$1 =~ m|^\s*$| ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m:^.*?(\b$keyword\b).*?$:i
            || $tokdup =~ m:^.*?(\b$keyword\b).*?$:i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut




Revision-number: 46
Prop-content-length: 239
Content-length: 239

K 7
svn:log
V 137
When emailing a link (EmailLink feature), add the file:/// link too so a user can easily access it from his browser.
(Savannah bug 3471)

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-05T16:01:47.333406Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 14578
Text-content-md5: afb0ff47af75539411ce30a065108b79
Content-length: 14578

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.5.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.5.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Get today's date in the format we want.
$today = strftime "%d %B %Y (%a)", localtime;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.5.0",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
	if (! $addsubject eq "") {
		$subj = "$today";
	} else {
		$subj = "$addsubject - $today";
	}
    &MailMessage( $linkmsg, $subj, $digestEmail );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};

# If block without URL, assume parameter setting block and update default values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name - $today";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
						print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
						print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
							my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;
        if ( $email ne "" ) { MailMessage( $errmsg, $subj, $email ); }
        if ( $emailLink ne "" ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut




Revision-number: 47
Prop-content-length: 167
Content-length: 167

K 7
svn:log
V 66
Add DateFMT feature to control the Date Format in E-Mail subjects

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-05T16:23:46.124130Z
PROPS-END

Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 3455
Text-content-md5: b15db0d375759fe867376c53e1098f6c
Content-length: 3455

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://segfault.org/
Name = Segfault.org
Prefix = segfault
Hicolor = grey

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"



Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 14594
Text-content-md5: 83ee4ea3c318b482fb75464f10b590f0
Content-length: 14594

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.5.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.5.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.5.0",
	DateFMT    => " - %d %B %Y (%a)",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
	$subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
						print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
						print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
							my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;
        if ( $email ne "" ) { MailMessage( $errmsg, $subj, $email ); }
        if ( $emailLink ne "" ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut




Revision-number: 48
Prop-content-length: 134
Content-length: 134

K 7
svn:log
V 33
Add ChangeLog generator utility.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-05T16:32:50.754621Z
PROPS-END

Node-path: trunk/utils/gnuify-changelog.pl
Node-kind: file
Node-action: add
Prop-content-length: 36
Text-content-length: 949
Text-content-md5: c41a6a04020d500d1639fc03ea39125f
Content-length: 985

K 14
svn:executable
V 1
*
PROPS-END
#!/usr/bin/perl -w

# a script to munge the output of 'svn log' into something approaching the 
# style of a GNU ChangeLog.
#
# to use this, just fill in the 'hackers' hash with the usernames and 
# name/emails of the people who work on your project, go to the top level 
# of your working copy, and run:
#
# $ svn log | /path/to/gnuify-changelog.pl > ChangeLog

%hackers = ( "baruch"      => 'Baruch Even <baruch@ev-en.org>', );

$parse_next_line = 0;

while (<>) {
  # axe windows style line endings, since we should try to be consistent, and 
  # the repos has both styles in it's log entries.
  $_ =~ s/\r\n$/\n/;

  if (/^-+$/) {
    # we're at the start of a log entry, so we need to parse the next line
    $parse_next_line = 1;
  } elsif ($parse_next_line) {
    # transform from svn style to GNU style
    $parse_next_line = 0;

    @parts = split (/ /, $_);

    print "$parts[5] $hackers{$parts[3]}\n";
  } else {
    print "\t$_";
  }
}


Revision-number: 49
Prop-content-length: 126
Content-length: 126

K 7
svn:log
V 25
Add ChangeLog to release

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-05T16:34:53.929313Z
PROPS-END

Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 1100
Text-content-md5: e58d22929d63ff796751fb33171b98d1
Content-length: 1100

#!/bin/sh

PROJ=websec
VERSION=$1
DIR=files/$PROJ.pkg/$VERSION/
TAG=version-$VERSION
TAGFILE=file:///home/svn/$PROJ/tags/$PROJ/$TAG

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

if [ "$VERSION" = "snapshot" ]
then
	echo "Doing snapshot"
	VERSION=snapshot-`date +%Y.%m.%d`
	TAGFILE=file:///home/svn/$PROJ/trunk/$PROJ
	DIR=files/$PROJ.pkg/snapshot
	rm -f $DIR/*
fi

FILE=$PROJ-$VERSION.tar.gz
BASEDIR=$PROJ-$VERSION/

# Export clean copy of the repository
svn export $TAGFILE $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	rm -rf $BASEDIR
	exit 1
fi

# Prepare version for release
if [ -f $BASEDIR/autogen.sh ]
then
	pushd $BASEDIR
	./autogen.sh
	popd
fi

# Create tar.gz file
tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

# Create a dump of the repository for online backup
svnadmin dump /home/svn/$PROJ | gzip -9 > files/svn/$PROJ.svn-dump.gz

# Sign the tar.gz
echo "Now we GPG sign the release:"
pushd $DIR
gpg --detach --armor --sign $FILE
popd

# Generate ChangeLog
svn log $TAGFILE | utils/gnuify-changelog.pl > $DIR/ChangeLog


Revision-number: 50
Prop-content-length: 127
Content-length: 127

K 7
svn:log
V 26
Version updates for 1.6.0

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-05T16:45:47.214462Z
PROPS-END

Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 17855
Text-content-md5: cee53c545e4ec5e5afb041190fd258e7
Content-length: 17855

WEB SECRETARY Version 1.6.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux. Users report that it works on Windows too.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://www.nongnu.org/websec/


5. URL LIST

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
	             be used to bypass servers that prevent access based on the user
				 agent.
	
	DateFMT    - Date format to use in e-mail messages, can be empty for no date.
	             Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.
	

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


6. IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


7. IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


8. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


9. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


10. HISTORY

1.6.0 - Released on 05 May 2003

* Added DateFMT feature to set Date Formats in mail subjects.

* Add to EmailLink the local file system link so that a user can open it in his
  browser.

* Add Program option to open a program on changed link instead of mailing
  notification.

* Multiple bug fixes.

1.5.0 - Released on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.

* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.

* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com

* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


11. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12627
Text-content-md5: df352c90206e0fe2ea2060a9d1f8e5f2
Content-length: 12627

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.6.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;    # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;   # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig
  ;    # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags) {
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ( $tag =~ s/\*/ / ) {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            $token =
"<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>"
              . $token
              . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i ) {
            $token = $2;
            if ( !$1 =~ m|^\s*$| ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m:^.*?(\b$keyword\b).*?$:i
            || $tokdup =~ m:^.*?(\b$keyword\b).*?$:i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut




Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 14594
Text-content-md5: ed2dc904d3742184284fecd16b598f35
Content-length: 14594

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.6.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.6.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.6.0",
	DateFMT    => " - %d %B %Y (%a)",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
	$subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
						print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
						print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
							my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;
        if ( $email ne "" ) { MailMessage( $errmsg, $subj, $email ); }
        if ( $emailLink ne "" ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut




Revision-number: 51
Prop-content-length: 119
Content-length: 119

K 7
svn:log
V 18
Tag version 1.6.0

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-05T16:46:20.235407Z
PROPS-END

Node-path: tags/websec/version-1.6.0
Node-kind: dir
Node-action: add
Node-copyfrom-rev: 50
Node-copyfrom-path: trunk/websec


Revision-number: 52
Prop-content-length: 137
Content-length: 137

K 7
svn:log
V 36
Automate full upload of new version

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-05T16:50:07.917771Z
PROPS-END

Node-path: trunk/utils/upload
Node-kind: file
Node-action: change
Text-content-length: 203
Text-content-md5: e6c04b39960c3ab8c1e715f137cb42e6
Content-length: 203

#!/bin/sh

# Upload all files
rsync -alvzC --delete -e ssh files/ baruch@freesoftware.fsf.org:/upload/websec/

# Make new site
make -C site

# Upload new site
pushd realsite
cvs ci -m 'New version'
popd


Revision-number: 53
Prop-content-length: 156
Content-length: 156

K 7
svn:log
V 55
Move subversion repository dump from release to upload

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-06T05:51:13.292607Z
PROPS-END

Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 977
Text-content-md5: b6eff7d30986a47d3ee8107cb0981d6e
Content-length: 977

#!/bin/sh

PROJ=websec
VERSION=$1
DIR=files/$PROJ.pkg/$VERSION/
TAG=version-$VERSION
TAGFILE=file:///home/svn/$PROJ/tags/$PROJ/$TAG

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

if [ "$VERSION" = "snapshot" ]
then
	echo "Doing snapshot"
	VERSION=snapshot-`date +%Y.%m.%d`
	TAGFILE=file:///home/svn/$PROJ/trunk/$PROJ
	DIR=files/$PROJ.pkg/snapshot
	rm -f $DIR/*
fi

FILE=$PROJ-$VERSION.tar.gz
BASEDIR=$PROJ-$VERSION/

# Export clean copy of the repository
svn export $TAGFILE $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	rm -rf $BASEDIR
	exit 1
fi

# Prepare version for release
if [ -f $BASEDIR/autogen.sh ]
then
	pushd $BASEDIR
	./autogen.sh
	popd
fi

# Create tar.gz file
tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

# Sign the tar.gz
echo "Now we GPG sign the release:"
pushd $DIR
gpg --detach --armor --sign $FILE
popd

# Generate ChangeLog
svn log $TAGFILE | utils/gnuify-changelog.pl > $DIR/ChangeLog


Node-path: trunk/utils/upload
Node-kind: file
Node-action: change
Text-content-length: 401
Text-content-md5: d175f674a21863998b22b4d5e4599190
Content-length: 401

#!/bin/sh

PROJ=websec

#TODO: Check if everything is committed in $PROJ and site

# Create a dump of the repository for online backup
svnadmin dump /home/svn/$PROJ | gzip -9 > files/svn/$PROJ.svn-dump.gz

# Upload all files
rsync -alvzC --delete -e ssh files/ baruch@freesoftware.fsf.org:/upload/websec/

# Make new site
make -C site

# Upload new site
pushd realsite
cvs ci -m 'Update website'
popd


Revision-number: 54
Prop-content-length: 167
Content-length: 167

K 7
svn:log
V 66
Small changes to site, add timestamp on page and a few more links

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-06T05:51:52.299916Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 4288
Text-content-md5: 9292cd92b799d2f9f12a0d1f1f8eecb7
Content-length: 4288

#use wml::std::info
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<title>WebSec - A Web Secretary</title>
<info style=meta>
</head>
<body>
<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>

<h2>Where to Download?</h2>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $ver, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
<ul>
<:listversions():>
</ul>

<h2>Are there any dependencies?</h2>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h2>How to install?</h2>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h2>How to use?</h2>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>

<h2>How do I get help? How do I help?</h2>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</p>

<h2>Who is the Author?</h2>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>

<h2>Links</h2>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>

<hr> 
Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISOTIME />.

</body>
</html>


Revision-number: 55
Prop-content-length: 139
Content-length: 139

K 7
svn:log
V 38
Add some meta information to the page

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-06T06:16:53.568021Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 4599
Text-content-md5: 09df039deb3da4b6c31c154f440648e2
Content-length: 4599

#use wml::std::info
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<title>WebSec - A Web Secretary</title>
<info style=meta>
<meta NAME=description CONTENT="Web Secretary, tracks web page changes for you">
<link REL="parent" TITLE="Project Page" HREF="http://nongnu.org/projects/websec/">
<link REL="author" HREF="http://baruch.ev-en.org/">
<!--<link REL="shortcut icon" HREF="/favicon.ico" TYPE="image/x-icon">-->
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>

<h2>Where to Download?</h2>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $ver, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
<ul>
<:listversions():>
</ul>

<h2>Are there any dependencies?</h2>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h2>How to install?</h2>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h2>How to use?</h2>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>

<h2>How do I get help? How do I help?</h2>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</p>

<h2>Who is the Author?</h2>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>

<h2>Links</h2>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>

<hr> 
<font size=-2>Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISOTIME /></font>

</body>
</html>


Revision-number: 56
Prop-content-length: 126
Content-length: 126

K 7
svn:log
V 25
Optimize the output HTML

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-06T06:17:07.120240Z
PROPS-END

Node-path: trunk/site/Makefile
Node-kind: file
Node-action: change
Text-content-length: 510
Text-content-md5: 74fe8c2a5333bd6a3ffb96b85b262ccf
Content-length: 510

SRC=index.wml
OUT=$(SRC:%.wml=%.html)

all: files_changed run install

run:
	wmk --optimize=4

files_changed:
	@ls -R ../files | cat - Makefile | md5sum > md5sum.new
	@if [ ! -e "md5sum.old" ]; then \
		echo "No old md5sum file, recompiling everything."; \
		make clean; \
	else \
		if [ "`cat md5sum.old`" != "`cat md5sum.new`" ]; then \
			echo "File list changed, recompiling!"; \
			make clean; \
		fi ;\
	fi
	@mv md5sum.new md5sum.old

clean:
	-rm -f $(OUT)

install:
	install -m 0644 $(OUT) ../realsite/


Revision-number: 57
Prop-content-length: 293
Content-length: 293

K 7
svn:log
V 191
Switch subversion repository dump to bzip2 compression, much smaller file.
Change order of actions, do all local actions first, so that if something fails, we will update nothing in the net.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-06T06:24:49.993635Z
PROPS-END

Node-path: trunk/utils/upload
Node-kind: file
Node-action: change
Text-content-length: 589
Text-content-md5: 93395598bc8af8d4a87b373578e37d7c
Content-length: 589

#!/bin/sh

PROJ=websec

### Do local changes first ###

#TODO: Check if everything is committed in $PROJ and site

# Create a dump of the repository for online backup
# bzip2 create very small files
# Best after that is gzip --rsyncable not for size as much as for smaller rsync uploads
svnadmin dump /home/svn/$PROJ | bzip2 -9 > files/svn/$PROJ.svn-dump.bz2

# Make new site
make -C site

### Update remote sites later ###

# Upload all files
rsync -alvC --delete -e ssh files/ baruch@freesoftware.fsf.org:/upload/websec/

# Upload new site
pushd realsite
cvs ci -m 'Update website'
popd


Revision-number: 58
Prop-content-length: 168
Content-length: 168

K 7
svn:log
V 67
Make it possible to do local update only without actuall uploading

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-06T06:34:10.260028Z
PROPS-END

Node-path: trunk/utils/upload
Node-kind: file
Node-action: change
Text-content-length: 651
Text-content-md5: c0f75918e2e23ec5ac821ffac1be2dac
Content-length: 651

#!/bin/sh

PROJ=websec

### Do local changes first ###

#TODO: Check if everything is committed in $PROJ and site

# Create a dump of the repository for online backup
# bzip2 create very small files
# Best after that is gzip --rsyncable not for size as much as for smaller rsync uploads
svnadmin dump /home/svn/$PROJ | bzip2 -9 > files/svn/$PROJ.svn-dump.bz2

# Make new site
make -C site

if [ ! -z "$1" ];
then
	echo "Local updates only"
	exit 0
fi

### Update remote sites later ###

# Upload all files
rsync -alvC --delete -e ssh files/ baruch@freesoftware.fsf.org:/upload/websec/

# Upload new site
pushd realsite
cvs ci -m 'Update website'
popd


Revision-number: 59
Prop-content-length: 122
Content-length: 122

K 7
svn:log
V 21
Use $PROJ everywhere

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-06T08:54:36.813155Z
PROPS-END

Node-path: trunk/utils/upload
Node-kind: file
Node-action: change
Text-content-length: 650
Text-content-md5: 2d2412186deb4939833a082116dbb88e
Content-length: 650

#!/bin/sh

PROJ=websec

### Do local changes first ###

#TODO: Check if everything is committed in $PROJ and site

# Create a dump of the repository for online backup
# bzip2 create very small files
# Best after that is gzip --rsyncable not for size as much as for smaller rsync uploads
svnadmin dump /home/svn/$PROJ | bzip2 -9 > files/svn/$PROJ.svn-dump.bz2

# Make new site
make -C site

if [ ! -z "$1" ];
then
	echo "Local updates only"
	exit 0
fi

### Update remote sites later ###

# Upload all files
rsync -alvC --delete -e ssh files/ baruch@freesoftware.fsf.org:/upload/$PROJ/

# Upload new site
pushd realsite
cvs ci -m 'Update website'
popd


Revision-number: 60
Prop-content-length: 123
Content-length: 123

K 7
svn:log
V 22
Add a Penguin favicon

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-06T10:37:36.138855Z
PROPS-END

Node-path: trunk/site/favicon.ico
Node-kind: file
Node-action: add
Prop-content-length: 59
Text-content-length: 2238
Text-content-md5: ba345d9200ac136addc51ba877aca6d7
Content-length: 2297

K 13
svn:mime-type
V 24
application/octet-stream
PROPS-END
               (       @          	                    l{ Ft     Bhq  ~|| .46 	N_  s ]n I ,    NOP ;65  %%% "KV     } - fnp G=; T   S            +1 +   s  (1  b  
   h dba Vpx  u }ur d EU "iz &U_  y   4lz   Yy  r\W pnn   YYY      l ;NS HEE   Xk ' 3 )`l ^LI   ~c]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^	
^^^^^^^^^^^^^^USSS	^^ S!"^^^^#SSSSSS$%%%%%%&'SSSS()^^^*SSSSSSS
+,%%%%%%-.SSSS/^^^012SSS2^34%%%%%%-522SR^^^^67@8E9>%%%%%%%-:!;/!(<^^^^^^^^020=>%%%%%%%%>?@;^^/A^^^^^^^^8B4O%%%%%%%%%%CDE^3/^^^^^^^^^^^FG,%%%%%%%%%%%%ONI,3^^^^^^^^^^^^B^%%%%%%%%%%%%4^^^^^^^^^^^^33B%%%%%%%%%%%%I3R4^^^^^^^^^^^^^^^RH%%%%%%%%%%%I	J^3^^^^^^^^^^^^	4%%%%%%%%%%%GJ^^^^^^^^^^^^^^RJK%%%%%%%%%,#^^^^^^^^^^^^^^^	3&%%%%%%%%OG	J^^^^^^^^^^^^^^^^^#%%%%%%%%KR3^^^^^^^^^^^^^^^^^^JK%LM%%%%I3^3^^^^^^^^^^^^^^^^^^^,N;
IO%H^^J^^^^^^^^^^^^^^^^^^^^	
@@8PQR6^^^^^^^^^^^^^^^^^^^^^;ST U	G^^^^^^^^^^^^^^^^^^^^^XV WX^3^^^^^^^^^^^^^^^^^^^^^^^Y<WZ[\#^^^^^^^^^^^^^^^^^^^^^^^4F]4K&^^^^^^^^^^^^^^^^^^^^^^^^%\^%>^	^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^J^^^^^^^^^^^^^^^^^^^^^^^^^^33^^^^^^^^^^^^^^^^^^^^^^^^^^^#RR6^^^^^^^^^^^^^^^^^^^^^^^^3^^#G^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^?                          ?  ?    

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 4592
Text-content-md5: 7c96595889c058ae0bf42c696eeaf80c
Content-length: 4592

#use wml::std::info
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<title>WebSec - A Web Secretary</title>
<info style=meta>
<meta NAME=description CONTENT="Web Secretary, tracks web page changes for you">
<link REL="parent" TITLE="Project Page" HREF="http://nongnu.org/projects/websec/">
<link REL="author" HREF="http://baruch.ev-en.org/">
<link REL="shortcut icon" HREF="/favicon.ico" TYPE="image/x-icon">
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>

<h2>Where to Download?</h2>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $ver, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
<ul>
<:listversions():>
</ul>

<h2>Are there any dependencies?</h2>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h2>How to install?</h2>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h2>How to use?</h2>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>

<h2>How do I get help? How do I help?</h2>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</p>

<h2>Who is the Author?</h2>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>

<h2>Links</h2>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>

<hr> 
<font size=-2>Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISOTIME /></font>

</body>
</html>


Revision-number: 61
Prop-content-length: 119
Content-length: 119

K 7
svn:log
V 18
Ignore md5sum.old

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-06T10:38:00.879158Z
PROPS-END

Node-path: trunk/site
Node-kind: dir
Node-action: change
Prop-content-length: 50
Content-length: 50

K 10
svn:ignore
V 18
*.html
md5sum.old

PROPS-END


Revision-number: 62
Prop-content-length: 129
Content-length: 129

K 7
svn:log
V 28
Correct location of favicon

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-06T10:41:50.690749Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 4591
Text-content-md5: 04bd58e4ae3609ddf2865ce2a5ca3706
Content-length: 4591

#use wml::std::info
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<title>WebSec - A Web Secretary</title>
<info style=meta>
<meta NAME=description CONTENT="Web Secretary, tracks web page changes for you">
<link REL="parent" TITLE="Project Page" HREF="http://nongnu.org/projects/websec/">
<link REL="author" HREF="http://baruch.ev-en.org/">
<link REL="shortcut icon" HREF="favicon.ico" TYPE="image/x-icon">
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>

<h2>Where to Download?</h2>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $ver, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
<ul>
<:listversions():>
</ul>

<h2>Are there any dependencies?</h2>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h2>How to install?</h2>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h2>How to use?</h2>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>

<h2>How do I get help? How do I help?</h2>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</p>

<h2>Who is the Author?</h2>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>

<h2>Links</h2>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>

<hr> 
<font size=-2>Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISOTIME /></font>

</body>
</html>


Revision-number: 63
Prop-content-length: 136
Content-length: 136

K 7
svn:log
V 35
Add RandomWait and fix remove tabs

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-06T12:38:54.921239Z
PROPS-END

Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 18182
Text-content-md5: 3c3c7c0310daec5f50d9942c3cb98665
Content-length: 18182

WEB SECRETARY Version 1.6.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux. Users report that it works on Windows too.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://www.nongnu.org/websec/


5. URL LIST

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


6. IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


7. IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


8. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


9. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


10. HISTORY

1.6.0 - Released on 05 May 2003

* Added DateFMT feature to set Date Formats in mail subjects.

* Add to EmailLink the local file system link so that a user can open it in his
  browser.

* Add Program option to open a program on changed link instead of mailing
  notification.

* Multiple bug fixes.

1.5.0 - Released on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.

* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.

* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com

* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


11. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Revision-number: 64
Prop-content-length: 145
Content-length: 145

K 7
svn:log
V 44
Use vim modelines to avoid tabs in the code

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-07T03:37:43.547405Z
PROPS-END

Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12643
Text-content-md5: d89c810d1646fc9cc6a3dab8c8a109df
Content-length: 12643

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.6.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;    # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;   # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig
  ;    # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags) {
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ( $tag =~ s/\*/ / ) {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            $token =
"<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>"
              . $token
              . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i ) {
            $token = $2;
            if ( !$1 =~ m|^\s*$| ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m:^.*?(\b$keyword\b).*?$:i
            || $tokdup =~ m:^.*?(\b$keyword\b).*?$:i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut

vim:set et ts=4:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 14610
Text-content-md5: 2377690edf5242e9612296c8fa572739
Content-length: 14610

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.6.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.6.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.6.0",
	DateFMT    => " - %d %B %Y (%a)",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
	$subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
						print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
						print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
							my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;
        if ( $email ne "" ) { MailMessage( $errmsg, $subj, $email ); }
        if ( $emailLink ne "" ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 65
Prop-content-length: 205
Content-length: 205

K 7
svn:log
V 103
Add MailFrom feature, set the address from which the mail will appear to come (Thanks to Adam Stanley)

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-07T03:42:13.828107Z
PROPS-END

Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 18462
Text-content-md5: e577905cb1d2d2296b84eb479ceab180
Content-length: 18462

WEB SECRETARY Version 1.6.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux. Users report that it works on Windows too.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://www.nongnu.org/websec/


5. URL LIST

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


6. IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


7. IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


8. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


9. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


10. HISTORY

1.7 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

1.6.0 - Released on 05 May 2003

* Added DateFMT feature to set Date Formats in mail subjects.

* Add to EmailLink the local file system link so that a user can open it in his
  browser.

* Add Program option to open a program on changed link instead of mailing
  notification.

* Multiple bug fixes.

1.5.0 - Released on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.

* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.

* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com

* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


11. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 3548
Text-content-md5: 045eda1b443ee1d7777bea8140cae019
Content-length: 3548

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://segfault.org/
Name = Segfault.org
Prefix = segfault
Hicolor = grey

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"



Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 15307
Text-content-md5: d0ea8bc6cdfbdd57806d08f39f730aaa
Content-length: 15307

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.6.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.6.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.6.0",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;
        if ( $email ne "" ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 66
Prop-content-length: 137
Content-length: 137

K 7
svn:log
V 36
Add Vim example with proper ignores

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-07T08:16:01.071740Z
PROPS-END

Node-path: trunk/websec/ignore.list
Node-kind: file
Node-action: change
Text-content-length: 905
Text-content-md5: e47e5c9a1f4b1f6ee607e780ae16034a
Content-length: 905

[General]
all rights reserved
an error occurred
click here
comments
copyright
daily articles for
details
discussion forum
downloads
in issues
last modified
last updated
maintained
posted
posted at
previous cartoon
search by
special offer
the current week
total votes
visits
votes

[Date_Time]
January \d+
February \d+
March \d+
April \d+
May \d+
June \d+
July \d+
August \d+
September \d+
October \d+
November \d+
December \d+
Jan \d+
Feb \d+
Mar \d+
Apr \d+
Jun \d+
Jul \d+
Aug \d+
Sep \d+
Oct \d+
Nov \d+
Dec \d+
\d+ January
\d+ February
\d+ March
\d+ April
\d+ May
\d+ June
\d+ July
\d+ August
\d+ September
\d+ October
\d+ November
\d+ December
\d+ Jan
\d+ Feb
\d+ Mar
\d+ Apr
\d+ Jun
\d+ Jul
\d+ Aug
\d+ Sep
\d+ Oct
\d+ Nov
\d+ Dec
\d+\/\d+\/\d+

[Adverts]
http://www.news.com/cgi-bin/acc_clickthru
http://ads2.zdnet.com/adverts/

[VIM]
[\d,]+ scripts, [\d,]+ downloads
[\d,]+ tips, [\d,]+ tip views


Revision-number: 67
Prop-content-length: 138
Content-length: 138

K 7
svn:log
V 37
Add ViM example, use the ViM ignores

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-07T19:53:44.227138Z
PROPS-END

Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 3629
Text-content-md5: 73a41f8c59520f75822a0e5bd237b4f1
Content-length: 3629

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://segfault.org/
Name = Segfault.org
Prefix = segfault
Hicolor = grey

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM


Revision-number: 68
Prop-content-length: 154
Content-length: 154

K 7
svn:log
V 53
Transform webpage into XHTML and add appropriate CSS

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-08T17:25:14.070469Z
PROPS-END

Node-path: trunk/site/Makefile
Node-kind: file
Node-action: change
Text-content-length: 575
Text-content-md5: 6f3bf7de047c74ff6b44f3401237d80b
Content-length: 575

SRC=index.wml
STATIC_SRC=favicon.ico websec.css
OUT=$(SRC:%.wml=%.html)

all: files_changed run install

run:
	#wmk --optimize=4
	wmk -W2,-X1034

files_changed:
	@ls -R ../files | cat - Makefile | md5sum > md5sum.new
	@if [ ! -e "md5sum.old" ]; then \
		echo "No old md5sum file, recompiling everything."; \
		make clean; \
	else \
		if [ "`cat md5sum.old`" != "`cat md5sum.new`" ]; then \
			echo "File list changed, recompiling!"; \
			make clean; \
		fi ;\
	fi
	@mv md5sum.new md5sum.old

clean:
	-rm -f $(OUT)

install:
	install -m 0644 $(OUT) $(STATIC_SRC) ../realsite/


Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 4842
Text-content-md5: bc44758b815275c6341df17d5045b644
Content-length: 4842

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">



<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>

<h2>Where to Download?</h2>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $ver, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
</p>
<ul>
<:listversions():>
</ul>

<h2>Are there any dependencies?</h2>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h2>How to install?</h2>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h2>How to use?</h2>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>

<h2>How do I get help? How do I help?</h2>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>

<h2>Who is the Author?</h2>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>

<h2>Links</h2>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISOTIME /></p>

</body>
</html>


Node-path: trunk/site/websec.css
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 624
Text-content-md5: afac6b2c7252aeb5b43108ff0c5dff6f
Content-length: 634

PROPS-END
body { 
  color: black; 
  background-color: white; 
}

/*
p + p {
  text-indent: 1em;
}
*/

hr {
  height: 2pt;
}

a img {
  background: transparent;
  color: white;
  border: 0;
}

h2 {
	width: 95%;
	margin-left: auto;
	margin-right: auto;
	border: solid thin; 
	background: #ddd;
	text-align: center;
	font-size: larger;
}


h1, h2, .center, .addresses, .notice {
  text-indent: 0;
  text-align: center;
  margin-left: 0%;
}

.bottomtext { 
  font-size: smaller; 
  text-align: right; 
}

.addresses {
  background-color: #ffffcc;
  width: 50%;
  margin-left: 25%;
}

.notice {
  color: red;
  background-color: white;
}


Revision-number: 69
Prop-content-length: 135
Content-length: 135

K 7
svn:log
V 34
Make sure to validate xhtml files

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-08T17:40:14.325770Z
PROPS-END

Node-path: trunk/site/Makefile
Node-kind: file
Node-action: change
Text-content-length: 596
Text-content-md5: 68250d94ca0cfb105e685552bef74a71
Content-length: 596

SRC=index.wml
STATIC_SRC=favicon.ico websec.css
OUT=$(SRC:%.wml=%.html)

all: files_changed run install

run:
	#wmk --optimize=4
	wmk -W2,-X1034
	validate index.html

files_changed:
	@ls -R ../files | cat - Makefile | md5sum > md5sum.new
	@if [ ! -e "md5sum.old" ]; then \
		echo "No old md5sum file, recompiling everything."; \
		make clean; \
	else \
		if [ "`cat md5sum.old`" != "`cat md5sum.new`" ]; then \
			echo "File list changed, recompiling!"; \
			make clean; \
		fi ;\
	fi
	@mv md5sum.new md5sum.old

clean:
	-rm -f $(OUT)

install:
	install -m 0644 $(OUT) $(STATIC_SRC) ../realsite/


Revision-number: 70
Prop-content-length: 149
Content-length: 149

K 7
svn:log
V 48
Make the text more indented than the gray boxes

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-08T17:40:43.912921Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 5082
Text-content-md5: 8b82b20c24f13e99e36d37af8cbb08ac
Content-length: 5082

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">



<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<div class=subsection>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>
</div>

<h2>Where to Download?</h2>
<div class=subsection>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $ver, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
</p>
<ul>
<:listversions():>
</ul>
</div>

<h2>Are there any dependencies?</h2>
<div class=subsection>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
</div>

<h2>How to install?</h2>
<div class=subsection>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
</div>

<h2>How to use?</h2>
<div class=subsection>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</div>

<h2>How do I get help? How do I help?</h2>
<div class=subsection>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</div>

<h2>Who is the Author?</h2>
<div class=subsection>
<p>
The original author is Chew Wei Yih, with the help of several contributors (see
the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</div>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISOTIME /></p>

</body>
</html>


Node-path: trunk/site/websec.css
Node-kind: file
Node-action: change
Text-content-length: 706
Text-content-md5: 44f8848401e75aa54674120d229085ef
Content-length: 706

body { 
  color: black; 
  background-color: white; 
}

/*
p + p {
  text-indent: 1em;
}
*/

hr {
  height: 2pt;
}

a img {
  background: transparent;
  color: white;
  border: 0;
}

h2, div.header {
	width: 95%;
	margin-left: auto;
	margin-right: auto;
	border: solid thin; 
	background: #ddd;
	text-align: center;
	font-size: larger;
}

.subsection {
	width: 90%;
	margin-left: auto;
	margin-right: auto;
}

h1, h2, .center, .addresses, .notice {
  text-indent: 0;
  text-align: center;
  margin-left: 0%;
}

.bottomtext { 
  font-size: smaller; 
  text-align: right; 
}

.addresses {
  background-color: #ffffcc;
  width: 50%;
  margin-left: 25%;
}

.notice {
  color: red;
  background-color: white;
}


Revision-number: 71
Prop-content-length: 211
Content-length: 211

K 7
svn:log
V 109
Change formatting a bit
 - Make a big install section out of three sections
 - Print snapshot with it's date

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-08T18:34:33.144755Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 5233
Text-content-md5: 10e3a3fe9a764fc792ff60f42ce1eeb0
Content-length: 5233

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">



<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<div class=subsection>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>
</div>

<h2>Download</h2>
<div class=subsection>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);
			$text = $ver;
			if ($text eq 'snapshot') {
				$text = $file;
				$text =~ s/.*(snapshot-\d+\.\d+\.\d+).*/$1/;
			}

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $text, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
</p>
<ul>
<:listversions():>
</ul>
</div>

<h2>Installation</h2>

<h3>Are there any dependencies?</h3>
<div class=subsection>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
</div>

<h3>How to install?</h3>
<div class=subsection>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
</div>

<h3>How to use?</h3>
<div class=subsection>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</div>

<h2>How do I get help? How do I help?</h2>
<div class=subsection>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</div>

<h2>Authors</h2>
<div class=subsection>
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</div>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISOTIME /></p>

</body>
</html>


Revision-number: 72
Prop-content-length: 139
Content-length: 139

K 7
svn:log
V 38
Only output date of last modification

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-08T19:36:00.986143Z
PROPS-END

Node-path: trunk/site/Makefile
Node-kind: file
Node-action: change
Text-content-length: 633
Text-content-md5: ea87ffb3d556af18b74bd7b993633b7e
Content-length: 633

SRC=index.wml
STATIC_SRC=favicon.ico websec.css
OUT=$(SRC:%.wml=%.html)

all: files_changed run install

run:
	#wmk --optimize=4
	wmk -W2,-X1034 -D"WML_GEN_ISODATE~`date +%Y-%m-%d`"
	validate index.html

files_changed:
	@ls -R ../files | cat - Makefile | md5sum > md5sum.new
	@if [ ! -e "md5sum.old" ]; then \
		echo "No old md5sum file, recompiling everything."; \
		make clean; \
	else \
		if [ "`cat md5sum.old`" != "`cat md5sum.new`" ]; then \
			echo "File list changed, recompiling!"; \
			make clean; \
		fi ;\
	fi
	@mv md5sum.new md5sum.old

clean:
	-rm -f $(OUT)

install:
	install -m 0644 $(OUT) $(STATIC_SRC) ../realsite/


Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 5233
Text-content-md5: ecb2be73d841ea4d0b6bec4a20227449
Content-length: 5233

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">



<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<div class=subsection>
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>
</div>

<h2>Download</h2>
<div class=subsection>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);
			$text = $ver;
			if ($text eq 'snapshot') {
				$text = $file;
				$text =~ s/.*(snapshot-\d+\.\d+\.\d+).*/$1/;
			}

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $text, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
</p>
<ul>
<:listversions():>
</ul>
</div>

<h2>Installation</h2>

<h3>Are there any dependencies?</h3>
<div class=subsection>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
</div>

<h3>How to install?</h3>
<div class=subsection>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
</div>

<h3>How to use?</h3>
<div class=subsection>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</div>

<h2>How do I get help? How do I help?</h2>
<div class=subsection>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</div>

<h2>Authors</h2>
<div class=subsection>
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</div>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Revision-number: 73
Prop-content-length: 125
Content-length: 125

K 7
svn:log
V 24
Add URL to mail subject

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-09T19:12:30.430277Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 15314
Text-content-md5: 9888fc503f490f019a04c62b7aca528e
Content-length: 15314

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.6.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.6.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.6.0",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;
        if ( $email ne "" ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 74
Prop-content-length: 150
Content-length: 150

K 7
svn:log
V 49
Added comment to README about the url in subject

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-09T19:14:21.483880Z
PROPS-END

Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 18563
Text-content-md5: cd872a70c12bc0e857abd0b86c36a066
Content-length: 18563

WEB SECRETARY Version 1.6.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux. Users report that it works on Windows too.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://www.nongnu.org/websec/


5. URL LIST

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


6. IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


7. IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


8. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


9. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


10. HISTORY

1.7 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
	  the mail folder.

1.6.0 - Released on 05 May 2003

* Added DateFMT feature to set Date Formats in mail subjects.

* Add to EmailLink the local file system link so that a user can open it in his
  browser.

* Add Program option to open a program on changed link instead of mailing
  notification.

* Multiple bug fixes.

1.5.0 - Released on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.

* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.

* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com

* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


11. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Revision-number: 75
Prop-content-length: 192
Content-length: 192

K 7
svn:log
V 91
Add EmailError option so the user can choose if he wants to receive error messages or not.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-09T19:31:46.764933Z
PROPS-END

Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 18750
Text-content-md5: f6049e4ca53760d08aa6cac89fd46826
Content-length: 18750

WEB SECRETARY Version 1.6.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux. Users report that it works on Windows too.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://www.nongnu.org/websec/


5. URL LIST

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


6. IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


7. IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


8. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


9. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


10. HISTORY

1.7 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
	  the mail folder.

    * Added an EmailError option to disable sending error messages. This was
	  requested numerous times (Bug#3498 on Savannah), this fix is based on a
	  patch from Peter Bieringer.
	  
1.6.0 - Released on 05 May 2003

* Added DateFMT feature to set Date Formats in mail subjects.

* Add to EmailLink the local file system link so that a user can open it in his
  browser.

* Add Program option to open a program on changed link instead of mailing
  notification.

* Multiple bug fixes.

1.5.0 - Released on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.

* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.

* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com

* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


11. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 3734
Text-content-md5: a488c569599136be963d5cd43c194399
Content-length: 3734

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://segfault.org/
Name = Segfault.org
Prefix = segfault
Hicolor = grey

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 15386
Text-content-md5: dae43faf89fcce9937f5990176dbe9d9
Content-length: 15386

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.6.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.6.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.6.0",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;

    # Ignore comments
    if (m/^#.*?$/) { next; }

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 76
Prop-content-length: 347
Content-length: 347

K 7
svn:log
V 245
Create manpages from documentation in the README, also moved the documentation to the files themselves to localize changes.
To accomplish all this added the __END__ finish identifier to url.list and ignore.list and added comments to ignore.list

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-10T08:56:20.195425Z
PROPS-END

Node-path: trunk/websec
Node-kind: dir
Node-action: change
Prop-content-length: 46
Content-length: 46

K 10
svn:ignore
V 14
*.swp
*.[1-9]

PROPS-END


Node-path: trunk/websec/Makefile
Node-kind: file
Node-action: change
Text-content-length: 912
Text-content-md5: f4317c6a75d0d9314cfea268283835cd
Content-length: 912

PREFIX=/usr/local
BINDIR=$(DESTDIR)$(PREFIX)/bin
MANDIR=$(DESTDIR)$(PREFIX)/share/man
MAN1DIR=$(MANDIR)/man1
MAN5DIR=$(MANDIR)/man5
DOCDIR=$(DESTDIR)$(PREFIX)/share/doc/websec

PROGSRC=websec webdiff
CONFSRC=url.list ignore.list

PROGMAN=$(PROGSRC:%=%.1)
CONFMAN=$(CONFSRC:%=%.5)

# Generated files
GENFILES=$(PROGMAN) $(CONFMAN)

all: $(GENFILES)

install: all
	install -d $(BINDIR)
	install websec $(BINDIR)/
	install webdiff $(BINDIR)/

	install -d $(MAN1DIR)
	install websec.1 $(MAN1DIR)
	install webdiff.1 $(MAN1DIR)

	install -d $(MAN5DIR)
	install url.list.5 $(MAN5DIR)
	install ignore.list.5 $(MAN5DIR)

	install -d $(DOCDIR)
	install -d $(DOCDIR)/examples
	install -m 0644 url.list $(DOCDIR)/examples/
	install -m 0644 ignore.list $(DOCDIR)/examples/
	install -m 0644 README $(DOCDIR)/

clean:
	rm -f $(GENFILES)

$(PROGMAN) : %.1 : %
	pod2man $< > $@

$(CONFMAN) : %.5 : %
	pod2man --section 5 $< > $@


Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 11643
Text-content-md5: 1249128512a2d9738cf2216300381e51
Content-length: 11643

WEB SECRETARY Version 1.6.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux. Users report that it works on Windows too.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://www.nongnu.org/websec/


5. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


6. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


7. HISTORY

1.7 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
	  the mail folder.

    * Added an EmailError option to disable sending error messages. This was
	  requested numerous times (Bug#3498 on Savannah), this fix is based on a
	  patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
	  from the README file and moved it to the files themselves for easier
	  reference.

1.6.0 - Released on 05 May 2003

* Added DateFMT feature to set Date Formats in mail subjects.

* Add to EmailLink the local file system link so that a user can open it in his
  browser.

* Add Program option to open a program on changed link instead of mailing
  notification.

* Multiple bug fixes.

1.5.0 - Released on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.

* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.

* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com

* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


8. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Node-path: trunk/websec/ignore.list
Node-kind: file
Node-action: change
Text-content-length: 3788
Text-content-md5: 58a9b90ab4a775e8f70bc19e25d51e3d
Content-length: 3788

[General]
all rights reserved
an error occurred
click here
comments
copyright
daily articles for
details
discussion forum
downloads
in issues
last modified
last updated
maintained
posted
posted at
previous cartoon
search by
special offer
the current week
total votes
visits
votes

[Date_Time]
January \d+
February \d+
March \d+
April \d+
May \d+
June \d+
July \d+
August \d+
September \d+
October \d+
November \d+
December \d+
Jan \d+
Feb \d+
Mar \d+
Apr \d+
Jun \d+
Jul \d+
Aug \d+
Sep \d+
Oct \d+
Nov \d+
Dec \d+
\d+ January
\d+ February
\d+ March
\d+ April
\d+ May
\d+ June
\d+ July
\d+ August
\d+ September
\d+ October
\d+ November
\d+ December
\d+ Jan
\d+ Feb
\d+ Mar
\d+ Apr
\d+ Jun
\d+ Jul
\d+ Aug
\d+ Sep
\d+ Oct
\d+ Nov
\d+ Dec
\d+\/\d+\/\d+

[Adverts]
http://www.news.com/cgi-bin/acc_clickthru
http://ads2.zdnet.com/adverts/

[VIM]
[\d,]+ scripts, [\d,]+ downloads
[\d,]+ tips, [\d,]+ tip views


__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

=head2 IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


=head2 IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut



Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 8460
Text-content-md5: 7f8c310b4182f73e08fde20ce2f6b4fa
Content-length: 8460

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://segfault.org/
Name = Segfault.org
Prefix = segfault
Hicolor = grey

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut



Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12955
Text-content-md5: 88cb969c205b13253fc934e026eef0a0
Content-length: 12955

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.6.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;    # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;   # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig
  ;    # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags) {
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ( $tag =~ s/\*/ / ) {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            $token =
"<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=$hicolor>"
              . $token
              . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i ) {
            $token = $2;
            if ( !$1 =~ m|^\s*$| ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m:^.*?(\b$keyword\b).*?$:i
            || $tokdup =~ m:^.*?(\b$keyword\b).*?$:i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut

vim:set et ts=4:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 15451
Text-content-md5: 2700acc12927c50f635012e8ebc04c51
Content-length: 15451

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.6.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.6.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.6.0",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 77
Prop-content-length: 146
Content-length: 146

K 7
svn:log
V 45
Abort if there is an error while making site

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-10T08:59:46.157024Z
PROPS-END

Node-path: trunk/utils/upload
Node-kind: file
Node-action: change
Text-content-length: 714
Text-content-md5: 90a76ba7319b64e270f855bb2a920be2
Content-length: 714

#!/bin/sh

PROJ=websec

### Do local changes first ###

#TODO: Check if everything is committed in $PROJ and site

# Create a dump of the repository for online backup
# bzip2 create very small files
# Best after that is gzip --rsyncable not for size as much as for smaller rsync uploads
svnadmin dump /home/svn/$PROJ | bzip2 -9 > files/svn/$PROJ.svn-dump.bz2

# Make new site
make -C site
if [ $? != 0 ];
then
	echo "Error while making site"
	exit 1
fi

if [ ! -z "$1" ];
then
	echo "Local updates only"
	exit 0
fi

### Update remote sites later ###

# Upload all files
rsync -alvC --delete -e ssh files/ baruch@freesoftware.fsf.org:/upload/$PROJ/

# Upload new site
pushd realsite
cvs ci -m 'Update website'
popd


Revision-number: 78
Prop-content-length: 132
Content-length: 132

K 7
svn:log
V 31
Try to be quieter when working

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-10T09:06:01.563799Z
PROPS-END

Node-path: trunk/utils/upload
Node-kind: file
Node-action: change
Text-content-length: 814
Text-content-md5: f3cfbc9bec8c980ba0ada37f1afe7d3a
Content-length: 814

#!/bin/sh

PROJ=websec

### Do local changes first ###

#TODO: Check if everything is committed in $PROJ and site

# Create a dump of the repository for online backup
# bzip2 create very small files
# Best after that is gzip --rsyncable not for size as much as for smaller rsync uploads
echo "Dump source control repository"
svnadmin dump -q /home/svn/$PROJ | bzip2 -9 > files/svn/$PROJ.svn-dump.bz2

# Make new site
echo "Update site with new release"
make -C site -s
if [ $? != 0 ];
then
	echo "Error while making site"
	exit 1
fi

if [ ! -z "$1" ];
then
	echo "Local updates only, upload is skipped."
	exit 0
fi

### Update remote sites later ###

# Upload all files
rsync -alvC --delete -e ssh files/ baruch@freesoftware.fsf.org:/upload/$PROJ/

# Upload new site
pushd realsite
cvs ci -m 'Update website'
popd


Revision-number: 79
Prop-content-length: 171
Content-length: 171

K 7
svn:log
V 70
When doing a release, automatically update the site and the dump file

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-10T09:06:18.901365Z
PROPS-END

Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 1023
Text-content-md5: f17d6fbb20007dc51afe586fcfd00bf1
Content-length: 1023

#!/bin/sh

PROJ=websec
VERSION=$1
DIR=files/$PROJ.pkg/$VERSION/
TAG=version-$VERSION
TAGFILE=file:///home/svn/$PROJ/tags/$PROJ/$TAG

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

if [ "$VERSION" = "snapshot" ]
then
	echo "Doing snapshot"
	VERSION=snapshot-`date +%Y.%m.%d`
	TAGFILE=file:///home/svn/$PROJ/trunk/$PROJ
	DIR=files/$PROJ.pkg/snapshot
	rm -f $DIR/*
fi

FILE=$PROJ-$VERSION.tar.gz
BASEDIR=$PROJ-$VERSION/

# Export clean copy of the repository
svn export $TAGFILE $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	rm -rf $BASEDIR
	exit 1
fi

# Prepare version for release
if [ -f $BASEDIR/autogen.sh ]
then
	pushd $BASEDIR
	./autogen.sh
	popd
fi

# Create tar.gz file
tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

# Sign the tar.gz
echo "Now we GPG sign the release:"
pushd $DIR
gpg --detach --armor --sign $FILE
popd

# Generate ChangeLog
svn log $TAGFILE | utils/gnuify-changelog.pl > $DIR/ChangeLog

# Locally update the site
utils/upload local


Revision-number: 80
Prop-content-length: 137
Content-length: 137

K 7
svn:log
V 36
Move NEWS stuff to NEWS from README

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-10T09:45:20.327248Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 7622
Text-content-md5: bea877a8e78e757284b496bf94a5104c
Content-length: 7632

PROPS-END
1.7 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
	  the mail folder.

    * Added an EmailError option to disable sending error messages. This was
	  requested numerous times (Bug#3498 on Savannah), this fix is based on a
	  patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
	  from the README file and moved it to the files themselves for easier
	  reference.

1.6.0 - Released on 05 May 2003

* Added DateFMT feature to set Date Formats in mail subjects.

* Add to EmailLink the local file system link so that a user can open it in his
  browser.

* Add Program option to open a program on changed link instead of mailing
  notification.

* Multiple bug fixes.

1.5.0 - Released on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.

* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.

* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com

* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 4007
Text-content-md5: eeea233f18a5e6da68653be107964920
Content-length: 4007

WEB SECRETARY Version 1.6.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux. Users report that it works on Windows too.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://www.nongnu.org/websec/


5. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


6. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


7. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Revision-number: 81
Prop-content-length: 157
Content-length: 157

K 7
svn:log
V 56
Mention ActiveState Perl for running websec in windows.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-10T19:51:34.716223Z
PROPS-END

Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 4030
Text-content-md5: d678d43f88e25473e53869569c52bd92
Content-length: 4030

WEB SECRETARY Version 1.6.0


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux. Users report that it works on Windows too using ActiveState Perl.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://www.nongnu.org/websec/


5. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


6. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


7. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Revision-number: 82
Prop-content-length: 270
Content-length: 270

K 7
svn:log
V 168
Add "ascii" color highlighting from Javier M. Mora. Instead of using color to highlight it uses ascii characters around the change.
(Patch 1463 in Savannah - Modified)

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-10T20:46:55.074130Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 7782
Text-content-md5: 6f798a34a78a8477cff789b50050d869
Content-length: 7782

1.7 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

1.6.0 - Released on 05 May 2003

* Added DateFMT feature to set Date Formats in mail subjects.

* Add to EmailLink the local file system link so that a user can open it in his
  browser.

* Add Program option to open a program on changed link instead of mailing
  notification.

* Multiple bug fixes.

1.5.0 - Released on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.

* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.

* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com

* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.


Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 8636
Text-content-md5: 78c40fb5cc97b7baf2bb41a20d38a9d2
Content-length: 8636

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://segfault.org/
Name = Segfault.org
Prefix = segfault
Hicolor = grey

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    AsciiMarker - If set to 1 it will add ascii markers around the changes so
	             that highlighting is noticeable in text mode too. Useful for
				 text MUAs users.

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut



Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 13125
Text-content-md5: 3f31348bd9afc64040df51f02c27118f
Content-length: 13125

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.6.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$asciimarker = 0;
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "asciimarker"  => \$asciimarker,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s:[\r\n]|\s\s: :sig;    # Handle MSDOS-style line separators
$newpage =~ s:[\r\n]|\s\s: :sig;
$oldpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;   # Handle non-breaking white space
$newpage =~ s:&nbsp;:\@\@\@\@&nbsp;~~~~:sig;
$oldpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig
  ;    # Handle nested brackets
$newpage =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>:~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@:sig;
foreach (@tags) {
    $tag = $_;
    $oldpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    $newpage =~ s:<(/*$tag)>:~~~~$1\@\@\@\@:sig;
    if ( $tag =~ s/\*/ / ) {
        $oldpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
        $newpage =~ s:<(/*$tag.*?)>:~~~~$1\@\@\@\@:sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            if ($asciimarker) {
                $token = "###>>>". $token ."<<<###";
            }
            $token =
                    "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR>".
                    "<TD BGCOLOR=$hicolor>" . $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m|^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$|i ) {
            $token = $2;
            if ( !$1 =~ m|^\s*$| ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s:\@\@\@\@&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m:^.*?(\b$keyword\b).*?$:i
            || $tokdup =~ m:^.*?(\b$keyword\b).*?$:i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s:\@\@\@\@&nbsp;~~~~: :sig;
    $tokdup =~ s:~~~~:<:sig;
    $tokdup =~ s:\@\@\@\@:>:sig;
    $tokdup =~ s:<A(\s+[^>]*)<([^>]*)>([^>])*>::sig;
    $tokdup =~ s:<[^>]*>::sig;
    $tokdup =~ s:^\s*::sig;
    $tokdup =~ s:\s*$::sig;
    $tokdup =~ s:\s+: :sig;
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m:~~~~A.*?HREF=.*?$url.*?\@\@\@\@:i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m:~~~~/A\@\@\@\@:i;
    return 0;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut

vim:set et ts=4:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 15552
Text-content-md5: f58b4a0c04a4e0dc84a1eec4f71b3d8d
Content-length: 15552

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.6.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.6.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.6.0",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz.

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 83
Prop-content-length: 177
Content-length: 177

K 7
svn:log
V 76
Create the man pages in the release package, makes it easier for the users.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-10T20:52:38.233322Z
PROPS-END

Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 1086
Text-content-md5: 5d6f3a2b8949556eb95bad04986d2b5a
Content-length: 1086

#!/bin/sh

PROJ=websec
VERSION=$1
DIR=files/$PROJ.pkg/$VERSION/
TAG=version-$VERSION
TAGFILE=file:///home/svn/$PROJ/tags/$PROJ/$TAG

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

if [ "$VERSION" = "snapshot" ]
then
	echo "Doing snapshot"
	VERSION=snapshot-`date +%Y.%m.%d`
	TAGFILE=file:///home/svn/$PROJ/trunk/$PROJ
	DIR=files/$PROJ.pkg/snapshot
	rm -f $DIR/*
fi

FILE=$PROJ-$VERSION.tar.gz
BASEDIR=$PROJ-$VERSION/

# Export clean copy of the repository
svn export $TAGFILE $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	rm -rf $BASEDIR
	exit 1
fi

# Prepare version for release
if [ -f $BASEDIR/autogen.sh ]
then
	pushd $BASEDIR
	./autogen.sh
	popd
elif [ -f $BASEDIR/Makefile ]
then
	pushd $BASEDIR
	make
	popd
fi

# Create tar.gz file
tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

# Sign the tar.gz
echo "Now we GPG sign the release:"
pushd $DIR
gpg --detach --armor --sign $FILE
popd

# Generate ChangeLog
svn log $TAGFILE | utils/gnuify-changelog.pl > $DIR/ChangeLog

# Locally update the site
utils/upload local


Revision-number: 84
Prop-content-length: 142
Content-length: 142

K 7
svn:log
V 41
Add reference to config files man pages.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-10T21:01:26.584015Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 15586
Text-content-md5: 5c109de1a2f471d67c58ff4a8061c75f
Content-length: 15586

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.6.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.6.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.6.0",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    if ( $value =~ m/^\${([^}]+)}/ ) {
        $value = $ENV{$1};
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 85
Prop-content-length: 142
Content-length: 142

K 7
svn:log
V 41
Break upload script to update and upload

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-11T06:15:15.383790Z
PROPS-END

Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 1080
Text-content-md5: 41060d48866d266daba59e9da63c14fc
Content-length: 1080

#!/bin/sh

PROJ=websec
VERSION=$1
DIR=files/$PROJ.pkg/$VERSION/
TAG=version-$VERSION
TAGFILE=file:///home/svn/$PROJ/tags/$PROJ/$TAG

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

if [ "$VERSION" = "snapshot" ]
then
	echo "Doing snapshot"
	VERSION=snapshot-`date +%Y.%m.%d`
	TAGFILE=file:///home/svn/$PROJ/trunk/$PROJ
	DIR=files/$PROJ.pkg/snapshot
	rm -f $DIR/*
fi

FILE=$PROJ-$VERSION.tar.gz
BASEDIR=$PROJ-$VERSION/

# Export clean copy of the repository
svn export $TAGFILE $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	rm -rf $BASEDIR
	exit 1
fi

# Prepare version for release
if [ -f $BASEDIR/autogen.sh ]
then
	pushd $BASEDIR
	./autogen.sh
	popd
elif [ -f $BASEDIR/Makefile ]
then
	pushd $BASEDIR
	make
	popd
fi

# Create tar.gz file
tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

# Sign the tar.gz
echo "Now we GPG sign the release:"
pushd $DIR
gpg --detach --armor --sign $FILE
popd

# Generate ChangeLog
svn log $TAGFILE | utils/gnuify-changelog.pl > $DIR/ChangeLog

# Locally update the site
utils/update


Node-path: trunk/utils/update
Node-kind: file
Node-action: add
Prop-content-length: 36
Text-content-length: 187
Text-content-md5: d43d47187a63e4b3ca2936dd5b2fded7
Content-length: 223

K 14
svn:executable
V 1
*
PROPS-END
#!/bin/sh

PROJ=websec

# Upload new site
pushd realsite
cvs ci -m 'Update website'
popd

# Upload all files
rsync -alvC --delete -e ssh files/ baruch@freesoftware.fsf.org:/upload/$PROJ/


Node-path: trunk/utils/upload
Node-kind: file
Node-action: change
Text-content-length: 501
Text-content-md5: 8ec4095d4a03bdc2747d2f0bffb03d42
Content-length: 501

#!/bin/sh

PROJ=websec

#TODO: Check if everything is committed in $PROJ and site

# Create a dump of the repository for online backup
# bzip2 create very small files
# Best after that is gzip --rsyncable not for size as much as for smaller rsync uploads
echo "Dump source control repository"
svnadmin dump -q /home/svn/$PROJ | bzip2 -9 > files/svn/$PROJ.svn-dump.bz2

# Make new site
echo "Update site with new release"
make -C site -s
if [ $? != 0 ];
then
	echo "Error while making site"
	exit 1
fi


Revision-number: 86
Prop-content-length: 128
Content-length: 128

K 7
svn:log
V 27
Mixed up update and upload

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-11T06:17:09.552312Z
PROPS-END

Node-path: trunk/utils/update
Node-kind: file
Node-action: change
Text-content-length: 501
Text-content-md5: 8ec4095d4a03bdc2747d2f0bffb03d42
Content-length: 501

#!/bin/sh

PROJ=websec

#TODO: Check if everything is committed in $PROJ and site

# Create a dump of the repository for online backup
# bzip2 create very small files
# Best after that is gzip --rsyncable not for size as much as for smaller rsync uploads
echo "Dump source control repository"
svnadmin dump -q /home/svn/$PROJ | bzip2 -9 > files/svn/$PROJ.svn-dump.bz2

# Make new site
echo "Update site with new release"
make -C site -s
if [ $? != 0 ];
then
	echo "Error while making site"
	exit 1
fi


Node-path: trunk/utils/upload
Node-kind: file
Node-action: change
Text-content-length: 187
Text-content-md5: d43d47187a63e4b3ca2936dd5b2fded7
Content-length: 187

#!/bin/sh

PROJ=websec

# Upload new site
pushd realsite
cvs ci -m 'Update website'
popd

# Upload all files
rsync -alvC --delete -e ssh files/ baruch@freesoftware.fsf.org:/upload/$PROJ/


Revision-number: 87
Prop-content-length: 146
Content-length: 146

K 7
svn:log
V 45
Provide better description of what WebSec is

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-11T06:20:14.470002Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 6205
Text-content-md5: c20a5453c5b55de9e823f52a8f6a7f27
Content-length: 6205

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<div class=subsection>
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>
</div>

<h2>Download</h2>
<div class=subsection>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);
			$text = $ver;
			if ($text eq 'snapshot') {
				$text = $file;
				$text =~ s/.*(snapshot-\d+\.\d+\.\d+).*/$1/;
			}

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $text, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
</p>
<ul>
<:listversions():>
</ul>
</div>

<h2>Installation</h2>

<h3>Are there any dependencies?</h3>
<div class=subsection>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
</div>

<h3>How to install?</h3>
<div class=subsection>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
</div>

<h3>How to use?</h3>
<div class=subsection>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</div>

<h2>How do I get help? How do I help?</h2>
<div class=subsection>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</div>

<h2>Authors</h2>
<div class=subsection>
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</div>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Revision-number: 88
Prop-content-length: 134
Content-length: 134

K 7
svn:log
V 33
Add example of highlighted change
K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-11T06:32:48.345633Z
PROPS-END

Node-path: trunk/site
Node-kind: dir
Node-action: change
Prop-content-length: 54
Content-length: 54

K 10
svn:ignore
V 22
index.html
md5sum.old

PROPS-END


Node-path: trunk/site/Makefile
Node-kind: file
Node-action: change
Text-content-length: 722
Text-content-md5: abc77a77e4fe8c073c78cff45f1f1669
Content-length: 722

SRC=index.wml
STATIC_SRC=favicon.ico websec.css websec-homepage.old.html \
	   websec-homepage.new.html websec-homepage.highlighted.html
OUT=$(SRC:%.wml=%.html)

all: files_changed run install

run:
	#wmk --optimize=4
	wmk -W2,-X1034 -D"WML_GEN_ISODATE~`date +%Y-%m-%d`"
	validate index.html

files_changed:
	@ls -R ../files | cat - Makefile | md5sum > md5sum.new
	@if [ ! -e "md5sum.old" ]; then \
		echo "No old md5sum file, recompiling everything."; \
		make clean; \
	else \
		if [ "`cat md5sum.old`" != "`cat md5sum.new`" ]; then \
			echo "File list changed, recompiling!"; \
			make clean; \
		fi ;\
	fi
	@mv md5sum.new md5sum.old

clean:
	-rm -f $(OUT)

install:
	install -m 0644 $(OUT) $(STATIC_SRC) ../realsite/


Node-path: trunk/site/websec-homepage.highlighted.html
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 5881
Text-content-md5: 4fad03080c44cce03248adde76d8ff9a
Content-length: 5891

PROPS-END

<!-- X-URL: http://www.nongnu.org/websec/ -->

<BASE HREF= "http://www.nongnu.org/websec/">

<?xml version="1.0" encoding="iso-8859-1"?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<title>
WebSec - A Web Secretary
</title>

<meta name="description" content="Web Secretary, tracks web page changes for you" />

<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />

<link rel="author" href="http://baruch.ev-en.org/" />

<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />

<link rel="stylesheet" href="websec.css" />

</head>

<body>

<h1>
WebSec - A Web Secretary
</h1>

<h2>
What is WebSec?
</h2>

<div class="subsection">

<p>
<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=#66ccff>Web Secretary is a web page monitoring software. However, it goes beyond the normal functionalities offered by such software. It will detect changes based on content analysis, making sure that it's not just HTML that changed, but actual content. You can tell it what to ignore in the page (hit counters and such), and it can mail you the document with the changes highlighted or load the highlighted page in a browser.</TD></TR></TABLE>
</p>

<p>
<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=#66ccff>Web Secretary is actually a suite of two Perl scripts called websec and webdiff. websec retrieves web pages and email them to you based on a URL list that you provide. webdiff compares two web pages (current and archive) and creates a new page based on the current page but with all the differences highlighted using a predefined color.</TD></TR></TABLE>
</p>

<p>
<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR><TD BGCOLOR=#66ccff>Personally, I put Web Secretary on crontab to monitor a large number of web pages. When the highlighted pages are delivered to me, I use procmail to sort them out and file them into another folder. Sometimes, when I am busy, I will not have time to accessing the web for a few days. However, with Web Secretary, I can always access the "archive" that it has created for me at my own leisure.</TD></TR></TABLE>
</p>

</div>

<h2>
Download
</h2>

<div class="subsection">

<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<p>
The latest versions are:
</p>

<ul>

<li>
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/1.6.0/websec-1.6.0.tar.gz">1.6.0</a>  - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/1.6.0/websec-1.6.0.tar.gz.asc">GPG Signature</a>  - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/1.6.0/ChangeLog">ChangeLog</a>
</li>

<li>
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/websec-snapshot-2003.05.11.tar.gz">snapshot-2003.05.11</a>  - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/websec-snapshot-2003.05.11.tar.gz.asc">GPG Signature</a>  - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/ChangeLog">ChangeLog</a>
</li>

</ul>

</div>

<h2>
Installation
</h2>

<h3>
Are there any dependencies?
</h3>

<div class="subsection">

<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

</div>

<h3>
How to install?
</h3>

<div class="subsection">

<p>
Simply unpack the archive and modify the config file to your hearts content. There is no GUI to configure this program, it's all in the text files.
</p>

</div>

<h3>
How to use?
</h3>

<div class="subsection">

<p>
Just run the program and it will do its magic, the best mode would be to put it in a cron job for automatic daily work, this is great if you are connected all the time.
</p>

<p>
If you are connected by dialup, you may want to make it run automatically upon connection, how to do this is different between OSes &amp; Distributions so exact instructions you will need to find on your own.
</p>

</div>

<h2>
How do I get help? How do I help?
</h2>

<div class="subsection">

<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>. Post messages to request help and offer help. You can suggest ideas and even provide patches to implement them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary, as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>

<ul>

<li>
<a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs
</li>

<li>
<a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone
</li>

</ul>

</div>

<h2>
Authors
</h2>

<div class="subsection">

<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help of several contributors (see the README file).
</p>

<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when it was mostly unmaintained to give it a new public home on <a href="http://savannah.gnu.org/">Savannah</a>.
</p>

</div>

<h2>
Links
</h2>

<div class="subsection">

<ul>

<li>
<a href="http://homemade.hypermart.net/websec/">Old Homepage</a>
</li>

<li>
<a href="http://savannah.nongnu.org/projects/websec/">Project Page</a>
</li>

<li>
<a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a>
</li>

<li>
<a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a>
</li>

<li>
<a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a>
</li>

</ul>

</div>

<hr />

<p class="bottomtext">
Last modified by Baruch Even on 2003-05-11
</p>

</body>

</html>



Node-path: trunk/site/websec-homepage.new.html
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 5512
Text-content-md5: bdda92392f01d91324aceb21f361d826
Content-length: 5522

PROPS-END
<!-- X-URL: http://www.nongnu.org/websec/ -->
<BASE HREF= "http://www.nongnu.org/websec/">
<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>WebSec - A Web Secretary</title>
<meta name="description" content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>
<h1>WebSec - A Web Secretary</h1>
<h2>What is WebSec?</h2>
<div class="subsection">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>
</div>
<h2>Download</h2>
<div class="subsection">
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>
<p>
The latest versions are:
</p>
<ul>
<li>
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/1.6.0/websec-1.6.0.tar.gz">1.6.0</a>
 - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/1.6.0/websec-1.6.0.tar.gz.asc">GPG Signature</a>
 - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/1.6.0/ChangeLog">ChangeLog</a>
</li>
<li>
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/websec-snapshot-2003.05.11.tar.gz">snapshot-2003.05.11</a>
 - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/websec-snapshot-2003.05.11.tar.gz.asc">GPG Signature</a>
 - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/ChangeLog">ChangeLog</a>
</li>
</ul>
</div>
<h2>Installation</h2>
<h3>Are there any dependencies?</h3>
<div class="subsection">
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
</div>
<h3>How to install?</h3>
<div class="subsection">
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
</div>
<h3>How to use?</h3>
<div class="subsection">
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</div>
<h2>How do I get help? How do I help?</h2>
<div class="subsection">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>
<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>
<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</div>
<h2>Authors</h2>
<div class="subsection">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>
<h2>Links</h2>
<div class="subsection">
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</div>
<hr />
<p class="bottomtext">Last modified by Baruch Even on 2003-05-11</p>
</body>
</html>


Node-path: trunk/site/websec-homepage.old.html
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 4538
Text-content-md5: a4106ceebc15519217370ba30970f31e
Content-length: 4548

PROPS-END
<!-- X-URL: http://www.nongnu.org/websec/ -->
<BASE HREF= "http://www.nongnu.org/websec/">
<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>WebSec - A Web Secretary</title>
<meta name="description" content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>
<h1>WebSec - A Web Secretary</h1>
<h2>What is WebSec?</h2>
<div class="subsection">
<p>
WebSec is a program to go over a list of sites that you provide and check if any
of the sites changed. It can then send e-mail on a change and even send the HTML
with the changes highlighted.
</p>
</div>
<h2>Download</h2>
<div class="subsection">
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>
<p>
The latest versions are:
</p>
<ul>
<li>
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/1.6.0/websec-1.6.0.tar.gz">1.6.0</a>
 - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/1.6.0/websec-1.6.0.tar.gz.asc">GPG Signature</a>
 - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/1.6.0/ChangeLog">ChangeLog</a>
</li>
<li>
<a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/websec-snapshot-2003.05.11.tar.gz">snapshot-2003.05.11</a>
 - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/websec-snapshot-2003.05.11.tar.gz.asc">GPG Signature</a>
 - <a href="http://savannah.nongnu.org/download/websec/websec.pkg/snapshot/ChangeLog">ChangeLog</a>
</li>
</ul>
</div>
<h2>Installation</h2>
<h3>Are there any dependencies?</h3>
<div class="subsection">
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
</div>
<h3>How to install?</h3>
<div class="subsection">
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
</div>
<h3>How to use?</h3>
<div class="subsection">
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</div>
<h2>How do I get help? How do I help?</h2>
<div class="subsection">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>
<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>
<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</div>
<h2>Authors</h2>
<div class="subsection">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>
<h2>Links</h2>
<div class="subsection">
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</div>
<hr />
<p class="bottomtext">Last modified by Baruch Even on 2003-05-11</p>
</body>
</html>


Revision-number: 89
Prop-content-length: 127
Content-length: 127

K 7
svn:log
V 26
Add "Competitors" section

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-12T06:16:29.058710Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 7291
Text-content-md5: c3184d6cb5407798b8c30ddd92b08194
Content-length: 7291

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<div class=subsection>
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>
</div>

<h2>Download</h2>
<div class=subsection>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);
			$text = $ver;
			if ($text eq 'snapshot') {
				$text = $file;
				$text =~ s/.*(snapshot-\d+\.\d+\.\d+).*/$1/;
			}

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $text, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
</p>
<ul>
<:listversions():>
</ul>
</div>

<h2>Installation</h2>

<h3>Are there any dependencies?</h3>
<div class=subsection>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
</div>

<h3>How to install?</h3>
<div class=subsection>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
</div>

<h3>How to use?</h3>
<div class=subsection>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</div>

<h2>How do I get help? How do I help?</h2>
<div class=subsection>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</div>

<h2>Authors</h2>
<div class=subsection>
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</div>

<h2>"Competitors"</h2>
<div class=subsection>
<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
  <p>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
  </p>
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
  <p>
    Checks only timestamps and not actual page changes. It's main advanges
    is being written in Python.
  </p>
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
  <p>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
  </p>
  <p>
   There apears to be two wrep's, from two different authors, with totally
   different methods of work.
  </p>
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd><p>A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.</p></dd>
</dl>
</div>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Revision-number: 90
Prop-content-length: 152
Content-length: 152

K 7
svn:log
V 51
Add bits from kwebwatch 0.9.6 to ignore a bit more

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-12T06:29:33.565483Z
PROPS-END

Node-path: trunk/websec/ignore.list
Node-kind: file
Node-action: change
Text-content-length: 3821
Text-content-md5: 1a8179a135827ee27601148c0b687fdc
Content-length: 3821

[General]
all rights reserved
an error occurred
click here
comments
copyright
daily articles for
details
discussion forum
downloads
in issues
last modified
last updated
maintained
posted
posted at
previous cartoon
search by
special offer
the current week
total votes
visits
votes
copyright

[Date_Time]
January \d+
February \d+
March \d+
April \d+
May \d+
June \d+
July \d+
August \d+
September \d+
October \d+
November \d+
December \d+
Jan \d+
Feb \d+
Mar \d+
Apr \d+
Jun \d+
Jul \d+
Aug \d+
Sep \d+
Oct \d+
Nov \d+
Dec \d+
\d+ January
\d+ February
\d+ March
\d+ April
\d+ May
\d+ June
\d+ July
\d+ August
\d+ September
\d+ October
\d+ November
\d+ December
\d+ Jan
\d+ Feb
\d+ Mar
\d+ Apr
\d+ Jun
\d+ Jul
\d+ Aug
\d+ Sep
\d+ Oct
\d+ Nov
\d+ Dec
\d+\/\d+\/\d+

[Adverts]
http://www.news.com/cgi-bin/acc_clickthru
http://ads2.zdnet.com/adverts/
http://doublclick4.net

[VIM]
[\d,]+ scripts, [\d,]+ downloads
[\d,]+ tips, [\d,]+ tip views


__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

=head2 IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


=head2 IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut



Revision-number: 91
Prop-content-length: 117
Content-length: 117

K 7
svn:log
V 16
Remove dead url

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-12T06:38:59.980234Z
PROPS-END

Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 8555
Text-content-md5: 1288b416fee47615ae98e018060884df
Content-length: 8555

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    AsciiMarker - If set to 1 it will add ascii markers around the changes so
	             that highlighting is noticeable in text mode too. Useful for
				 text MUAs users.

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut



Revision-number: 92
Prop-content-length: 157
Content-length: 157

K 7
svn:log
V 56
Simplify ignore lists and add ignoring for cvsweb stuff

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-12T17:27:53.980579Z
PROPS-END

Node-path: trunk/websec/ignore.list
Node-kind: file
Node-action: change
Text-content-length: 3728
Text-content-md5: 7190f2a586e4c6027d8b68758bc575b7
Content-length: 3728

[General]
all rights reserved
an error occurred
click here
comments
copyright
daily articles for
details
discussion forum
downloads
in issues
last modified
last updated
maintained
posted
posted at
previous cartoon
search by
special offer
the current week
total votes
visits
votes
copyright

[Date_Time]
\d+ Jan(uary)? \d+
\d+ Feb(ruary)? \d+
\d+ Mar(ch)? \d+
\d+ Apr(il)? \d+
\d+ May \d+
\d+ June? \d+
\d+ July? \d+
\d+ Aug(ust)? \d+
\d+ Sep(tember)? \d+
\d+ Oct(ober)? \d+
\d+ Nov(ember)? \d+
\d+ Dec(ember)? \d+
# 28-03-2005 28/03/2005 28.3.2005 2005-03-28
\d+[\/\-.]\d+[\/\-.]\d+
# 02:24 PST
\d{2}:\d{2} [A-Z]{3}

[Adverts]
http://www.news.com/cgi-bin/acc_clickthru
http://ads2.zdnet.com/adverts/
http://doublclick4.net

[VIM]
[\d,]+ scripts, [\d,]+ downloads
[\d,]+ tips, [\d,]+ tip views

[cvsweb]
\d+ (months?|weeks?|days?|hours?|minutes?)

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

=head2 IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


=head2 IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut



Revision-number: 93
Prop-content-length: 147
Content-length: 147

K 7
svn:log
V 46
Add url examples, show usage of cvsweb ignore

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-12T17:28:16.487833Z
PROPS-END

Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 8905
Text-content-md5: 99916f4978440d6e19062d2fa04896d0
Content-length: 8905

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM

URL = http://www.joelonsoftware.com/
Name = Joel on Software
Prefix = joelonsoftware

URL = http://www.openbsd.org/cgi-bin/cvsweb/src/sys/sys/tree.h
Name = OpenBSD tree implementation
Prefix = openbsd-tree
Ignore = cvsweb

URL = http://slashdot.org/
Name = Slashdot
Prefix = slashdot

URL = http://freshmeat.net/
Name = FreshMeat
Prefix = freshmeat

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    AsciiMarker - If set to 1 it will add ascii markers around the changes so
	             that highlighting is noticeable in text mode too. Useful for
				 text MUAs users.

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut



Revision-number: 94
Prop-content-length: 131
Content-length: 131

K 7
svn:log
V 30
Add TODO file to distribution

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T04:06:06.494843Z
PROPS-END

Node-path: trunk/websec/TODO
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 1263
Text-content-md5: f5e18823200483db37d5d16626f17e31
Content-length: 1273

PROPS-END
Next Release
------------

http://groups.yahoo.com/group/websec/message/79 has an elisp mode to highlight
url.list.
  - need an equivalent from vim.
  - Find out where to install the emacs mode


Short Term
----------

Enable running multiple websec instances at the same time:
http://groups.yahoo.com/group/websec/message/70
  - Implement

Send mail from windows:
http://groups.yahoo.com/group/websec/message/21
  - Improve mail sending

Problem with script pages (eg Slashdot):
http://groups.yahoo.com/group/websec/message/35 
  - Check if needed

Patch to remove script/noscript:
http://groups.yahoo.com/group/websec/message/38
  - Check if needed, related to Slashdot issues.

Multi-User patches and RPM spec:
http://groups.yahoo.com/group/websec/message/45
  - Investigate


Long Term
---------

Keep a website as an image to be put in the desktop background.
http://groups.yahoo.com/group/websec/message/72
  - Investigate

Create GUI for websec, similar to kwebwatch, but implement the GUI with Perl
bindings, this will allow wxWindows bindings for windows and gtk/qt bindings for
Linux.
  - Implement, need to modulrize websec to work as a library and have differt
    GUI, where 'text' is one of them and is the same as now, a simple command
    line.




Revision-number: 95
Prop-content-length: 145
Content-length: 145

K 7
svn:log
V 44
Add Adam Stanleys patches in the todo list 

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T04:26:02.048372Z
PROPS-END

Node-path: trunk/websec/TODO
Node-kind: file
Node-action: change
Text-content-length: 1532
Text-content-md5: 7e02aedc1ab6971f891c6f130e0bcd39
Content-length: 1532

Next Release
------------

http://groups.yahoo.com/group/websec/message/79 has an elisp mode to highlight
url.list.
  - need an equivalent from vim.
  - Find out where to install the emacs mode

Adam Stanley provided patches to add:
 + 'MailFrom', the address from which the mail will come from.
 - 'DiffOnly', send only the difference, as opposed to whole document with highlighted differences
 - Follow Refresh zero, Follow links if the site uses refresh zero.


Short Term
----------

Enable running multiple websec instances at the same time:
http://groups.yahoo.com/group/websec/message/70
  - Implement

Send mail from windows:
http://groups.yahoo.com/group/websec/message/21
  - Improve mail sending

Problem with script pages (eg Slashdot):
http://groups.yahoo.com/group/websec/message/35 
  - Check if needed

Patch to remove script/noscript:
http://groups.yahoo.com/group/websec/message/38
  - Check if needed, related to Slashdot issues.

Multi-User patches and RPM spec:
http://groups.yahoo.com/group/websec/message/45
  - Investigate


Long Term
---------

Keep a website as an image to be put in the desktop background.
http://groups.yahoo.com/group/websec/message/72
  - Investigate

Create GUI for websec, similar to kwebwatch, but implement the GUI with Perl
bindings, this will allow wxWindows bindings for windows and gtk/qt bindings for
Linux.
  - Implement, need to modulrize websec to work as a library and have differt
    GUI, where 'text' is one of them and is the same as now, a simple command
    line.




Revision-number: 96
Prop-content-length: 129
Content-length: 129

K 7
svn:log
V 28
Add Emacs mode for url.list

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T04:26:41.445483Z
PROPS-END

Node-path: trunk/websec/websec.el
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 1652
Text-content-md5: 592f4cec15a8f452de422dca7ce31da6
Content-length: 1662

PROPS-END
;;; url-list-mode.el --- simple mode for Web Secretary url files ("url.list")

;; Copyright (C) 2002 Christoph Conrad

;; Author: Christoph Conrad <christoph.conrad@gmx.de>
;; Created: 11 Feb 2002
;; Version: 0.5
;; Keywords: fontlock web secretary

;; This program is free software; you can redistribute it and/or
;; modify it under the terms of the GNU General Public License as
;; published by the Free Software Foundation; either version 2, or (at
;; your option) any later version.

;; This program is distributed in the hope that it will be useful, but
;; WITHOUT ANY WARRANTY; without even the implied warranty of
;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
;; General Public License for more details.

;; You should have received a copy of the GNU General Public License
;; along with GNU Emacs; see the file COPYING. If not, write to the
;; Free Software Foundation, Inc., 59 Temple Place - Suite 330,
;; Boston, MA 02111-1307, USA.

;; Not available in XEmacs
(require 'generic)
(require 'font-lock)

;; Keywords embedded anywhere (except [normally] in quoted strings)
(defvar url-list-font-lock-list
'(("^\\([^=\n\r]*\\)=\\([^\n\r]*\\)$"
(2 font-lock-variable-name-face))))

(defvar url-list-keywords
'( "URL" "Auth" "Name" "Prefix" "Diff" "Hicolor" "Ignore" "IgnoreURL"
"Tmin" "Tmax" "Proxy" "ProxyAuth" "Email" "EmailLink" "Digest" ))

(define-generic-mode 'url-list-mode
;; comment-list
'("#")
;; keyword-list
url-list-keywords
;; font-lock-list
url-list-font-lock-list
;; auto-mode-list
'("/url\.list$")
;; function-list for setup
nil )
"Mode for highlighting Web Secretary URL files.")

(provide 'url-list-mode)


Revision-number: 97
Prop-content-length: 143
Content-length: 143

K 7
svn:log
V 42
Add comment in Emacs mode as to its state

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T04:27:10.210135Z
PROPS-END

Node-path: trunk/websec/websec.el
Node-kind: file
Node-action: change
Text-content-length: 1807
Text-content-md5: 4562927ba9598fae75ca8e4e67112e28
Content-length: 1807

;;; url-list-mode.el --- simple mode for Web Secretary url files ("url.list")

;; Copyright (C) 2002 Christoph Conrad

;; Author: Christoph Conrad <christoph.conrad@gmx.de>
;; Created: 11 Feb 2002
;; Version: 0.5
;; Keywords: fontlock web secretary

;; This program is free software; you can redistribute it and/or
;; modify it under the terms of the GNU General Public License as
;; published by the Free Software Foundation; either version 2, or (at
;; your option) any later version.

;; This program is distributed in the hope that it will be useful, but
;; WITHOUT ANY WARRANTY; without even the implied warranty of
;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
;; General Public License for more details.

;; You should have received a copy of the GNU General Public License
;; along with GNU Emacs; see the file COPYING. If not, write to the
;; Free Software Foundation, Inc., 59 Temple Place - Suite 330,
;; Boston, MA 02111-1307, USA.

;; This file lost the indentation in the mail archive of websec in yahoo.
;; It is placed here in the hope that some user will provide a correct version.

;; Not available in XEmacs
(require 'generic)
(require 'font-lock)

;; Keywords embedded anywhere (except [normally] in quoted strings)
(defvar url-list-font-lock-list
'(("^\\([^=\n\r]*\\)=\\([^\n\r]*\\)$"
(2 font-lock-variable-name-face))))

(defvar url-list-keywords
'( "URL" "Auth" "Name" "Prefix" "Diff" "Hicolor" "Ignore" "IgnoreURL"
"Tmin" "Tmax" "Proxy" "ProxyAuth" "Email" "EmailLink" "Digest" ))

(define-generic-mode 'url-list-mode
;; comment-list
'("#")
;; keyword-list
url-list-keywords
;; font-lock-list
url-list-font-lock-list
;; auto-mode-list
'("/url\.list$")
;; function-list for setup
nil )
"Mode for highlighting Web Secretary URL files.")

(provide 'url-list-mode)


Revision-number: 98
Prop-content-length: 124
Content-length: 124

K 7
svn:log
V 23
Install the Emacs mode

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T04:29:30.291129Z
PROPS-END

Node-path: trunk/websec/Makefile
Node-kind: file
Node-action: change
Text-content-length: 1002
Text-content-md5: d1288bb3e81f6886e624468042e21769
Content-length: 1002

PREFIX=/usr/local
BINDIR=$(DESTDIR)$(PREFIX)/bin
MANDIR=$(DESTDIR)$(PREFIX)/share/man
MAN1DIR=$(MANDIR)/man1
MAN5DIR=$(MANDIR)/man5
DOCDIR=$(DESTDIR)$(PREFIX)/share/doc/websec
EMACSDIR=$(DESTDIR)$(PREFIX)/share/emacs/site-lisp

PROGSRC=websec webdiff
CONFSRC=url.list ignore.list

PROGMAN=$(PROGSRC:%=%.1)
CONFMAN=$(CONFSRC:%=%.5)

# Generated files
GENFILES=$(PROGMAN) $(CONFMAN)

all: $(GENFILES)

install: all
	install -d $(BINDIR)
	install websec $(BINDIR)/
	install webdiff $(BINDIR)/

	install -d $(MAN1DIR)
	install websec.1 $(MAN1DIR)
	install webdiff.1 $(MAN1DIR)

	install -d $(MAN5DIR)
	install url.list.5 $(MAN5DIR)
	install ignore.list.5 $(MAN5DIR)

	install -d $(DOCDIR)
	install -d $(DOCDIR)/examples
	install -m 0644 url.list $(DOCDIR)/examples/
	install -m 0644 ignore.list $(DOCDIR)/examples/
	install -m 0644 README $(DOCDIR)/

	install -m 0644 emacs.el $(EMACSDIR)

clean:
	rm -f $(GENFILES)

$(PROGMAN) : %.1 : %
	pod2man $< > $@

$(CONFMAN) : %.5 : %
	pod2man --section 5 $< > $@


Revision-number: 99
Prop-content-length: 124
Content-length: 124

K 7
svn:log
V 23
Add note for Emacs mode
K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T04:32:34.745441Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 7948
Text-content-md5: 76de524266b6d26ad6ff57a13ec1df37
Content-length: 7948

1.7 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

1.6.0 - Released on 05 May 2003

* Added DateFMT feature to set Date Formats in mail subjects.

* Add to EmailLink the local file system link so that a user can open it in his
  browser.

* Add Program option to open a program on changed link instead of mailing
  notification.

* Multiple bug fixes.

1.5.0 - Released on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.

* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.

* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com

* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Revision-number: 100
Prop-content-length: 134
Content-length: 134

K 7
svn:log
V 33
Add vim syntax file for url.list

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T06:02:35.811075Z
PROPS-END

Node-path: trunk/websec/Makefile
Node-kind: file
Node-action: change
Text-content-length: 1148
Text-content-md5: 662289a460094edc15cf94f88612924a
Content-length: 1148

PREFIX=/usr/local
BINDIR=$(DESTDIR)$(PREFIX)/bin
MANDIR=$(DESTDIR)$(PREFIX)/share/man
MAN1DIR=$(MANDIR)/man1
MAN5DIR=$(MANDIR)/man5
DOCDIR=$(DESTDIR)$(PREFIX)/share/doc/websec
EMACSDIR=$(DESTDIR)$(PREFIX)/share/emacs/site-lisp
VIMSYNDIR=$(DESTDIR)$(PREFIX)/share/vim/vim61/syntax

PROGSRC=websec webdiff
CONFSRC=url.list ignore.list

PROGMAN=$(PROGSRC:%=%.1)
CONFMAN=$(CONFSRC:%=%.5)

# Generated files
GENFILES=$(PROGMAN) $(CONFMAN)

all: $(GENFILES)

install: all
	install -d $(BINDIR)
	install websec $(BINDIR)/
	install webdiff $(BINDIR)/

	install -d $(MAN1DIR)
	install websec.1 $(MAN1DIR)
	install webdiff.1 $(MAN1DIR)

	install -d $(MAN5DIR)
	install url.list.5 $(MAN5DIR)
	install ignore.list.5 $(MAN5DIR)

	install -d $(DOCDIR)
	install -d $(DOCDIR)/examples
	install -m 0644 url.list $(DOCDIR)/examples/
	install -m 0644 ignore.list $(DOCDIR)/examples/
	install -m 0644 README $(DOCDIR)/

	install -d $(EMACSDIR)
	install -m 0644 websec.el $(EMACSDIR)
	
	install -d $(VIMSYNDIR)
	install -m 0644 websec.vim $(VIMSYNDIR)

clean:
	rm -f $(GENFILES)

$(PROGMAN) : %.1 : %
	pod2man $< > $@

$(CONFMAN) : %.5 : %
	pod2man --section 5 $< > $@


Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 8930
Text-content-md5: 9dc45dc45768acd065c6d5da3dd9857b
Content-length: 8930

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.mp3.com/
Name = MP3.COM
Prefix = mp3-com

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM

URL = http://www.joelonsoftware.com/
Name = Joel on Software
Prefix = joelonsoftware

URL = http://www.openbsd.org/cgi-bin/cvsweb/src/sys/sys/tree.h
Name = OpenBSD tree implementation
Prefix = openbsd-tree
Ignore = cvsweb

URL = http://slashdot.org/
Name = Slashdot
Prefix = slashdot

URL = http://freshmeat.net/
Name = FreshMeat
Prefix = freshmeat

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    AsciiMarker - If set to 1 it will add ascii markers around the changes so
	             that highlighting is noticeable in text mode too. Useful for
				 text MUAs users.

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut

vim:set filetype=websec:


Node-path: trunk/websec/websec.vim
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 1658
Text-content-md5: 31450520f5f1b8b638e64b3cb0eca221
Content-length: 1668

PROPS-END
" Vim syntax file
" Language: 	Web Secretary url.list & ignore.list
" Maintainer:	Baruch Even <websec@ev-en.org>
" URL:			With websec itself
" Last Change:	Tue May 13 07:41:52 IDT 2003

" Place this file as ~/.vim/syntax/websec.vim
"
" Then add the following lines to ~/.vimrc
"
" au BufNewFile,BufRead  url.list,ignore.list setf svn

" For version 5.x: Clear all syntax items
" For version 6.x: Quit when a syntax file was already loaded
if version < 600
	syntax clear
elseif exists("b:current_syntax")
	finish
endif

syn keyword urllistCommand  Auth Diff Hicolor Ignore IgnoreURL Tmin Tmax ProxyAuth
syn keyword urllistCommand  Email EmailLink EmailError Program MailFrom Proxy con
syn keyword urllistCommand  Program URL Name Prefix AsciiMarker Digest
syn keyword urllistCommand  UserAgent DateFMT RandomWait

syn region	urllistString	start=+"+ skip=+\\\\\|\\"+ end=+"+ oneline
syn region	urllistString	start=+'+ skip=+\\\\\|\\'+ end=+'+ oneline

syn region  urllistEND    start="^\s*__END__" skip="." end="." contains=perlPOD

syn match	urllistComment	"^#.*"
syn match	urllistComment	"\s#.*"ms=s+1

" Define the default highlighting.
" For version 5.7 and earlier: only when not done already
" For version 5.8 and later: only when an item doesn't have highlighting yet
if version >= 508 || !exists("did_svn_syn_inits")
	if version < 508
		let did_svn_syn_inits = 1
		command -nargs=+ HiLink hi link <args>
	else
		command -nargs=+ HiLink hi def link <args>
	endif

    HiLink urllistComment	Comment
    HiLink urllistEND		Comment
    HiLink urllistString	String
	HiLink urllistCommand	Statement

	delcommand HiLink
endif

let b:current_syntax = "websec"


Revision-number: 101
Prop-content-length: 132
Content-length: 132

K 7
svn:log
V 31
Add note about vim syntax file

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T06:03:13.271563Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 7998
Text-content-md5: 117aa7c2410988ae273f064d4ab85389
Content-length: 7998

1.7 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

1.6.0 - Released on 05 May 2003

* Added DateFMT feature to set Date Formats in mail subjects.

* Add to EmailLink the local file system link so that a user can open it in his
  browser.

* Add Program option to open a program on changed link instead of mailing
  notification.

* Multiple bug fixes.

1.5.0 - Released on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.

* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.

* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com

* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Revision-number: 102
Prop-content-length: 159
Content-length: 159

K 7
svn:log
V 58
MP3.com doesn't look like it has any useful info to track

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T06:52:32.057321Z
PROPS-END

Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 8871
Text-content-md5: cffed7087aafd53a857199c33dc3c32a
Content-length: 8871

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM

URL = http://www.joelonsoftware.com/
Name = Joel on Software
Prefix = joelonsoftware

URL = http://www.openbsd.org/cgi-bin/cvsweb/src/sys/sys/tree.h
Name = OpenBSD tree implementation
Prefix = openbsd-tree
Ignore = cvsweb

URL = http://slashdot.org/
Name = Slashdot
Prefix = slashdot

URL = http://freshmeat.net/
Name = FreshMeat
Prefix = freshmeat

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    AsciiMarker - If set to 1 it will add ascii markers around the changes so
	             that highlighting is noticeable in text mode too. Useful for
				 text MUAs users.

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut

vim:set filetype=websec:


Revision-number: 103
Prop-content-length: 142
Content-length: 142

K 7
svn:log
V 41
Add todo items from Savannah bug tracker

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T15:47:01.030796Z
PROPS-END

Node-path: trunk/websec/TODO
Node-kind: file
Node-action: change
Text-content-length: 1934
Text-content-md5: 07de6f57dda4b40543d222ae327c66f9
Content-length: 1934

Next Release
------------

http://groups.yahoo.com/group/websec/message/79 has an elisp mode to highlight
url.list.
  - need an equivalent from vim.
  - Find out where to install the emacs mode

Adam Stanley provided patches to add:
 + 'MailFrom', the address from which the mail will come from.
 - 'DiffOnly', send only the difference, as opposed to whole document with highlighted differences
 - Follow Refresh zero, Follow links if the site uses refresh zero.


Short Term
----------

B3450, Inconsitency with the .websec/url.list location finding and using websec with a file parameter.

B3497, Show also removed content from the html page, possibly use HTML::Diff

Enable running multiple websec instances at the same time:
http://groups.yahoo.com/group/websec/message/70
  - Implement

Send mail from windows:
http://groups.yahoo.com/group/websec/message/21
  - Improve mail sending

Problem with script pages (eg Slashdot):
http://groups.yahoo.com/group/websec/message/35 
  - Check if needed

Patch to remove script/noscript:
http://groups.yahoo.com/group/websec/message/38
  - Check if needed, related to Slashdot issues.

Multi-User patches and RPM spec:
http://groups.yahoo.com/group/websec/message/45
  - Investigate

B3449, Notify when pages in url.list haven't been updated for a long time.

B3472, Provide a method to specify how often to check for changes, mostly for dialup users.
Can also be used for a constantly running program.


Long Term
---------

Keep a website as an image to be put in the desktop background.
http://groups.yahoo.com/group/websec/message/72
  - Investigate

Create GUI for websec, similar to kwebwatch, but implement the GUI with Perl
bindings, this will allow wxWindows bindings for windows and gtk/qt bindings for
Linux.
  - Implement, need to modulrize websec to work as a library and have differt
    GUI, where 'text' is one of them and is the same as now, a simple command
    line.




Revision-number: 104
Prop-content-length: 266
Content-length: 266

K 7
svn:log
V 164
Fixed bug in usage of environment variables in configuration lines, also added ability to use multiple env vars in a single line such as:
Email = ${USER}@${DOMAIN}

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T18:55:30.965972Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 8132
Text-content-md5: 3d47fc34dab6b103d3746962e0578f1f
Content-length: 8132

1.7 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

1.6.0 - Released on 05 May 2003

* Added DateFMT feature to set Date Formats in mail subjects.

* Add to EmailLink the local file system link so that a user can open it in his
  browser.

* Add Program option to open a program on changed link instead of mailing
  notification.

* Multiple bug fixes.

1.5.0 - Released on 02 May 2003

* Added UserAgent option to control the user agent sent by the web client. This
  enables access to websites which lockdown unknown browsers.

* Enabled the user to run websec from the current directory without the
  multi-user ~/.websec/ setup, the multi-user setup is the default unless you
  run websec from a directory with the url.list file.

* Allow using environment variables in config file, this enables things like:
  Email = ${USER}@example.com

* Moved the manpages to pod and into the files, this enables fancy options such
  as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

* Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

* Added tip on how to install LWP in READE. Thanks to Jeff for contributing
  this info!

* Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
  for contributing this!

1.3.3 - Released on 31 Jan 2000

* Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
  this out. This fixes problems under certain mailers, where messages are
  treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

* Added <CODE> as a recognized tag.

* Added the ability to send updated page to multiple recipients.

* Added the ability to send URL link instead of entire page to recipients.
  Check "EmailLink" tag.

* "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
  Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

* Volker Stampa contributed some code to allow websec to work with proxies
  that require authentication.

1.3  - Released on 20 Mar 1999

* Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
  "-hicolor" parameter of webdiff. This feature has been included.

* Webdiff had some problems with a tag of this nature: <A HREF="xxx <yyy>">,
  first found in the ZDNET series of web sites.  This has been fixed.

* A new "ignore URL" feature has been included. This allows certain
  hyperlinks sections in a web page to be skipped during webdiff processing.

* All ignore keywords and URLs have been consolidated into one file. 

1.22 - Released on 13 Jan 1999

* A small shell script has been included to "rollback" the files in the
  archive directory for one session.

* Proxy settings can now be supplied via the "http_proxy" environment
  variable. However, the "Proxy" parameter will take precedence over the
  environment variable.

* When checking for short and long tokens (based on the Tmin and Tmax
  parameters), any mangled HTML tags are first stripped from the token before
  word count is done.  Therefore, word count is done on the "plain text"
  version of the token.

* When checking for ignore keywords, the token which possibly contains
  mangled HTML tags is first checked. Then it is stripped of any mangled
  HTML tags and checked again. This is cater for cases where the mangled HTML
  tag precedes or follows an actual word without any spacing. Hence the
  entire string is treated as one word, and will fail to match any of the
  ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

* Made minor modification to try downloading any URL up to 3 times before
  giving up. I did not find it necessary to include this as a parameter,
  so it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

* Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
  email transmission. Hence, it is no longer necessary to have 'lynx' and
  'metasend' installed on your system in order to use Web Secretary.

* Since Web Secretary was rewritten to use the LWP module (instead of lynx),
  for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
  myself) who are behind a firewall.

* Added the 'Tmin' parameter to ignore short tokens when highlighting
  differences. This is useful because certain sites have tokens containing
  one or two words which change constantly but are uninteresting to track.

* Added the 'Tmax' parameter to prevent long tokens from being processed by
  the ignore keywords filter. This was included because certain sites have
  tokens containing the current day/month etc. which I want to filter off. 
  But at the same time, I do not want to filter off long paragraphs that
  contain these words.

1.11 - Released on 10 Oct 1998

* Minor modification to the comparison algorithm so that it won't be fooled
  by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

* Improved the detection algorithm for multiple consecutive mangled HTML
  tags so that they will not be incorrectly highlighted.

* Support for Javascript and stylesheet tags so that they will not be
  incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

The idea for this tool originated from a software package called Tierra
Highlights for the PC (http://www.tierra.com). I tried it out for a while
and found it to be extremely useful.  However, like most PC tools, it was
closely tied to the PC that you installed the software on. If you are
working on some other computer, you will not be unable to access the pages
being monitored. At that time, I was already convinced that email is the
best "push" platform the world has ever seen, so why not deliver the changed
pages via email?

I bounced the idea around for a while amongst friends and colleagues, and
when I could not find any sucker to write this for me :-), I wrote the first
version in a crazy moment of unrest using shell script. However, this first
version was not very configurable, so I quickly wrote the second version in
Perl.

So far, however, the program does nothing but retrieve pages and email them
to me. I quickly added a quick hack to do a diff between an archive page
and the current page before deciding whether to email the page, but the
scheme proved too brittle for detecting changes in most cases.

I lived with this scheme for a while. Finally, lunacy got the better of me.
I figured out a quick and dirty way of doing what Tierra Highlights does,
and actually thought I could implement the whole idea in one day. It took
two days instead, and the initial version sucked like hell and failed
miserably on many pages. However, you should have seen the grin on my face
when it highlighted PC Magazine and PC Week properly. :-)

Like most programmers who are crazy enough to think that they can "do this
thing in one day", I spent the next two weeks feverishly debugging the
project. Everyday, I will add new pages to the URL list, and debug those
that failed to be highlighted. Finally, I have something which I use on a
daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 15761
Text-content-md5: e9059048396f6983c88f46795234fbe6
Content-length: 15761

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.6.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.6.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.6.0",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>

=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 105
Prop-content-length: 147
Content-length: 147

K 7
svn:log
V 46
Switch all regexes to use // and not :: or ||

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T19:03:54.406329Z
PROPS-END

Node-path: trunk/websec/TODO
Node-kind: file
Node-action: change
Text-content-length: 1988
Text-content-md5: 024f80e76272c2a5ed360e6bf442ed1c
Content-length: 1988

Next Release
------------

http://groups.yahoo.com/group/websec/message/79 has an elisp mode to highlight
url.list.
  + need an equivalent from vim.
  + Find out where to install the emacs mode
  ? Fix it, it seems broken, need user input on this.

Adam Stanley provided patches to add:
 + 'MailFrom', the address from which the mail will come from.
 - 'DiffOnly', send only the difference, as opposed to whole document with highlighted differences
 - Follow Refresh zero, Follow links if the site uses refresh zero.


Short Term
----------

B3450, Inconsitency with the .websec/url.list location finding and using websec with a file parameter.

B3497, Show also removed content from the html page, possibly use HTML::Diff

Enable running multiple websec instances at the same time:
http://groups.yahoo.com/group/websec/message/70
  - Implement

Send mail from windows:
http://groups.yahoo.com/group/websec/message/21
  - Improve mail sending

Problem with script pages (eg Slashdot):
http://groups.yahoo.com/group/websec/message/35 
  - Check if needed

Patch to remove script/noscript:
http://groups.yahoo.com/group/websec/message/38
  - Check if needed, related to Slashdot issues.

Multi-User patches and RPM spec:
http://groups.yahoo.com/group/websec/message/45
  - Investigate

B3449, Notify when pages in url.list haven't been updated for a long time.

B3472, Provide a method to specify how often to check for changes, mostly for dialup users.
Can also be used for a constantly running program.


Long Term
---------

Keep a website as an image to be put in the desktop background.
http://groups.yahoo.com/group/websec/message/72
  - Investigate

Create GUI for websec, similar to kwebwatch, but implement the GUI with Perl
bindings, this will allow wxWindows bindings for windows and gtk/qt bindings for
Linux.
  - Implement, need to modulrize websec to work as a library and have differt
    GUI, where 'text' is one of them and is the same as now, a simple command
    line.




Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 13168
Text-content-md5: 3b7a481f7ca3d35e322348c63c920891
Content-length: 13168

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.6.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$asciimarker = 0;
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "asciimarker"  => \$asciimarker,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage =~ s/[\r\n]|\s\s/ /sig;    # Handle MSDOS-style line separators
$newpage =~ s/[\r\n]|\s\s/ /sig;
$oldpage =~ s/&nbsp;/\@\@\@\@&nbsp;~~~~/sig;   # Handle non-breaking white space
$newpage =~ s/&nbsp;/\@\@\@\@&nbsp;~~~~/sig;
$oldpage =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>/~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@/sig
  ;    # Handle nested brackets
$newpage =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>/~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@/sig;
foreach (@tags) {
    $tag = $_;
    $oldpage =~ s/<(\/*$tag)>/~~~~$1\@\@\@\@/sig;
    $newpage =~ s/<(\/*$tag)>/~~~~$1\@\@\@\@/sig;
    if ( $tag =~ s\/\*/ / ) { # XXX WTF is going here with the re?
        $oldpage =~ s/<(\/*$tag.*?)>/~~~~$1\@\@\@\@/sig;
        $newpage =~ s/<(\/*$tag.*?)>/~~~~$1\@\@\@\@/sig;
    }
}

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            if ($asciimarker) {
                $token = "###>>>". $token ."<<<###";
            }
            $token =
                    "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR>".
                    "<TD BGCOLOR=$hicolor>" . $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m/^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$/i ) {
            $token = $2;
            if ( !$1 =~ m/^\s*$/ ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s/\@\@\@\@&nbsp;~~~~/ /sig;
    $tokdup =~ s/~~~~/</sig;
    $tokdup =~ s/\@\@\@\@/>/sig;
    $tokdup =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>//sig;
    $tokdup =~ s/<[^>]*>//sig;
    $tokdup =~ s/^\s*//sig;
    $tokdup =~ s/\s*$//sig;
    $tokdup =~ s/\s+/ /sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m/^.*?(\b$keyword\b).*?$/i
            || $tokdup =~ m/^.*?(\b$keyword\b).*?$/i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s/\@\@\@\@&nbsp;~~~~/ /sig;
    $tokdup =~ s/~~~~/</sig;
    $tokdup =~ s/\@\@\@\@/>/sig;
    $tokdup =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>//sig;
    $tokdup =~ s/<[^>]*>//sig;
    $tokdup =~ s/^\s*//sig;
    $tokdup =~ s/\s*$//sig;
    $tokdup =~ s/\s+/ /sig;
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m/~~~~A.*?HREF=.*?$url.*?\@\@\@\@/i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m/~~~~\/A\@\@\@\@/i;
    return 0;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut

vim:set et ts=4:


Revision-number: 106
Prop-content-length: 162
Content-length: 162

K 7
svn:log
V 61
Refactor some code into a function to avoid code duplication

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T19:38:28.832478Z
PROPS-END

Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 13099
Text-content-md5: d22a8e0ca89a9feab8caf905f04e2f4c
Content-length: 13099

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.6.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$asciimarker = 0;
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "asciimarker"  => \$asciimarker,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage = &MangleHTML($oldpage, @tags);
$newpage = &MangleHTML($newpage, @tags);

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            if ($asciimarker) {
                $token = "###>>>". $token ."<<<###";
            }
            $token =
                    "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR>".
                    "<TD BGCOLOR=$hicolor>" . $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m/^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$/i ) {
            $token = $2;
            if ( !$1 =~ m/^\s*$/ ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup =~ s/\@\@\@\@&nbsp;~~~~/ /sig;
    $tokdup =~ s/~~~~/</sig;
    $tokdup =~ s/\@\@\@\@/>/sig;
    $tokdup =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>//sig;
    $tokdup =~ s/<[^>]*>//sig;
    $tokdup =~ s/^\s*//sig;
    $tokdup =~ s/\s*$//sig;
    $tokdup =~ s/\s+/ /sig;
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m/^.*?(\b$keyword\b).*?$/i
            || $tokdup =~ m/^.*?(\b$keyword\b).*?$/i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    $tokdup = $token;

    # If this token contains <= tmin no. of words, don't check
    $tokdup =~ s/\@\@\@\@&nbsp;~~~~/ /sig;
    $tokdup =~ s/~~~~/</sig;
    $tokdup =~ s/\@\@\@\@/>/sig;
    $tokdup =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>//sig;
    $tokdup =~ s/<[^>]*>//sig;
    $tokdup =~ s/^\s*//sig;
    $tokdup =~ s/\s*$//sig;
    $tokdup =~ s/\s+/ /sig;
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m/~~~~A.*?HREF=.*?$url.*?\@\@\@\@/i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m/~~~~\/A\@\@\@\@/i;
    return 0;
}

sub MangleHTML() {
    my $page = shift(@_);
    my @tags = shift(@_);

    $page =~ s/[\r\n]|\s\s/ /sig;    # Handle MSDOS-style line separators
    $page =~ s/&nbsp;/\@\@\@\@&nbsp;~~~~/sig;   # Handle non-breaking white space
    $page =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>/~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@/sig;    # Handle nested brackets
    foreach (@tags) {
        $tag = $_;
        $page =~ s/<(\/*$tag)>/~~~~$1\@\@\@\@/sig;
        if ( $tag =~ s/\*/ / ) { # XXX WTF is going here with the re?
            $page =~ s/<(\/*$tag.*?)>/~~~~$1\@\@\@\@/sig;
        }
    }

    return $page;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut

vim:set et ts=4:


Revision-number: 107
Prop-content-length: 123
Content-length: 123

K 7
svn:log
V 22
Refactor ReduceSpaces

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-13T20:02:21.524554Z
PROPS-END

Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12924
Text-content-md5: b4fdf903f9f81c9c4ce78f4de6b3e238
Content-length: 12924

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.6.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$asciimarker = 0;
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "asciimarker"  => \$asciimarker,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage = &MangleHTML($oldpage, @tags);
$newpage = &MangleHTML($newpage, @tags);

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            if ($asciimarker) {
                $token = "###>>>". $token ."<<<###";
            }
            $token =
                    "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR>".
                    "<TD BGCOLOR=$hicolor>" . $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m/^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$/i ) {
            $token = $2;
            if ( !$1 =~ m/^\s*$/ ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup = &ReduceSpaces($token);
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m/^.*?(\b$keyword\b).*?$/i
            || $tokdup =~ m/^.*?(\b$keyword\b).*?$/i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    # If this token contains <= tmin no. of words, don't check
    $tokdup = &ReduceSpaces($token);
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m/~~~~A.*?HREF=.*?$url.*?\@\@\@\@/i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m/~~~~\/A\@\@\@\@/i;
    return 0;
}

sub MangleHTML() {
    my $page = shift(@_);
    my @tags = shift(@_);

    $page =~ s/[\r\n]|\s\s/ /sig;    # Handle MSDOS-style line separators
    $page =~ s/&nbsp;/\@\@\@\@&nbsp;~~~~/sig;   # Handle non-breaking white space
    $page =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>/~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@/sig;    # Handle nested brackets
    foreach (@tags) {
        $tag = $_;
        $page =~ s/<(\/*$tag)>/~~~~$1\@\@\@\@/sig;
        if ( $tag =~ s/\*/ / ) { # XXX WTF is going here with the re?
            $page =~ s/<(\/*$tag.*?)>/~~~~$1\@\@\@\@/sig;
        }
    }

    return $page;
}

sub ReduceSpaces() {
    my $token = shift(@_);
    
    $token =~ s/\@\@\@\@&nbsp;~~~~/ /sig;
    $token =~ s/~~~~/</sig;
    $token =~ s/\@\@\@\@/>/sig;
    $token =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>//sig;
    $token =~ s/<[^>]*>//sig;
    $token =~ s/^\s*//sig;
    $token =~ s/\s*$//sig;
    $token =~ s/\s+/ /sig;

    return $token;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.

=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut

vim:set et ts=4:


Revision-number: 108
Prop-content-length: 124
Content-length: 124

K 7
svn:log
V 23
Reformat the NEWS file

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-14T07:18:43.385262Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 8558
Text-content-md5: 705252282c45153cdfae3238de9d013a
Content-length: 8558

1.7 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Revision-number: 109
Prop-content-length: 171
Content-length: 171

K 7
svn:log
V 70
As a ChangeLog use the human intended NEWS source and not commit logs

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-14T07:53:25.813003Z
PROPS-END

Node-path: trunk/utils/extractNEWS
Node-kind: file
Node-action: add
Prop-content-length: 36
Text-content-length: 343
Text-content-md5: 0152b78a9791054795197a7e3fc6d92e
Content-length: 379

K 14
svn:executable
V 1
*
PROPS-END
#!/usr/bin/awk -f
BEGIN {
	printing = 0 # Have we printed anything?
}

/^[0-9]/ {
	if (!printing) {
		printing = 1 # We will print this line
	} else {
		exit 0 # Processing is over now
	}
	
	print
}

# Print the lines that start with four spaces or those with no characters in them
/^    / { if (printing) print }
/^$/ { if (printing) print }


Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 1067
Text-content-md5: 161dc7298550550afbba41add614f09f
Content-length: 1067

#!/bin/sh

PROJ=websec
VERSION=$1
DIR=files/$PROJ.pkg/$VERSION/
TAG=version-$VERSION
TAGFILE=file:///home/svn/$PROJ/tags/$PROJ/$TAG

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

if [ "$VERSION" = "snapshot" ]
then
	echo "Doing snapshot"
	VERSION=snapshot-`date +%Y.%m.%d`
	TAGFILE=file:///home/svn/$PROJ/trunk/$PROJ
	DIR=files/$PROJ.pkg/snapshot
	rm -f $DIR/*
fi

FILE=$PROJ-$VERSION.tar.gz
BASEDIR=$PROJ-$VERSION/

# Export clean copy of the repository
svn export $TAGFILE $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	rm -rf $BASEDIR
	exit 1
fi

# Prepare version for release
if [ -f $BASEDIR/autogen.sh ]
then
	pushd $BASEDIR
	./autogen.sh
	popd
elif [ -f $BASEDIR/Makefile ]
then
	pushd $BASEDIR
	make
	popd
fi

# Generate ChangeLog
utils/extractNEWS $BASEDIR/NEWS > $DIR/ChangeLog

# Create tar.gz file
tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

# Sign the tar.gz
echo "Now we GPG sign the release:"
pushd $DIR
gpg --detach --armor --sign $FILE
popd

# Locally update the site
utils/update


Revision-number: 110
Prop-content-length: 217
Content-length: 217

K 7
svn:log
V 115
Add a link from websec-snapshot.tar.gz to the actual snapshot. This is useful for development branch on FreshMeat.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-14T08:35:05.462788Z
PROPS-END

Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 1151
Text-content-md5: d40b4cf160a40f712252355cd2298172
Content-length: 1151

#!/bin/sh

PROJ=websec
VERSION=$1
DIR=files/$PROJ.pkg/$VERSION/
TAG=version-$VERSION
TAGFILE=file:///home/svn/$PROJ/tags/$PROJ/$TAG
DOLINK=0

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

if [ "$VERSION" = "snapshot" ]
then
	echo "Doing snapshot"
	DOLINK=1
	VERSION=snapshot-`date +%Y.%m.%d`
	TAGFILE=file:///home/svn/$PROJ/trunk/$PROJ
	DIR=files/$PROJ.pkg/snapshot
	rm -f $DIR/*
fi

FILE=$PROJ-$VERSION.tar.gz
BASEDIR=$PROJ-$VERSION/

if [ $DOLINK ]
then
	ln -s $FILE $DIR/websec-snapshot.tar.gz
fi

# Export clean copy of the repository
svn export $TAGFILE $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	rm -rf $BASEDIR
	exit 1
fi

# Prepare version for release
if [ -f $BASEDIR/autogen.sh ]
then
	pushd $BASEDIR
	./autogen.sh
	popd
elif [ -f $BASEDIR/Makefile ]
then
	pushd $BASEDIR
	make
	popd
fi

# Generate ChangeLog
utils/extractNEWS $BASEDIR/NEWS > $DIR/ChangeLog

# Create tar.gz file
tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

# Sign the tar.gz
echo "Now we GPG sign the release:"
pushd $DIR
gpg --detach --armor --sign $FILE
popd

# Locally update the site
utils/update


Revision-number: 111
Prop-content-length: 134
Content-length: 134

K 7
svn:log
V 33
Add years to cvsweb ignore rules

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-15T11:17:46.789332Z
PROPS-END

Node-path: trunk/websec/ignore.list
Node-kind: file
Node-action: change
Text-content-length: 3735
Text-content-md5: 73511a304e9b481817959efdf40ea50c
Content-length: 3735

[General]
all rights reserved
an error occurred
click here
comments
copyright
daily articles for
details
discussion forum
downloads
in issues
last modified
last updated
maintained
posted
posted at
previous cartoon
search by
special offer
the current week
total votes
visits
votes
copyright

[Date_Time]
\d+ Jan(uary)? \d+
\d+ Feb(ruary)? \d+
\d+ Mar(ch)? \d+
\d+ Apr(il)? \d+
\d+ May \d+
\d+ June? \d+
\d+ July? \d+
\d+ Aug(ust)? \d+
\d+ Sep(tember)? \d+
\d+ Oct(ober)? \d+
\d+ Nov(ember)? \d+
\d+ Dec(ember)? \d+
# 28-03-2005 28/03/2005 28.3.2005 2005-03-28
\d+[\/\-.]\d+[\/\-.]\d+
# 02:24 PST
\d{2}:\d{2} [A-Z]{3}

[Adverts]
http://www.news.com/cgi-bin/acc_clickthru
http://ads2.zdnet.com/adverts/
http://doublclick4.net

[VIM]
[\d,]+ scripts, [\d,]+ downloads
[\d,]+ tips, [\d,]+ tip views

[cvsweb]
\d+ (years?|months?|weeks?|days?|hours?|minutes?)

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

=head2 IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.txt" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


=head2 IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.txt". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut



Revision-number: 112
Prop-content-length: 121
Content-length: 121

K 7
svn:log
V 20
Add example to site

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-16T12:51:09.958825Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 7687
Text-content-md5: f91975db31579d6d620924bea537b2bd
Content-length: 7687

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>What is WebSec?</h2>
<div class=subsection>
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>
</div>

<h2>Download</h2>
<div class=subsection>
<p>
In the <a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);
			$text = $ver;
			if ($text eq 'snapshot') {
				$text = $file;
				$text =~ s/.*(snapshot-\d+\.\d+\.\d+).*/$1/;
			}

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $text, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
</p>
<ul>
<:listversions():>
</ul>
</div>

<h2>Installation</h2>

<h3>Are there any dependencies?</h3>
<div class=subsection>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
</div>

<h3>How to install?</h3>
<div class=subsection>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
</div>

<h3>How to use?</h3>
<div class=subsection>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</div>

<h2>How do I get help? How do I help?</h2>
<div class=subsection>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</div>

<h2>Authors</h2>
<div class=subsection>
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</div>

<h2>"Competitors"</h2>
<div class=subsection>
<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
  <p>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
  </p>
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
  <p>
    Checks only timestamps and not actual page changes. It's main advanges
    is being written in Python.
  </p>
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
  <p>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
  </p>
  <p>
   There apears to be two wrep's, from two different authors, with totally
   different methods of work.
  </p>
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd><p>A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.</p></dd>
</dl>
</div>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Revision-number: 113
Prop-content-length: 145
Content-length: 145

K 7
svn:log
V 44
Add screenshot of websec highlighted output

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-16T13:01:06.593112Z
PROPS-END

Node-path: trunk/site/websec-homepage.highlighted.png
Node-kind: file
Node-action: add
Prop-content-length: 59
Text-content-length: 100700
Text-content-md5: 13a7530058fc31d5f71ecea6d75a2cbd
Content-length: 100759

K 13
svn:mime-type
V 24
application/octet-stream
PROPS-END
PNG

   IHDR          	pHYs   H   H Fk>   IDATxw| ''[ lQ2DA*( 
e)(t^ofHV|?ro<y\Q`W[i!BZe[6R5_NtAAAMGB!@!BULB!0G!Bt!B:}B!,PZs*?n=W&*=4Mryv-vh=}lW.#}fU%^iw60?/ O?}+UaW
k3Zm.!B+3+M{+/<uf34U*997TUcm'^=~ymuU?s s?S(.]u3raG*U:w^wq~ k^|_#B*Q5y"[45!~u<{V_w+ueChV._oU*oYhAZK\PWa}q-`!N {uym`/W| s}7f+^VXF ]DG%&<m =|UkC=-Ys-1+nC=<{HIB!jJ<NSg9::U6sW*)l/*wwa85n^T#>zx ?++3Hn }wx !ae9N;qbE.HXS Eik 4x5p7&//_v::9j% qm4~)o;UwwM ]]EjqA7w6mk44F#TF"}W.u^~{Go7'5B!jV0mz?hUe{W.]x OmwomZ[7veIn99>]Z&%&oX6!!M8&zA
a~}"H z~4M@&M\}\mze?*).~m+S/$\Ex~u*=a6nBE  +3C=`]W ^-/-++3C!qUe7wN~Khh5&$>:}u[ppH$2UDDVo=.4ljG4EC3Cb<~R ]3i\/|'j f<r>,7B b)me	&I_A/ 61il0E!YG &euL{inAY>cnn3*^5r
	KKK"Iel'iQ8%ztikoo]`YWVJY!!a~RBX<u{h=6fkmwg%~2@$"gZtB2 XDVq&/O^ vVt!S+.Zu"B,Ber'\36mth3BZ=/f])\M]Ft%B!^0vG!B6o_* B!BV*K1G!Bq"B!^#B!Tg ..rA!BTkD\\\a5)B!B"_,CNNB\E!BH,[n!B!Su	 D!BUwae*g  	&!BVb4A deY520{`5i&A!BE,M0$,OLkP		m@!-{93_F7aX2# @+*)j,2 ^ 8 a%
F?"(a0	  @ xyq61[
!B!9y%U<<ot4e1OrEDT&s

HJXcu	I*iXYBD"=8?,_*T	 mbi X&/Ir>=+_B!Z7(18ber}2 8=GgE=z|q h++VjrsW@hmZ5	r9.A0,K+='OSEbV>LyliaeTAza5_=_AL7H5AbFVrW-d"B$d#lfry }Vc6l6w)h_PxyED=::;$ sGZ^+(Gq	AG;8#	iF?MIzztn~=;{:mB4E3Mm M* ,MUI.a>!Cs=o8T^yDL	 <? $SNSWO?&J&N`@ MEg-PM3[mu8{Srv $ $X $	 H_'lzKQw9aM X'Kztj~\v1 k0k6:?rc=v	 iM,iGmDk~pK!B<`<A0F.V333=Xlr/Ar0W:9f(XLi)BQN ]{us*qn$ IHD@,&n
f:|bQLhp`f	tR4M3a$\[?<I#..n.u6(k4Eht:I?~Nc@[&5H \$ffm* *fTLBa-E]KYxyyu pbX.Suh+..cn+J@@YvF 7?{7'%x(G2 K6P*D"eI$rKE<y448Mfh(:E3,0$i8&O3fL&|mr	DR=MHV&8V14 B!ML5S7B**6?k+P\\w
U%H4h];X|}KfNPvtWTl )5ZnQbnEQ21 b ]Wul$) d+KyF&i^$MS4c+:ulJFfVK4EC;:8zz:eft_.M2.   mmdb<0Ekubu:_'K%b%A$A,$-MAbTBt~~M:bZK1,M#JJY95y9:yC@!;QXYM[cS,,iyDQT@@@VVF oooD(*-- bqHH?WaaZm
E.6k+I\! aL|o :QV.0Yo1 (~s:#(fhIIv^XH3 @D\Wj^44Ya$b r	iVs}f:OvvZ5	pwl	k'u3G;X(h Pz 	6 8H@,{9'eVqlm=]m#;feZ Tyx};5|%!z=0YAj:
hV?EW7ei..wF!O>2z^^[nj d2"""=zd}w w(2H6;'g!|62.(fh h{w6.-[fgdQ4W )JD}>z"AQ4YOo}g| @zvr*X,zCjfwd21MTavm[I6k `SbZp CeiUT4Krt<7;RUMGBJf>[6
:uT]1ZJQTw+zN<W/L&dG;Vk+.. a+t1[Y $ K1	 4n7ba ;h)P:Nfha= &h $I#b
@L@jFv~^^:a,df*;DM$Eiu:it40\cbB<I"I|Mb8;$)6 OEnDtffx{"1 zR	)iNnE1_P\_ !e,tcE1@M+ey0Jt   EEE88s
MKFTVtb`Y1[eL])A $)U NQVy|C,Cs>MQ: [,oo/ $M3 6afdf3U6QIY*V ,en4PPmAsGooYy^n q8 xALp`M7Ct4C)YA<`|KY_PGbyXK >B,W[/N/>JC?f{B?|YRZqNSTtHdgg/]0/=C,0IYh:/OxyZ07<p dRL* ZVkHRh(?7'/UD	$I	kpmJG*nnJN(* (KHGS|hHUez^A,Lzzy	DGQF?1y9(AH@jfXni.A\\\1&7'e48980,KQ4wh?3 19U`\$6+LzT$'%040sPH[&VToolYfEDDDB"\AMgQS;4!.=p#5 ^
#s)>RT.pj5wR\.JW.fY0?ee|%NeX,:8:t^=>!arDGQPc(hV]F*Jd.$ZGS4-2YT ehg'XBxn|RQXh\j	FWRi|/z~^apk-T$;rv$I1LcT-MQ>M :dhpT%It@=%Xr}f'2MS}h2rEf^(.&eRWnNNb\ktmDOE6RB!AeK>=sjnymf{o.#-s]z"&*Y)W 2NNN\@$.--8::2cteY=q>,e6:P]*sWQGQ4wa.WjZV hfah1a1RBHb	 4E]1qOJ(fmmRNGqWeiJ0`Yo \ Z=}T"!Bu:VKiZi61BtNKtFv^FNK)]^a$Z	KD W(/2:KuoMx7s~FG]yhZhIC>CsrO/l*.HC*w^cg5/8*J`4$e,)	&3zL}~
.]=K"b;9A>&r>7^CO6akVTJNfV$G9FhZR<bKcZ{K"$g?B? 0-L)JGQ:VK1&?@+K)Z2d%Iw'.m 5QCCtJhVt7EG3ZEF,,*"QkZ ah?.N( (ev  s:
 \M`04CS>`htZVR40LY7QC$E:nP40* p)0oXd8Kff[_?DQ*sprWNFGdTTv?1c_f_<rowy+Zylr*H^Z|TUIm[3"^Boc,f2J4JdR9BvJ  /% EQw1g rI_3,:kZgZEsw*p@&IR(>w- pttT* {{{ |~OE#$?/P4,2,ko#wus9vuu9qJ:V[Ms/}}GQj  eVX[\&mJJ4-jt:e"-+'5#+!%#>!9i'?xTk:Jt:J}?yd:%dMhaV.k.bVGq=)|-X""VKSVP  IF7&qJerFU'ln:;hu:bZK414Amn[4oJs}~
 |b,1GDF6:4gt}sx0yw':M 'lmO~= 4
|mHe j42 ?mK~ }k]9 pY8on-95Z3WS"5g lY^]:-dW;H{p.mtn"us;8;ij4#^Q?x7_}R*'m0_CmXT^Zf5|~ e 0S}*nv6v>50ME-,imY3mPmdjfRIsf<F'\qf;oWfckX,{SF3qxTNV7T*qB~F{,0GYfg>E6_ gg'eKKAH$[[[BAsqq1A 7hGTqKJJj?,d-7/4?iZn#Kmks	en^s.NvM@aIiIIIA0/DQ)Ko)aml\ .{`vyYL
$QZJ))V(KJK<=it4UiQKjDYX  Ml2 7?ssQf(rs(tMa9H|jNx`oP:JRZK*U:I%24,$ZZP;10>Y+J/zWJai1 Cnv6N.fOstjjYw9{hGkw%<}, &6eTs[~\\=^q_<hX3_YvO  7'gn\='Hf5?u["8i}?_XX^/Y?'WMGGu}Nm"y+?b^Ia\B'O >>SM[zhKJm[YLhBafo,^3i^_[vj}@pCw"b~{b)?nXt,VV3 S68b	+CF||U]{ Zp
I WIwppT_{{ <4Md2s|+cr%N$^` X!3' @zf+w:D4'HXsgQ4 kYs|==ApJanZX+\"
V :x\y3f0&\gf04g  Km2`gj)TC4 e`m;;[oOw-E2,%$)JBRih'AE3GM&$E!}n~UjMD*[+W(9::?V/-O1UD 9yOKxqepFiC$ sWz iK?}chJ,M#Bxv_83CBeq=yVqZ>}Z^?(..1j	K
}pwivq)_VQI;QOynZz1pf4Zz5"[ !T{?z+T(1:;72]f\"wkMyW:3?/G,ocW^p6AYod7Ht%~Hq{_lQn+P(z@4wCbb_KI$iooox%K6z .ur	V"4!I*5kX]T,R[s 8< \}vBaa{{yzzp'H ( 'qO3s@FQ  a_Hm/OO/;7vKx^KeR[RiFi-6HmbA2Ht"MHL+(~J	IimK1<~>9SCO3$j(?'}wO'_}7_WnwS\hy`(9))0(H/<g		~e!$4B|s=\=	O57'&nyv[
A%%	O14CHi4z}eKOK;M1g_JKK[6n8%q:88}h!_H[d'^5M}D?,)1"/`*-sZ+cIohM0bMAR:a#=f|q`p/>9{p]\w"w5~-&b{*%+U^_;X,+)l@JRLf~6SlF^IdY wW>YKLEhe%bQ0*;	UR_P c\wJz'>  a$CS4dcqf\1l.\SMick$ldY/A5wKRAbX(Z"Zo{T&>	;W RSrhVG98;fgVA8=\~g.6??AC6k]yE\_TmDKzmN{a?xWq2sS:yr&z{'MQTC_ B|?@Rr_&,7XRzV.e+V asvw[og2+W3eD[/u?e
lLVxk;$m=$i&5nGwH4wG1JX9>W?a#ei:"V }5i]f[Rm_9ft2Fhr@:`gC [PYI:7_^7jr)k6;ws_K]z}^|<vvv"C N0 (X:q'6)qqq/,7$@$qy E?rXQ\ 1la|tP(UqO+ ;sbX
33SGgD"E\"DR'Oo+m@p8P^,CST(ZUQFp\$I>;":6C Mf>^4Q|/l]"m 6XbYr40 ]BJZaQ"/IJh07?|>K1qI>Q KR(.uFou>e/)R;oLk ; j[+]K	\9AXr/2dH~~j:|;[=2`d zo~=eZc0P	#-,*VhXX}/hi9{5LC
Oc	;vkW<y (..|>ot"h	gIJJ  $$SN'Hff"yb` $whT?41SEk.,mm _Tw3S[nTD$" 3fX, 48%53!AbrE{244Q"ktR6(A=udl^0@6CA(yY23b6H2m BRI3PT\gNQk @"q\XQTRb]L2E H}5nafFNb5mAh2R{gR)]xO.qR.~^`/kVHHHRw}WAV73b"dqZ?B	5jpNuHMM ''GJEQ: _v_[ynK4,d3th*++4:qa#E`0KmQ+;wtqqfX(,)MNJLOMahaGPw K}EYOIZ+5,B~$T](Q+={twYvjkAhjIh Ja	 HlATRQ(
J/?5-SYPSV/
 "!0Nnm?]|pq%6c.G=zhKJ"krUYs)o[-3T] </-;
E)7]xlZ5
}L}>,K e2i^]jM:UqX ` WK fbbzY&z{y$Ev18q5/Ym];8J)Uj\=} 0niW5_ND*^pDo >ID"X~b\.k \o tousUlJtr<{|d0B!L~H_~|Rf;Qij ^, NR(J*prsFF6#s5S+PI zU8:?#A} os ;n]{g`ZR*J|G<v}ztrvjjzh'w[[.x\6 8wI0+N:1&vvruvx|& R IbC,2p~EAzSLK"Bgg}n ,UVnJY8:9)
aiB ee
 kkk+,%K= 0=HA 	8;wI	OGtrpr JT 
Cv"HMmKtC^'Qtxh@Q}Dz*Hd V  . 8j~z&wABveGE$I tEtt1/A|0Vo BH#Gk:FKiKy?7t0!C @.jE"}F 1))	]gakkk,tzuM<+,EdCB?IJJHI.h!"	R$&	yt?"2R&2f}#HlIxhpBFR84h_]>jM$YOm$IIXb1 CCH`K E z^<Bgs7^js7,^,,T{ge	xK~sLSwY}>
3~<w'[[yM[6mP[e2[9s%+J/z1P*i)Lh,U3R[T_
FEM)y|zzk&*;/AY#bj:B*k:\5HQ7q^+2)VYBWV" X \UaH}oeb3$](?ogm{oV5Dnmmmgy m$`ogok 6!EFDD%DU8QR%:'dl!{j6)BiA$0$I0HB, I 0d`p7sW\?w[pwN-.,,h[$ [&,/aB!VY_%ie+ErC4=t`on~WPE1nmFtY&VV:()
,JJ!BBrRRNmSj:r+_EurQwnU*E4Eu-8#Oi+7k:BCd#5.[TxWKE?xFv4jURGMB!T5s*_IuKMI%.G=sBO<C!Bl-
3!B&Wz^msLl|MG[A}P B!R a%-)E"B!T @XpLT"!B"XDHD,SK04M4KU*Y:!BT>I2F&J=@ ,K0, ,reY B$"	2>GB!cL"ddXaXa	q VRk iH0,,AAHaKWSj/Zli`;Sk:B!l0GI%2eh`Xf nD?~BjF 3e	"1A,2Y!jEbEaaQqa Vyb6{<ES&DCB!*K%$Y`;;m wWNRu'OYesVv  j*vBL22X,Jb$ ?.:uzM7B! HNnH$" \?YEto68w +IU{UY2IJw(**.-)/)..())l0W_1msSX||>t **,/}e4A*WoMu?[_$'i]0%!Bu	[0v?U eXs}oOv" E&	)+8{7{SX`h-NPm7U?> *6<pv(.*Z	34$TRGF[[ 6pR3y;bY[4p)JbYfidL}wO5 (O"><77V}pB!*|!N Xa\sv|an;y y) $K?&Hu `"l|co\UDjr+V)JJ/_XPg1oN*'eKs`$2X,:q	y9>n|.6P j'432Ud)l+$1kk#[ FL{ObqoO{gt={-0#\|Sg59k > \ef	
] zY~y[Z ]EVf:z[Gn#Wo?oaB!
aa\'. ^aOO~P#L2,2 pq=5_jY]x@L>{@ujkH<QnO?R*JjFcvd`XFov.~a?oE;o7;8w?W_{y/|h{rquO{k=vuu/.*k@ 1z	=wW2hL}`79UV@4:;Y->"	?wM'N4(8v_yz_V$[7[!BuO, @bzH$a0<um t*UaQR^S7@6MHHKI	gYHIN4i$I e$'?m\;b	KdaxObgM,U
c3#
;~eYL3~lU+
EHE{W HNVWhAQ%y}~y4vwa7$5fR|9Awnt.>~.{mjXBWaYX`aXaJ% hoI$$I5jmT ,,yI&<gY/ .pqs4;sX`H,YD C3z$Fno:rg.}7fg<{U*W>L_X p}]z"HB jh\+V2i7
~]53}V	o<[pp EmzO$%%H7B!Tj- 0edr|a	`YaYeYK U =%IfNT5Z$!\ /XWG}hj)7a6^fduzO9XRM~@`06P"7'o{-[G?|PX144|Hi;YEy{.[MB?$Z[mq~q[D*%XX
 I,\^Th@~2ovf--oXB/,~=i3Sw=''{jc#DEt:fy	 Em7~ HNMKIT&}WL*7nAd8x
2MPiZGiiah]KNtTk~ p!BnqM6O4<N1~eB)o>ZH$"	l7U^qZcZou8B!2/,y\7}w=//BH$"fif02,20, ,!dRL"E^1
>kZV~M*B!B5bJ0 AHDrEbpy>lH?HEB!Bz}ae K@A$$]I>B!B/Xe!B!MFB! prYRep!B_Xb6aDMB!~#[nB!K#B!TgaB!P>B!BuvKSLz/oQNW,;9^3 zysk:"BU+z$XZ6rXozk}y_ojj/sxFje*;9>2p
	tZi)V za!YWmYB8
" e5)B!*t_(uh j"[r+j/I,TOVN! @")D$A sgGv8{>	$e2erX,J$!I$ wGt!\i> ZN*$E,f2~*bPn}ZT*kvMir,VI7qt$[P\ QJj
whQ:NKiPHll,J XTF>IJw(**.-)/)..())l5WCL5i u7!2R04f5o n^T0<ReXjr	m>EQXh\me>...qt[VH 7o> *8vL8I W^,|7 
UlxhkiVk!}4X 7Eyxrus13aO$_8eUZGdVkYuvvHD Y
KQD'gdZG:yz<oLG
2]s~s{Ut'`Y{L0[[\.,Y"$|+ky{ECDvN=<]R\G&'	qNy-N%@6y-?';3Ksx$2X,:qCIq1)"mvy 0{mwh`ws}PXW	LBJB7;orbZph@Hzx7JM2??3S:b6}S?o^y-qpuusHl$  ,&+iJiTYw3#Bc }yfgfXJV4NG7($3#GoeX. +
E%j\_Dk|.XBS^G$'5nT(gqusj$<''?8 QP.dunD= A\JLb^/i_l-YfCG0h;9i	O%jOBUReP2%qS%Uie	,dgU[r~Z BBvuw$sKT)5i)e+]G ?nuP^`Vv[/H[;;;JO$K-2+!@wz."ehsQ:X"RkkgK+ 6lT
! 	eIJNs}pdH$g&'sonJV $v5c&4jE!7FTj`e297E2,kc#xC!a$N\-+i&^XX"n$6?P&DFhu>Mv@RT* 	Jmc#INNo	MS!L/"YSGsr]\%*F.MNL h"NNRX? HOiGBhUBYtKO _nJjJZ> IO|nubB+_<z `c#OOWiV/=!>X lle,z=0hiI1G{H22%)X,NIJ0V<Y$o`'Tw $Aa aT,R4$,T%z	q$IyA;SI -8$LbGK>d>~XXl-GtM{{D!P%Aztdj~+kA
uQu<X%bX Xa,M6Yqe(@24C-|Wn`B!d_*;oe/^za<B:xb/a -v#BN}w/irG!B=^B!B/+B!zaB!PE!B!TCt!BjAB!-!T-2B!jjI;lWDuSp/f!-MII$6riv653RmEd`0K3e(iiYE^5EBr:f!-$a#99Nd3UCu@i=YJ&hfXa`aaxJCYj0[B5kNRmv)ZV0B!T"qrM+MU4w/( ?&amN.Yx h4ZV:B$Db 88%fMyo^M!/7f4lDQ((tzZ1B!Od,e\*IHj	  ^^ jyD"%w^=drD,&	 *S }TS7h

HINGtJhe'!BiX+JD$SL/9z-)H 42L&IRhiauVMOhu	IVjZBYZ??,fNWa"B?K}DDzw'7'{qA~s3uoI/{fP0AJI/)nn=jZ5I6RDT7R!BiXE"_gV"-[ PTiq[
"9fI$tQ40wh`GK2U&L8  >`?v;
O/svWF0v/cE,~d$U 'fDfhXHU-iqxI9y	Z-G!PM|R,KsA:z-[O% h._~5bQ4iaKGiiAo#3?ud%6o8E5oe/.7>pl?dmr~eV+bRrUhZ? .WTaBU,g)&eJ4: _~ p7MP.;oDE MSEI#& GF6&Z>~QR <yYsEI36q%/Y>  k4&Zu^o1h8zT9zB^^FL$g<vH]Cc|=
7b3)yZN%C/eP4cogC3Laq7JKUZ#BfU>cyvtah%Ic3{%\H<4^VD".(aa,I4;l;k;L9KF4m"6Q[-[G{xeefhZ{;'<S\o7lt[xg\Nv^W,]d;F?2[Fh+Sxw 2#Fzu{]{z[E%,>}F?j.-3WiuVG+Jgt!O|Rv.M3]ER^<l+Gum(\.e$)b	2,K %y+W(z.w>P
	'3 BY  9)O l^X8D-&r4.z$  +3K]" kp,haXm&  A~
wgwwo|VQ4ZEJ,{J!BT>c)K)&IMK%Q4V}S'b. H$(R[qBkWon\W\..n[nSgOiGB  59Y5<<Ztss7,4I``7_.h wc|!F4=~`aH.  U+}*-)H1kVGTn.*ujf?owmd/w95&B!*N"0,Ks)fhV+?h>wP I(C$A4a8W3nzZroJ'P4`cz36n"i< ch;;v-GtM{{ l\89/% @_7~\.g,/ H8!wpH8sp,%<9y|mC: ETXht:D$HJun^n*g.V,P,IB!P|Bpe2L*hZh5Z(uwkFA@s4vwsuH ABrzQJe4U v) -CwM V*auWveD67f[gR <X&TIQl{tmO7b%Vh}F4B!T*<A$I"h4&ju<F *
 "z,g0-w(wt/u6k~nMz5Ht}#tEQ4_EQ4cB!T*t:R$"$Io}u  X`pKR[> ~ECaBm bC^&Ooj:Z)+#{CQ4ghKk:LBU>c!q \n,3 $ <oI1p0B!^3X;?%Ry3
BD3:C5B!>84B!!BZ B!^#B!TgK5B!B!B!*^h!B!#
\]SlyQB! 52{:5#B!T$$$y5)-{>B!BR~QIeB!BY#B!TtLB!\h>B!B071G!B2`B!Pdjp:^!BZK^*Q(xt!B:#B!T+t!B'#XPB!B"8v!B: B!T[f_3#T:u*,,0]UVqeFW@*4h\_{S1f-aC;6<<^,viE]izT\\loo	fmd@hkeXPmj;uzZbFX&77SXfU]zB10/p:?yiNw IucO%Z6;wt: Rs/\:x}c$uss1bpRi]^^^ l2.AFh%%%zrk?0$bqWXaji= ;vUR{;p[t:L?yfnn^?~7o:;;RiEgoccc`q={UV/=z=v)u~(,,7nC4L$#<xPLnr>|WC|?{pttkX\(lzjiGR7 a&LT 	zGW@&5iD8E/Sjf,Y;T222QZZ?[][mhSy:T  :t:uJrRRSOgD^z}{|Ie]8ijuGR6 e,Y7t !T} ?tGlmmgv^t);;zaI$W\m[L0q~u2M6kh42-[>}UV;~aaaf:7kN"4h`yYYY/_(i+7oOLHH8|u6% ,hrrlmmP(,_U/_kAx{{'^z
D^^^}yaY',ywy JJJLpB!2(gaK:;;
Py	j 4HJJj\*#Hv-,)W^	2dHf-[oNQF6m*--5?(E"u:;Ex{{NBip{hS^ <<<SVXa/_jo}={%.] FtLLL=7[#[W_}%guAA"tCx>a:x Aqqq{*WnnnZVL:|jmmm@*>~/,y4MsWj?azLpuu]d+O?9s&7%--/Gz=yde
^hS^@olUfn*)w[Io {O.R5j,FN		7jHxsW_}BG^Z>pA,had\:5g.+3}t`)FT
y7~w~:MK,Dr'&&ZOJJ7DBr8p`uwwkqO}M4<p[neYVTr?4l0552_5RRRD;;;$B@/rsrrbB/LJKK;v,$I:u+w{2t:4vq5jO4/B0`QF\NYx>#d~~>_r53gMVn%sssSmXPPO/))z
[i}tL&cfK"
aNgVz`+jz7q}0	b(d%11hw7nGxj,aZ:;;YF8QoH`GR[a[?j9c~3+7B![["e;v[!^*PtP"Hyqq'{? nf
;vLg@Rd5I]xeow_GBkw.--5j%0m4a6TjQGn _N8q_~U?^z+WdBM#Q*vz+W~
5Bnt:ea2iu?-6 \.9u#<0::3,~:["0tPaXZ
[	 t:5kx'OV>H^``?wG*\
Bt:iI&	:T8kUqFEEY\r%55h&Mpt9\Ux/^X\7nu?_^Xr-u)kV***9s?6xyyU>B=oS7o\fI|)Z}0$d7|jt GFFOO{XR&fmS!+ON5+fKoy_?~<wFoU8T_+.]{Oq	!>Bu\\\C	

2,#<gtax#Wiiz+W666];F-D:SU2e
?z%YiJ{	ptt>|pU[P-hSj`u+q7wm `=K
\$I1:~rW!>!T\6UK. e%Z6""]1s'N=zti7nw,qqqt7o+s]2:yd:$0 0cogff^tic-&SEGpGl E	K+_??qDxxYuvvF\$B=OIB%x!N h-1Qiii53<RtaI d-IHH5j?WT*}_^RRP(\%j2?9(JvSVO8jt>cztQ~UuMyy6[	XWfKz7\ \?S|}}?+"];+.}h,T*;Wnm*oAO6-99YXl^^G}"d2YM+7*{:+V7I$Izzzv}Z_~TWWAY*Up<A4wc*Z$9s)#""XJ3g'A!!!Wh4r?uxeO< !!!W4B!j[lSBE}	L!z	j*==cAU% RsJL)Q(s3\\B!P(F c1G?}B.9r#G `Em[!LB5'Ot:7mB0LBCWOR}B<B!TkavvvRYfX!GBw}5BGB!0G!Bt!B:}B!,LB!0G!Bt!B:}B!,LB!0G!Bt!B:}B!,LB!0G!Bt!B:}B!,LB!0G!Bt!B:}B!,LB!0G!Bt!B:}B!,LB!^ cC!BbYCbb3tx!B 5B<!BY#B!TgaB!P>B!Bu!BY#so>44444t]i~j)7q7n?>22iSxx8WjZS/7^dIY2g;v>?|6l)4_~9o={veU&E!T0Gw1c822]v"w<x0_E M~{=z:uj@@WhW`o)wce/^<j-[XK.=p_~oeddlqV7,B>B4hH$gIg%_>p@1IZ'T*SLIKK3U`[^pau)b|w MiE9tPNN uU!B#hO<yMH$sqKT*Vuss~;w)O={7\t ou:7qA~0 JKKi Lx!:^;v{k0 P*/^p_~9i?|Q]tYxq^^ [/3d+VvCj+'N:444O>U
K,?~W;;w\~a]vIzcF#H[g+W4hE={\r%Ua6m3fLFFFff&A,!g,W.))WO<	 g9s& r~R\h;J;N
 /gj4h?t[lYf+lZeNc:n	JRXw	&<z[ /Y{T(#Fe3__-[ woH{_ua59 t!Td2n,'O3a] J%EQ~=zW @II	 <|PRD"n<B |q[%p+WD"\oo3g\.>}: PO?@&M>Xvv<h7sC:w }m1+GcMo! 0G=C=zK={DF5kt\v.<ug9~?\;v, ?{`*u h=r V=,??68qba6\\ &L8tPVKi*B&""Ey>}p1b`]f.GVog<<<jvH$[`~[%VW\fyJR
oN $&&@jjSE=B
`k2d`D///nYXKlT*-[f:3,j4F}m.\joc*s._iX-DN }@?~~F:<%K4jD!Ty# 6#FQW_}U00._> <%,{=}u9p wz///[W*?00 >_g;^XX -x&M 4MK &J-Zdiq}</?`Fe !Pt!Tn7Nsw.}m;?as}||D"[mllM&>~+3i$L&C|Uo9s5jy'O3foY
r|d/uo	 ^^^[j,X?3f}NNN Y={.\O>yg7U{'[l2///x#FY B2_	B!zABCC|IS_TbLB_<xt!B:}B!,LB!0G!Bt!B:}B!,BCCk:<B!L!B!3p0B!Bu!BY#B!TgaB!P>B!Bu!BY#B!TgaB!P>B!Bu!BY#B!TgaB!P>B!Bu!BY#B!Tgk:
z#@!Buz}B!,LB!0G!BM?6^8 [:n$~[ U~UdtTI?9wCI`HMzv ,l-\#TO;)/](7#6GW.L_98tNx)s3XNw}*?;O|0nJoELU^}[^[UyjMQS6G77._N!3N_c',P&#g66eh  [vq W>#63mg_eowaovMQ^0U[1|<k`tflYr"ZO^ vj AY 7M'IUrKm320=ZOmNnI}UT%K7?-vJzmUMgt7E5m/YK]km]mV'J3f',P} h4| <9)G@c'KeRo+[-;W$_[ue6S>ey#^_|iE7Jlm\|7**K B%{n6^c%Ml2bFka[{!wn<U[S[l5em:^)cS"1[p+5C%	t5kXr_(%v",Z	 ,0s ,Y"iS-c/Yu@mAn[vV}nbxM^ bwS6zmUMgjjr{E{GnqwQs~a>&o?e|Vemy?/^OO	G^q_2Zg]1$)Jt4r=9#^[;O$yp5w;_[5[	1Lo.8dG6]?$sv_o|xMhwG3{v^~6`l]?}df}s'gEo;'ER#>>ss~xY~u20/;=9 Sn-%{Wyr^/I8wA9o*TBZ0t9Onv~{i*la4G$u7kjfWAk>;#GC   IDAT$R=odwK-]uzLr<s`+'zm{1pFhfK3FWLC*.  `T,(2M^[ UwY1`3{iP#&/d`~O(nr
#Fy?'PLz~*bRb~ELmzbd]%om{f3Va/sG{^{	H}G]_0/>>=[3nW?9[	b[@<`G6B 85~g7_"B|=:Q$3G7:|- ];wgot~a1;5T_>i5
i#3WIXWNd? =O"{94?4K6> ,iDzm{uR+v^	?Fn "<bRTf]#qo9c#3x.*[?;_b`f\SttztF$?k_Y|c1{h|b+g \X1^[:1NKl3tyU zz,U	4Svi3z&~w,.d4l4wXMub{ XX6Rxo d+4FVefRw DZ,m |h-z!I=?]rG,yY*	M,{  uh8O2vzk(BR.Onp#|j[T0n{|ZCa `7Mmay${oSq%;jMUH}p<O- A-:==01Fj wv;*DGPX]OOI;'948wm\9S=>f~{uIz@`AByL07!?_TY.s+&D\:
 OOi0mNc{#3:'we%Vd+I  uHv$=X6?nd$IaS%Amy*?TTE)q: <ehJi~ &i;Zi 34US<9';7K--@h7c>HGRv"?E]_=zkjEbKr;)7Y_d7F
_?meJS+h*x3odTUF!}04ugBe&MwIz"X6o7K 	M21Tf6v%+bj[Zg4=A|^6Ew,";v~3]:i@!}Fe
= %rgw$IJ'/qf-wr3Z/g*IeV+HaXh%I`w*yO>|CX{SO_-zmz:6GnA-:=>S@MrQYze|#{  V7?{ va#=fF24Csj20Wjhj^E]_%+R.dhGLcZMJyu_}$-;e\IF
Y/Zf*|6-l=a%	7vJ[qZ rv,iO-}YZ4Q^wt Bz H"3C? l)~=e$iO3bCNnv^RqQ	gO:XMix ?0!)ei1k$>ZN\yBaM}PjeG|I]stV%D3bfc?h<|ZbdG&wr7gv|.|wzu^`G>;^^[TIGX)&u%e<r5f+Mxo73f+XUv|[:]S+BKh
l{|<Tg5eK>MS{o[~y*Wqk'R"  Npo	:851y
cn|	2'wGL]*   8]'[tg&yR$5
F$)2Xen&CS UZW~^\93X#XtoY:/+-2DF	bmioPpVbc_@F)o)2"3Y]3:5DYsM$?hbWN49S$\,am0z:d j{.d46/qaG{mcTft}h+|e?~n+'-A47F~n>WOWP[R  pX,[0o7 &&xu:8g?2Lc 5Xm<[NhELm[Z>g*3pe4_Dm{/mb^w_tR3OXUW{@O{;=93j44)'~uz%6v77.~|WOFGn5ZO{63b>1q_T0|i[Qru]
7:oWGhKkTaG6Qv_5EkZYoNw Y$kPBFC\+ pkq(NUda*_s?{%khKZ+@[Rw|Gl}tCF]e:rXaB*?+b~7o;/?$zYQWB[X[o	~p-aEosp}F:3Xs~=5[7oW/oQnij3J77??F";z2GG~H7O' ]l;zAM*^}UZ{M%J+h*x.|;Cc]V_~N]]37ol^c;lKj#H@SR9yul&H 0@o7LnYd}WNq've?^0d$upa0v7 D"+wye
) [%{H5io=S% 3FWh/kS7 o27l]>176066zX<'wp[Xfwb[Eie}0aaaJ/4 VTiNPSot/'Gs#^#{+_{?g5%=RU-7W%Wvj]b+>!T{NDjNqBBb~Q2%
eN~w<eW~jb XnU_[(Vb+>
7a_k<kHb}XTw}.eN%t*VQxg4V}UMDuKNbcqZz.C(=X$#^~D?7MKJ|[wk>awN5uue5pGy0G!B8!B:}B!,LB!0G!B275NYw|]k65oB'>\v24y5sY)5FU*&U]V
[>7#6GW.LVm$N/|#7 ^g<5H'>sPAA P}s#-mE7GZHp)#?,)J0]]Wf5.9;EeoF{G5OvaG*;}:5WgEcE2L_~2VS_ P]4ku}]mNnIUtFcq
FUER`2uoC=\R"T-	27h=y	 9! 85l?g5 DL^( H?_{W u9>A  l-J< zc{@MG S>u
oIUwejv\uwVV`}#{X{Tbk-.zv\&mEROS}kTM!3btWVY jGO-U[}WC1_&~3Ut.$x_R,i:rV.6)uef4Obk%X%DL1a^k4qgTW/P_bk_Mb	 "qOuK$[V^$ ct|K I
K5m9nw]u5X7dFGs)Rr/[Bf:BY ^  a'Ue6Aj"*y)EhsWeR`[pdK7~}27#S]hUvT0o3qkEiu)Q^7E$|/>N*eYFO.x0~e^fk-F;XnX  N*L!IQ?kGo.s^[ UwY1` xVMq>CBz|rt?{#f>O4]?U `R{'GsCl`MhOU'y=cCOk^OOaw.TQ?!wt-Ngor[3\nVB}{3
Pm^
[2Ilq	iC/MO.h[(K1g>ziNZ|9n6}]nD JwEY.70^^"cVlVA|#Fy?'X/57y"|7g;ahQzwqM]?PV uZoOkVjVR8ATD=Djo	@|?~y9ImQC.QxI59|R5$M<w`'gT}]i"i'Kc]a//7sB-D<v4g:j&;f6fE]Gx;7{
^H)
P41dP2kkFL{uKCAc?<'3d7|1j0WuOaZrQuW/`dG^8kS'@_V_<*yxBw_Y0YS>Kp&	gyS]GKQwmE%#CM|Vlp=o?w
!u'(xumosB27w9$W(ds77:M_(>!(3	g$9.4~9njj$j@X-F$soev]1_En[\3XQf-	`~O1_,qv?{Xevr#+UJb7?hN-N7	eq\=6xv53?iKuE!YM&C
/f2r+G++l&dFaI32V-N6\gKxjtJT5E7+
HVzL~7/>k<4K,AVqj[B(|tqPOK'C^&8Y3$S4j#bq_?2<Pzs}i2$Qc7t6;M^Bu7&?!zb0$uE4V"<B6gOZW?TR_ZHaCkk?4`'Z>+K	QE8#iI~g8au&S;9	s2EZ2+PfS !51S6l#Bu,1BT=!dL*+P2=4(/:SZyo;&o8t{KV*4eB2/e2&;
VOHK,vex.Qk#NTE]ktEW7Yywo2N_)kw;NZ4AMs-z!}Ck^!|anl$<H}U'>mSCm=S[#.gp+N_9tod'Sfq%$4uL< B!GCq&.FrWWp8vBG7o1 !T_clnMNy>WH.$M[]eRdj6Od ;7~poJ;Tmj:Tkua2)SwV#yC3f N\4-
hfriCV#\9;DLUMF5uEt|B_F{)41@:8isbdkp?:hkU-Lgluw}!T?D(kDy7r<>yV$JRj
&Oex:_g{8kS5Z2$^z~cEIc)$MfV;2}pg$2|k&]{*mx':1%tcMD#Hsp?ch=kP(o;4:--0FH77"R*cL#s6s[{	tFk#N2mVu;]Tf?m{k
"%\n$_#O=6cU?/Q%U\()_mo>wk*B	!Y[_/K}^!X\BqmNwlWYsY/`_$8Kyj*SV."\^WWZ_Z(JD"FX,Yr9B0dydaX;dF9hMuwk)?r&G$"3yN'Zafeqpl2cHk:'*(dv=9k
!T:ms\m
lTd$<y=^\N:j Wh^#r\GayY+Fe(kF#i&LWDIV7>H%P{[WcqP*/VzhHs&ke<R;f9npOtU})HQd9FX=P9	['.Unf@Y]Uq
?)Ohl5_\m] cso<% =iA'<ybcBqt:IO\SxGy>2X,:RL:1Id:vT$$1F`;*'5z=w-7Ls?ge/snIBgwNJt>0S[xzYztW!QUyCyQn>K]]$H,OQUiadbg)r3pFB:+LWf%5,`Cn'4X4Hk:'*kE1:zSlkpzwL]<M |kGAqY^9f({^E]&1-;&v5
[;x]?*e%nUW?%tp#6o}iYNl@FqBtwjr23"}#SKs.~M=&v.8'<_n#g4F\"jP|}IH0L&3RU>L6GUBi	g&=a9['#S{&oUJy|X:+$<js.Frmt}E3J2k]qnO#o_H];MEcv3a,~Hl)5E7AW7Yx4Y['},5.|`;RHhEKW/+Lr:5U\Hhdu]02De2*6q)Gg>\Z}9wo&..Q{aTYY Dao1HTr!/&"zI1i!'\<t8W3><H y2Tf%}c3:I6U ^g}SmHz:}m!IM- `93MgGUKnn3R.e_sL/ ou/yvdK$oSe Xbto_V\`}+ ousk` @_f      nf       0X`       l      `>       ,lP^tfZT_-8!:/&Z{/-x|P2((^iGVP^|p)j%R@QN5_sqEMU25]E&=tvL?PS2\&Kzr}[@Ea#(y@.oeReO55s3J9m!4Q6YjS["ZdNT(\v3>Zb [t:)jj}Z5.)[_o#l#ou*'dvoq]u_<y1ke=2)]Ku+_'{P71wj["ZdU(kv3Z$:Lf5>L-tEmig}IC!^ICwk-2)1-P
]/9m0GWH^Lu-NeiCDDmbHsV@dRI_r"NviIP{UT]"cskv"2mHn:o
B"31^m<c)( Fl?er$?6uT!L,-Dev2&gEbc{I
<jL&+~ $uO|gljY!I~o~yN7FX6;8C'2&6bZSW\wxFP\ls_=/6c8MY<:\V[C$blD$!^n)[sn4y;]OzG%1u%y1GJ^z{VWV9a#.oKSLcw5X*FsBmM]<iY4kPj^Ln#HaiN}G:%	,EFen]Qi^RLCxFTMlj.D7jtEw)pU9B5N[cqTg>]dmq<wM=y}Rl
Fy7I8/:MU?	>y1b,}Mg/;+sSW_sh	b0l_[3O&=Q. 5U9i&WVB0CJ^>gP]Ic5;?977Cq.SDXpF'{K:8n_H~mPe.gZ8u>l}lFIj#4Kn:UdUsss(79bg~aqS.xWf'szqmpeU;/aXqbkWs1}Gj8_d TtI!T6hJ'x__obbq}VlcqBE:tFuik{)K6O6G;>#WdV]UG"fQ1{CT0h5\9oy&f6O]TpNvCNcF8&Jr+3LM_}W(|~ge?`H?h,NzT_'JKj(F*{gG;{<(`CBM5iO9w:#7q/Q1|GPMM:^[;&I I%1M+QmH$B!PVD0Gpk2l#Bu,1BT=!dL*+#|"z-P~B(3\)KHNj4qX)F<dDjZahm+/QwZA&5;MXK(=hzR'rMXV{BVZV wYd`B#S>UtpLssS`jE12;C77dx&oakb[
!S{:0\L&e"uFk"L*4aO]eRt2Ji3~oV\]9y
pQu9X@*tjDm5
L6K:7G-h	crp2,wwC'w5;9*bAtx9isc{NG!mkiQ((A8ZiyjS*NZo
m.U PS}qpLssk\NG_!4)P`dfelfeb"k_d]gqYl{gy
v#U5:Mv?JI%|k&];(mxwP:JuR4{m%!U#3AQ#N@B/QKT2#0&Lo,|v7#yuqX)F<=`t&~`]/|ij3]Qii^DZIyXT:N'N+.+9hE*4Y	rF[4v_|J`q
U eRl|G)_2;-!*d3+I/-I%O"BRqI\[XQ<F0@09\GYIj_aNwlWYsY/0Yl6Oi>6{-2+4/Nd9#5TvbRbT;' z`S]XBJ"y#<#!$k:^r;B9~L81&ra[&7,Up9y]k=-h'b>GJTD/LxuBGWQ3eMbOOQCWkTwF/l<9r;0BZq/Vtx	4GGmE1.t9|3>+1Yl\QF[3mAm[\nHr,MSWyV;+(tvK?wld:vT$$1F`:l5-4(w:O^2In3,lsk^#};/n#gTd)|`O>vcs7dIjkU3g?w\v'H<k<K[[,B'\Jj(/Yd!B?}>jFIBU>7f>O7.s8t(o_?FBsPSDUE7,} e!S;}/'1@nt:b0n!|aS]*rO6)GPHD[SM0QRqcZWw.L:k*CO<v0sL~U]u98.'dTLBO<cizo(5x8QrIi bd%[j]2N 5kw!XTUsz#Uw*]e]:sQ&?{[[Fs6)WaE;.3NT8iX{+/+ToaD [;Vf{V(:,I$iO(zqjjj9;JgsbOmqGw&}6UVV6H_MgrM+g@_lcL=(?%p9QC^LDL/}WxR6T>%oX+=^f.M +{8]+j[C>6#:++lLMm}IyKgGu[YCB`:i(/R{"'saFZXs/}.i*+zP @k{5]+cQk`{[K|V\[rXTMs#8|^<;|FSdP^ :\>8 @{5]+[ y-x4.9t.f"##:zh} 1W|kx
h	Zj:X2Zv5&z3l      `v       h       `v             }       0X`       l      `>       ,              `v             }       0X`       l      `>       ,l}++[      @9oZ       l      `>       ,ixc^;J|i/#B|qd0?!B	M5a'/0GuSm44)Nx8lcIu^f_yy/OiN/77bmR:^9T_Z24C^,UiiA!LP;t[d(K;51suj`~kN_!I{FuIB,B<K[1|$M)WzzxsaUQrL9/lzfV>8Q;wORi3}Eg[B#w:<kNCZX(VPu KjMP((P;e2Lz-Z:Wrj`>BUoN[`/Vx}x,o	L1}}!-dNU96-vQ}(D'>V?,4W0b0BLA*:tLN:xv].{HKq*F2)Y ED[!mEEA"	K}VS[bO+Rhg}K/KWc/^9S(#/Ic=BH.&?J-JY6{7(S]gsTFG9vXgi'+Jo)'hK5qdFJ-|h vNzJm"[-I{}qco*uon[
{}v(fV:3#@?=v<AL&+~JzXF3TO|uE/}L|TT]F6b:NH7Fn]d-<6|#>8L<B(fpm~9tJLsUmq?FYOKON:e8c$b4BN5@uFBsB}9Y(4==eunL&OI [X}_[6bowM\J?5"JbF<(	n#PNb~i)( Fl?.tt9:	UM\"-s<:\V[zV^LhCT-.0)2}glwOM":/K	'U.4\8'+YX~515wJX5X[V=7a36N?]2(ax;O:M-EMHjV.51K9QY+(A5#']#ALf#%/c|e\WDUeFX 7thE6b9&<7# a.	"VU|Tyh}17^/fo<r>J9klI?gtkrn}iMsop3{V~1OALfG%/\3B$g{K:8n_H~m{.X=K}I~e%mqB<}Vc!dbndxFX6rSQY/(TnzhgDrB%Q;QH# CY1fn>Q"Cs|cBw_uU#wtY&?umeB?4sO7$!%~dRqcQ?9kL`Bp.m(J	@fg/;+sSsEFDe,|wNF=g)v">jfj?o{:	!K	'U.4!+Q?7j2y^J8Bs<~?<YfG^(M~2@}!Vv9/(&GlSlP/,qO:~9=z.\/RQ#wM{~SMv}X	%[gUQ%<St:^nA	]{8W=?UZSmK=q./30M&>
F}XY:czr{cvcat68":leR
e0xVS<@RU!^6Tf%8yU&f&$	gDNrC4
BG :=}@mPkr=6suFiq]5<E~Z[
!sog#GN{?Q?a'/;bb"Td&t
]Q=IHoaeR?x}"~t4!8KiVcp^,4E~NQ5{L"n;Jr	5>TEVd5[:!dO&jgP!:C!d_*nt2I&YEwhE6p9&=MD=:EJ'd:F;7BE/#1c*N_9toQBM5?Vd4w`2<+?e)@dJl ixv]A",wwC'wQn#]I3Vf'B	D5@
6(eErT&i`E.5Vr4p;c1{NG!m+b@FO!}G $(pFryr\Sv	a>2MmaN[*3vmw4N>.n}:!E%[
!S{yQB\&tb""Ls}!\)NSgeWKGb0Y9."luT6rb/IGTkIv.nxib$KEd2y6G*dR	n"*erI|A?t4d^3^jtq{M=r$N4#-r&X}iBG u* #v`e)&'qRobv5?SX|G_1d2FfVfV$$Q\" g*KShCpj.43_T;dPnW;kmUUL6Zd(J,im{gy
>Q:/
;+;PG3DXtY)6s=5xvgi<`YL!NYw>3oJun>N~|`G3d?I5X`D&qvU<3\LWdFeBT$!$7)w+ v;JS2dQ3zO`!81F.<TI5>6TZaE`yN'A0aXkX
^VQBEjq]mLY(>,E12XSFLqDUlf0Y8'P$!x#K.yDGTv.qfv\/RM\>	{DDj(7urw>-#QTKElmaNcEk-yQPuwCK	J@*!ZMDs}AD1)gmor@\#L*I EbO;'/
I$iO(zK 7)(O(4)X$O\SxGy>J%_?FBs2ej-sqVoCc*rCyq~m|kGs<UIDW=iay}nQ~9ddb17HE0X,e=:
efD%	1FV]UvSO/=,{l0P=3Ca_NYGL*QI}I>fKS1O"_?^P^|+Ryv6}bqEUe=7Z0Ylq]ui+,PX,6G`vHro?\\9<;M\6q	fYv9&e'Ij0X,yNAt867_/N:hcW,u3NJ:e!<+{sS}M?:[uK	'[;^zJQ#*\S[zyDQ?76.@V5>:|@vNBGW[RTKP'UK]Wd)|`O>Q:/
U8K9e]qMez'wf.iO"tr10	agrx&ihT(5&m"%1{aS+?U^j1ya.IYjfW'9=4(yT!tdG^}6UVV6dcQyy5h4sk!-I!kq{)>( AQ?%p9QC^LDm@ ,
4_F+`0rKjWtFLcUYWc0- B%u76ZBoO-{OR.e_sz} &H xj .E/4?`9~yryM+l!1aQF   Z|*296o;wXC       `}             }       0X`       AmRau/O_b*OqUTd&>!k{I}IBHyFBw+}mI8uS>-:3oIR K^l55kNV82zuKHDfB0):v4QxQ6n5P-p}o;joW9"K/g_~Vzy/!4os0SgMR|B>+5b:1U)G`x_dd2Y,UL`{cE'{oA&iJr=
y&}<<+{T{>maB~)W(x0;^J 2YVI+io@kZ*#LBz|!BA!OKS),BkSgdn'| ;`2	lGzgieVxT0VkMxvc~O9|Vi#)G3dzD\W:O^_5t/&u|0~AtZ6R\mV(
{[@_9]bh}b=	RQRy
rRv.Ss(j&hH~P%m\+g$2p~lxacsm
)^zID#QQu9o:|f
lyz#6#J/23;z_ iUW^;^ocEFD8t~SV=#=tE6m{iiq;RPw2t?q&gnn7;8I`qleI%i\:{cl{1z.ed<#(pa768tcF<3\cH.O8N.(Lu8!G< /&Z}{0^x4#~hbtPzh,7Yo,Ru [Xp*kv2;d]XF??Y/$
`2S.)y5K6E0n#6ca:j|hQ-6"boNk(PZDw3urWb2{}:jgH<gS/x%mm>a$jdB(
B a&?aS/o4YVt=VV,rFs-RbedE*[p<,"O#}mJI54lB i__sh	b0l_[39mk+|t5j]{cYFN}GesG?1_;iQ#wM{~S&9v^s![gcDs|cBw_uU#w3y!_eg/;+sr8t2qdS8kW	w@xzo4BHc'6Y!s?$!Qu1#60NFPXv<s|;kbkD!:7c77JrxI)Z!TWBuiNiA)Fn[<a}+KLjpyKj]MfZK:o	O&=Q.Z8J\SYe;Y	bt>jfj?o{:EO&Yv<"YaVR\W/6kr+#$p.:Ujsjs|&cC3	]m=d;v(nYO= 1+Kc`Ak&f&$	|P~kYI 2#u!T_vplL3Ny3a+9|7V`t:S 531#B(W<&Jc%JQ3#e2GwW!._}&?O8F}XYJ.4yv{M:t".l#Bu,1BT=!dL*/- KCyj0L\@?o"Z.dj?)UkB^dRI}i!j(Fq';|-5#Is3"%dB(JBj;!DpE=(#"l2\S+RVe'.BtM5;lWkxj z@A{`2M]<7u5p=I%KD]q!w;U7w+<pMnZKje,wwC'w!PV$Ie&kLXY;!$il+42ui}&}\0GT]6Wf'urhwycVrC&tI%!= kuZiGev2,%qQ$?_Ay;Vi[QGCq&.9"d2)2.jdB6@m?Q!+L\4'J1T&ZkRV%)4YKpWe-jC5F~iOWeMKr,qW)+2_ZxtW, (Sw8!q{M=r$oMA95$'^3^X,vY3l G`u0226V)}QV	B,5/IG7`t&~`]/|KIv.dZi&;+dRh".ty"Ii:Ysc{8kS-%qdr2LzuGRFz3##	4PT2t,"TRm?"4NJ
t[hd(Jl	Z4e55n5vI{U'!Q$~OmT|[}Y|kW1XeRM>!+R>%SoNi=!l:TWbq
U8E)U[L=$YM&y]k=-yN'11aXk\yqb\&yp!$&WrS L]O?2hx#4Y[_/K}^w7n=z&.Unf@	Q2?r&
k&/;+n_`N-:".	#%j*WSCNTmaNcERG19\GYIqGdu[}J$-*=TP ou#tYS5\{PdEeNmQJ#^j)~e$I
7RWe'.BtME5gZZo9m[WFPy9Fp9{gx]z3^p=kevr_Kbgacc@GI;LL84iqbt^tDIkI+OT_VrX;.|+yox9{j(7F[qXu,qG91YlE5/I8 P Q*j7Oq'\Jj(/YaO_fFQcdji'\+-z>rD2HhN1Ij{*5^9X7|p@N~BG{
<y{js	o.gqEUe=7Z0Ylq]ui+,$'_=`lfur]cU[;^*|voeV^9x|C?qME_S/Y,`&4xu22=T;|k3^E]:[us7..02"CSpyKoSjBF
"eMI/N~<Gw-eln&yt[5JAYvM5IVPna!4_&nB(LoN@FCrg6AKcPVZ$,ILH$ds)4eKlg(|&cu:l
m7|BLn4*o_lXT 
|!o?o|oi!'\<;O/cvpRZ1k@m$KDy16Nn?}j!Z@\S -Z=|[015%_.>CPC?g-C!m9	g\@u.EA??z)4@_@}@-)2/V$>Pj .E/4?j=;/yv{D?hdjoE/[h@+nSyAWk^Mq
B !:O*296o; M^s#\:              }GuC1xp>       ,              `vpkCYM=&Cp>HhQu^f_yyU|>!sqZ(E;crg7@]Y}   n9SrL.^=~kQu|c3WQWpUiD4]k-G}ge/JqOr*}3Xh-G1-hm&+'!'h:3u   nF<bnIe0giuM]>Yu"qmU[	nrT)oZnOM\\,-hm:&r
    WPs'Yusl"I u=j]99bIIB=    w?/&Z}{0^x4#~hbtPzh+/<UQu_"#A"n`G?})9|HS=QDY'Dh <:\V[I%

F!p|+?^7iiO^=sa@,#Aq1gH<gS/x%mm>a$Pzd&s?wc\-sN<feBr,f&YZdrk\cDe2<ovp)f'aqn2LAqn0NkS.Q@=1=~\`6MVK61P;?%M-s3d2/R}KuUFZ    EvhJ'x__obbq}VlcqB]_ z.X=K~;Oq&	g=nX_sh	b0l_[3vD'Gb0#/Flee,|wVXM:{QR~J0Epw&T}m~v?tRzhP+{gl,#Sn_POMCeG^(M~2@}!Vv9/QiqQc!8|a)Kvv-
UL&Mg/;+sd&VIjgDs|cBw_uU#w:f8o4E   @[}Ck^!|anl$<351#;e?~kqYO= 1+KSu2}dRIpSdR?x}iS~%@a2SDmBN6ur2u)l !?g8vm=
7m
CL:^[;&I>I|e^7"lBB({E*MUmJ_(ttzG%td9Z0!krx[f;-V>pLSC-    tu!wyw!dkp?:hF2L.g:{Uf'urFs'F'd+G8;tAFG9=0B+05wNVh&s46kd20!2L=5w"+KSj-   A#5?SX|GO.)[$ iD0/&B&iRQ|f !ToNAsMv%$TB&AX?nG7V|:Vm{gy5A\x7sZ4U#3+c3+QP	odRaM"O`|"1    `>B&.UnfO)X\BqmW|K]I^UNj}iL*eMwqD[::uRqe+'G?T"3B&1nR<%](	;M^wlW]e3#arB!6
$v9%c$
t?9r#4*.Jp1r,u6-
UINf\%ATU{fE'L&=j		:M^Faf+4!RO   mubqEUe=7Z0Ylq]ui+,1QCcyox9{[`T(5&m"%1{.
F&U96=9v^k7zN;]'=6urSZM=)B{
<y@^cso<% =W,b
[,UW*Zzxp{|P@hi$47sQU#=4(y&
|&H<k<K[[,PSDUE7,"ttdIr3boJ:|pS-e!<or}FiXw~;;C'FS'w@B?}>jFIBU>L6WUnWiS	^O U3x5<#gHtdGo?#~pmar29\jg>~	r?I+|oe0Ye1ur0d"nIB(\    dG}k@;rO?%p9QC^LD%"1;8)d??n5Sg}    mC>6c=V=|sYCB`:t% \1Zv'    `{V\[rXTMswW`_V[_O     $p3V}!6p[EF"G ts^(U7l?ggm    f      n(       {      `>       ,              `v             }       0X`       l      `>       ,              `v             }       0XV@V>      rGpu             }       0Xv<E9[[<aOI"jP^|p)RQw+})O& #R.=g?"L{82tLv#@_F-ahG-P^tf2ewZ'ygjC$vs6QZ8Qm}hJ}ipg@M:ZV io{'G}f7s3v.Vlc+bcB>+Yzhbrw]#:O!Cxv!]>\uJU2ISc/.:r's\~"vDUe-xR32GGq_
4FmQ;wORio]>\z-tn\&d	>W_rP^v{j Uv!dsB@19kvkobE6!|nIU=u*PZ7O-g0Y+csfJgvo[#6ifm8|9| LfYbipKsqI],L],om[A+: >\&k(c=QJg)k!$Jt%*zmnP:c%thFikj}@YWBkI{7G+s5K2Xn?i%j&mkkW$)m.$Wd&f:__VXn#]?zX{7ge9'sn3VI:7C&'1?-D?zx(ioC'!R.Us&z%-=tE6m{iiq;R#eRI)( Fl?.tt?ssI[gv,dA[]'s
9xeyv[0v=)sEn|5*'aWf<o	BG7eNf2Y9C|Wp;qfB6b:NH7)ZC4Om"#]T}{uBIbL]I}&3^s>^USG>8LZ}X-_UcFP\&kbr2;;qEBrX4uXiq<JB=>*fDD\[6bowM\J?5-P41hjN B:/3zq},:A-rZ)&Urp"'P'jspv7O4
!1De>qK;?6rI~t -Zlj!T'3*\&4DDn[5sXT]$p-C6to[42;is>CEXyR9`0Bvq!2?b0f-xvwW]^kB%Q;\ogJz9~+BM!3'w},Dyy%Sz.X1_{}%Sw#S+Bi!'=:/Sunwg=-S=@b/Q|PfzhPIbZu;T(%cmTLM\>*<bw\qiAL?"-Yw}O4ij}K>~SQIPv'}l$zEB&}fd_B]Syr7uz	e2;kbk][hbd("i^{ DMA6WN9
{d_k9L6K7NBvc{-Bs<~t Zfj!TW3*r{?Q?a'/;Pzs}i2$7&o8t;vCkk?4`|cb\vZ\W	BsW><+G CPWAce)mJz}?!qJi6}%/:]"n":q2;[{|*' FG>w^r])d>;yA&`2qPC54BDDP!O~t"Bm!:C!d_*nt2I&ia>MUh&FZW?TR_ZH]oEf,Iaq,Y@i8JYk_?-x7j\^pm[hbd("Nm#ib,NFg@~U)"D!2<y/K	e'tH'nDv!r(t:vn5f2|f4P<
*N_9tocrC7KrT&ibqB]2Yce)W` &S3~Sg^i?v:,-2!#BF>conN>x= k|id29\[yVvwXYS[,DTRHCYF)LevZC#@2er9hXfb8=0B+05wA4QW8,4[]wycakp?:h6m7FQDp]AB4@`
3AW)G=-Z!yra'tiXB.S&i5yY+tJnGnj-flpIq[%^z~cEIcL,Wim8"k_d]gqz/XnFKO/]}.+2G>z
Dz781m\*4Y	Ynxi   IDATb$KEd2y6G*dR	n"*erBCMSB:h)QufQl{8kS%+]@2"*en$_#O=6cU?/Q|EmHZid&+ZH8ZI\Nfg*K@	F6:djVy_U	:BvJfepg48vvRqe+P^' z`S]'4yq>6{-kK7u%yU92$I$BH*n5IzroZq#1>#N*QOUq1r,u}E)y.B1&-}"A(#mg)0ugU9i{ldv8cRgF}bHLT2A:h!FYXmD*r8q!+SoI&FmaNcEJQR60i8QJg[di"q]Z6jI	-P41hj\cG=	{4@2)j@Ldqz0G?<#!tHz"u3.SNcvh!cmor.Og)A';kZhP=OteB9!QUyCyQn>K])x"[LVzhP<+{sS}M?:;zI}r]akpzwL]<F vo(5x{sWwzhP=.du/>XT.,my}5<#g`WB?}>jFIBU>L66?;;N);3q=:eR/Wu)v%L&]?Yc3sx\<EXocB}5~M|gi|T"Lyu8?r`"CPv/COacn3LuDeRqcZWw.L:k*CO<v0sL~U[~bScyox9{S#m"#sB?(.=\9U>&Fi&_<~TysO/KY"ji5#42Xllqj30'gwV<K;T	-,FfldM,/GDS[Ku?edId:vT$$1F`:lt?ehTT&r}g$-#|GE3Nk't:cso<% =M56
W@Go?#D6s;*T`$ttOVzLC:n2jh!g a'+lb_yknMV>Zr8W3><@_lcp'4B5"{N[
3\.5DD8;8Iz51HsFO+s2Z	ikk+VcGuUeE/*_dj*;kg6o{ Q#iq<b0|}t[T B^4]yOQ<1HI;wrh/+LU~Q-7c9dsj .E/4?[/B&i^ ybhtb5hHm)nX/GGbUd$rBGa=m5XJZ41HsZ9
'@[Po>       vn              `v             }i(/:3PCyqa[#*kC7K^$Su^f_yyfF=6n=!FLv#@_F-Bim+:[=MyQ8tk[Dp"_Yjxi)`="|c3WQWel"L&Re\OjY4]ksyI4IE^RdFDnO=%r #|Vn[e+n}=5Z-"\8hGC+Mo=MzY5,g![rR[A&,ZAaYuO8|-lFBV"Z{z .:HQ]S9cN_i{Hx!nhf!{lO[3pQZhziM4f
S'v_G0rRv.k&z8E<Cb oIdU+x/#M6{7k]g8;wnkGhziQ{[?H7V=o"HR.OtG''0bqcSW\wxFPM-s3d2/^Nf2Y9C|Wp}{uBIbL]I}&3^s>^!r7t>L*I3UQu_I27kvgUg8\L\<}+2$vLu8!GFwDd]FN:e8c$s.)}glwOM":Bry}Mur0G5OCBlaql&	s(BrT82+J(PKZd^-dAhjO=xO:Q$6fXUJ>+a~	y1>#q99@;C&~pNdG*LzmL-:B(fV:3#<K;.YE6m{iiq;!=`.dL:U0[g
ldsQi#wt7~Uz
yE* c,7..S51K9Q)"VNf/9f	Q,etv	cD1;{NU]Lj^zGcecvj(+<9Feo?7yXW55q2=4~F~eQg%#>)fPasz,V<Zkk,(A'D_\< 3**F/CjlYF%Y/tB<otopf]U~BFvfO[;u'P:</ULA}I%uGszqB(15!_LT.4Q#t^<pJS	Ilc>.W)C60k15*'OA@!m&u3627LAEn$0P?M;8,-R"jpds77:M_(E!'+d6VoBfM-bY8GF0
w$;?5XQiIz	n?G[>8@DkG}|LE49I&(kv:FSJgL&BGw>XYqDu:M5iO9N6suFiq]#{o!!:C!d_*nt2I&yIC]}3qxm'6LXHxVSC-djtotM~6B(=tf@_Fx4T([hY[Vw9>k2<qdF	zdkWyf@L\35Y95BY]={VSD'tX7C iP}Ck^!|anl$<FrF~R@BL-,
0YI 2#uZxXujdV79IagB)Wvi'uh:=uzS}-BL*/-T4SBPYh]!K?2Ts<wc$eErT&ivdcX\#7wL&\&c0.g:{dZq+G8;tATJuV3SXZu3X-ZXGG+V(S[#.g4:NhO<WkU-BI4rdAo/]0L&|+c3+ N@\P	S;'_<mj})0dM~^:iH?-uU PS}5poJ;T9G;44N	E+42ui}&}\]`nNJ6 r_;=0B+05wt6'4O:&($F-]wAj,vY3I6	lx6*dR	dMKr,wZJ6O<Ky"#y12I_&FOLU5Thj!TqDvB{RhF^ [nxib$KE4UZBI&4D0fIGz,XmK$_>K["W	eJvoC7>~Ym=)w<Fy_UyI24u#_n^W|{yF<3i9M]N,"6RU[5x/*d3t6Oion}mZqvU<3\L_Kr9R\@zBT$!$7Keq
UdZ9hMu5gLL(1LI7:7RWf'?>[+4/Nd9#5
Z*[5x#Y5q'wt{wYF^,,BNs'b~UaO,uRFX'FqM81&ra4qZ\W-ttu5S$86'zmNZ%3}Q&Y;hs {z[p#Et<N)Vq2:U=Q5BNeq4:yz
y ,7ks+JH}]aNR!,]nWBUumu$p{kx6uEG_| #:{	BN<|zeVRCy&",m}oIBNso`bA\SzgSz+}r]2xBF7M]<8<SOx:)HsS/Bp6}9ETUP^}-BGWMbU'RSq{,Y$]\M:SN}q[W?$
efD%	1FV]0BLrRqsvM5IVPn7|p@NNs9W>Y19+Tg\/'%	9wQ^iU2ZxO5WqN]_Vr:MQQ'npIGyGI~V*$ s.IYNXN^?@\(z WQ{I	-,Ff!Ltu<a
)+~ergiHWQW$u4<2$EYu+c19|kGs{CHaZ
\xaARk6rvY`$tt1sQM>&uAy`,;Kt>0S[xz>EUu$4j)Ou
K/2{mZo>t?rgsj7ZC^3uR   m_#}+H9inZD5n#g \"jqvpyGuM   xh*;kgH{C}@!iq<b0|}=}R}Iv  0x8<A{ \>8@P+|vQuT,*~?QzLWnnF\t   \^So 	&/nHz.`kyNW<    .i#[==       Fv             }       0Xve_$b@_=_Tf'#*@_F9IO P^tfM5A pNv!rQ?@_Fb_$P^`G"T_vzP}I~xi?eoiCGQe<E+P^|p)vdDlKGY'%[TDzbL<U#//"5[SNR nO5-7uW[Bkk_!|u]ML?e2Lz-D'vrS>Bgi>fvuRt)[_!{.L?;to\R_ecB>+Yzh9a^E]iysJ3 jJRiPlwu'jN>i\*MS) ^<K.S,L6!dqGL{F<bnIe0gzAu*PZ7O-g0Y+e9^?3K[;7SL_i{H3,M-6!],<3Lsn]CrIV=@Vz~S;8|9|}m:-KJG!C&U9Q;yB^=R!q,QF_z\&+M~bC~Y6{7k]gsV7H"	m!?+0mN-jv VNz'1qb& #lA.O8N.(\>*.Mc#]Ov:r"}n4Yx[8y36?[`xWX$uO|gljY!I~oqm=Rw/<`SC4sI:(=4hp"31^m0'.w\0<`'v1-z.ed<#(PC:0X>8LWE/~16f'B:/3zqn7?f{~Yvy1L(]Aj5v(fV:3#@?=v<AL&+~T=z"|\jdE:tv?F5GN9xvv^;^4J$|I8iuibF:,.B}M&<He)td)@8}Efj#PT(ImSP=M,1U%Cf&YVP'vM(;

F!poU]X.YI%??Y/$
`2S.)y53RI3j4gtI[/4Y:}LU(j	on7;8I`qleLi& llc~%B9C//ly1^?sa1]B&`M*jyL-}ATZ9n;N::CIc=W`aqKM\B,m,I?gt<plE_%kb!d9nS\%L/$?6tAYw>9sG?1_PFy7I8/scoJ_-xz'&|r0BMBS/io?PcfO2/&v;iN(!TW3p#<Q|r='"4%=43>jfj?o{w$<Qaq"4 YHHQ

]dj0YlEIjhJ	R=CYU1MDDU7,m6(2;8x,mRUul&or9)\>m]N&uG4v G)C Ef977Cq.SDXbtFm)@;=#Jc-yVS<@RxE*^AeF<e	vcD[|0A{V)&)n3WgU#Zahm+/Qw}#COgFeLI%B^s>3 R>J08&o8t;'*ibVT~k	]{8W=?UZSm#!F}2}G)WB#{o&1
eRKUEid	Bs8WHV%P:Fj:BhhJ	tGDAExCPKF<Y\cW{BC'TRWEPFsk"H.|#Z iOWksDlc84uL< qIeifrWWp8vhJCY\&I]2Yce)W`wycrC&t5WN&Whe3:M&/$C6!Z5U&&C
y#:M\2o>r3[1!= k|id29\[yVvwXYS[, DT0pn'4MNVJ9QmbELr8X3jW


]tBhZNDAr6LuuAYI:Q<dWZh4,:B'5+k w}!T;.k_d]gqYA{/XnFKO/EaK6O6G%.Ky=6F<B1fIGz,XmK$_>0_!4))Eyz|o[w1R:eGb0Y9."luT6r?AaLdlf%sQ>XuoN#\*j$8R$JpQ)##AiB3}nsaq"4 YTgT_]6@m	TDAExCQ#5gY`;+, yvgi;5I\WWZ_Z(JD"FY!ds {z[r!6 n=Xe|2(t8^i"q]Z6jI	_oJun>N~|`7hL]O?2h<w[XqbLG?rrkT_V(,U$Mgi<`YL!NYw:*PX%#m2ugU9i{ldv8cRgF}b<F|ENP4p`YG!JE0i]h~qQ9n~ :mNEUbFn[Z']q"^"QPMQQQVkYdJNApk53bV,gW.R(9V5sBL:B#lo  m] cso<% =;I}r]6rvK"Q*j7O!')}n:S[C<#!'L6G`ddjYco##!LWf%5,>uXTUsz#Uw5-4(w:O^]:	]mv{|P@hi$4O?Ul"42QCcyox9{[`T(5&m"%1{k8t	\2Y-H,OQUiq8<AoR.Q"QxiR7ePC?qME_S/Y,*q#S6?;;GaU9*rCyq~mL#GG/&?d<K[,0TufXF&8ro9CxV;+(tvK!D^N<ciQ'ZAp&,X3ROpsu$C:TQ1IaU$jU8waJP44F.I6QYdAYaTI:+zTmaNyv\3uAWRaTr|kSO\<zuE2E-+]X}YacXr3	\j.sB?}>jFIB[{oh}#|>Cug>CZO9+-'v;SWw{oEGT3^ @|/`0n5/;=UeE/'M,Q"#fx [ hO)vgpx&r@e=,. xJ pok
^ t{i <{u1@f{h2 @%       b        0`       l      `>       ,              `v             }       0X`       l      `>       ,              `v       6u      VE.[[FF      VC*             }       0X`       l      `a7_  }{!FnnA>cQoN'__8q!~1   he>   o;6fe@+GuAvuQ   #  P@=zXdIMMsss[G[L&,   z   m2e
B.    @
  w'^^^?_|annb
Tt?\$umoWL7o{^z*@6BW^cbbb^^YYY!wkcci&}{   hq>  60s3g>|O?m~3fTUU>|!H6zG#d2Yn?d[Nqw~hhkBiii7nTbo_>uT}}}PP+1U3g   @}   -ngC#+BhE7o<Prrr~~Wg	D!>#F8;;#T!b,,,B=C>|!|r   @7   hIUUB!SSS^z3>>c?x`}8h)>K1{iddRY'''c      PRRmB7#=8v7~x#G9u)2T*%<   0``  7oD~AaB5'LWlff}Onnbmo711Q>K.   0``  X=hjjz_~?iV\WcUVa;kgg7~xs0d2Sl_.HRb$lXDDK
K,I?~O   `mo/  NNN~zppB<o).\xO?aW1\]]SSS/_b2EEE~!v=B#!!!WTT>}{r\H$F"9::!//oA?0117n\>}o}?0>>~+..  gG7nSl}>P}a_Af.-       Jxxx{<#a|s<       `v             }       0X`       l      `N}       nw       @       l      `>       ,              `v             }       0X`       l      `>       ,              `v             }       0X`       l      `>       ,i+V0o^SST\\EYz?K/};j.$U,U%>CE=>7.y]	*b     h>smzADL&?`r->e]6-=QLk-O:v?!l,dn+0s0F<;im     @[=mBl6!brT&fMc YK?'}{L}Za"1{AYzFp?X1bj8N`N7     h.[{`0cagrbt^aYRY[@Jo=j{(t     0V@Kd2'{GEEDCM4IyW+**~kk;w"2337nx)S ?_XXa9saae3O}Dq{ vaiVDPaa1*.^{]Q" H_ctE`AyygO3{3gOm+.FhQgs+hK`G'ooEm+5(!Wgw;gv^yn^[gW7M? Nfh^;3>j2-h]aL<XoMnXz#n<E<z(,2,(gpM/O"~"(aI+o6?_JD[gX;TG}O.xTA/y[^;yd#W   UBCC}NMM%)S<d%Jd.BDq	,Xtq{H$s8g!F/$Gq<OXH9b
5*IqOfYvq}Uk\n$=88CjHy&vq8DZVNc.N.4</~'B]8l?{yY<$WCaJ~xn=4y_M{:x{-&xx,POSU8@#   h|9Kl^P(kDKGYO-[v!^?zhsz]1!{0itLW	Vj'S6]f|<
)Ul$=RI;vDEp-25hPIJT={[r~}UV$f({C/l_eO,R5+,sbeSn;B.Ej\f@v#   }y2rwblrx #b$RF+bc~7)`GWA%p2QtthM.^=O=.sz1![_iIqi:Nj!&"'<',m,HX|sf6O_3XjaA32GxL   N#;666t?|oE'%"f+MJ	A~nuz7d6Gm
Xr^B,=9k!,f>sX=gHy~?#)ae0:FdP<nZ5)#    }^o!8e2YtttRRAKD,
}||{|'f).GW5OD=ycek:t&	AD:^c]/_q)`uZ6}[6<1
5g]FOXa(dj"1&2+oo/oyO5Z<K\0d{<[q+{_yv/W=KDD$'*e<#<ol(C=-FDek8Loon"D4e?    x)S]|y7o8h{cSRR-[v%77%3gtvv|$Ie/^|UWW7=3}ie
/ryh\f+,V8~3oyVmLBDVL':t}QEb6j7^:xd$xE[Aep`[lJUrH";^lm!Q)Rj)uKkNND5ttI.D"RUGuU=.zwU])LY{;vUW$Q}rq-_Si_TaWJb!>=%
.(j_vq'gOj:)NyG-WxVGWM2=Y^Eg??   |4naeee!52_":G2K;Un{1&   0RXd[t~|gw=?   Y*7y>b_$bv
^%rKGvt-P\>O    L=y"yXD4w!s[y.7a~
    f	E~]%O    r     f>    B    `    -     f>    B    `    -     f>    B    `    -     f>    B    `    -     f>    B    `    )oemsgaZhCWWWWWW"JIIyWr[WVZ111K^    JTz]GGG"bfe_~aEUVVV
g8N$Q'm    ^_~X?=LgOH4dUkmmi<>|jP$,--'O[   0H:0yx?W    G5xq_!1$$diCQm8gw}DO81v9{[6h`e9rDt%GGgV6jRJVzK;rR9ag<x[f'Oq?Y	<<<Zm~    :Qhhhq<`WWW_~KxgD"	P044THH$d

*\p.>>W^-_<<?uB
	,$3gZ*V._lz:|pgOSSSh),YDBde3'Xre]z}~F    =gI5[u7~>}YfO>%-[C$1S'^;::^xaRyd7?}t"E?;O?j{@,=~o_~Ybgg'Ti&ooolP(kx{{?\cA$eOvC[,    N8d=9Cv<xqZ11{{~-\'OO	d
N^zzzrv%NAFj32j`    _mo`ccsqCLDw.Vce2YM---.\me"1&Mj]:e+bI$ Ftvv622Fj32yX    _e/,O8.>>>=z3<h^i"RZV&EGG'%%eWk)))Dd(nnn:uuk,5P0s+<
O>+Wr9rd2,-0>BbF?    ?x`D4o+W-_/_[Yn]^z]iFGG+7mTP_mUVuttlJ}m_vlBD"Y#gzyy-Z_/<Jw^"qR\pa?6oLD;w|YCD6lI&wd7;|Y    /LvN     0RXd[t     !    0[    }     p    l!    0[    }     p    l!    0[    }     p    l!    0[    }     p    l!    0[    }     p    l!    0[    O%-.    Nj#db#3;Z)]ZiXX0D5_($=~aC}G(qc2I3x:4""7X}[11\)aav}Y^?.IX=QKU,je#0*U{Xx%
[}D;M;\a
   @oY__ep0;bH`K%K7f~k~ 36QVY/o!1cTIIu
kkz9SpFPxDCZ-_acZ]Zlm??YN#8G,Q~]MiYyg~ppNleH|Q   o L6[NdeODi
qHDzY:W]{~
+"HYHDDCDdaAay6)7xz'O	<YGxnGcj}^BOD%G.kQwDDD  }s
1={ZdtkksLSfYfWAqhXp@Dy:C'<KVl'cL_|M   sziFMe5^mU{uH.IJbLJHzXcu%S8fYvR%y6mJK5o*;;fofU,^9jzz/jNJDx9}Z[7u		+CCmEkOuso]V3f|X\Nnw+WcEGHIPE>w\'n~&AVFw":njZQWgw;gv^yn^;sV6B"n W(5|<s&M{ek{yD{On,WH4aP   vg32(+A6!!v		?dNMXZ2Db=Fl,GDw^{qQ.\Z?ncamRvDByqC?a|r+y'~=wNxv&}_9j_6nh(9?,Qbg?$',8O1gH&:/+]~1|qEtZ+pI"JI]6H5U0'Sm+/MoGY==vtl!==rLjzKfG]gI9yTs"YQ#M8   -v+5'eVVLy@3:ODcV-,<=Zne*8&q>}d[A&;wl,]GT6^O<)uYoo9:.^aR*qq1sE"i^&OevH0	:tL# "eRNxVf+5(DDEaH{peJdVR"2W,4QGUGV%*Kt->y~{Y=wS=QjWExw{R5   `M/NK!&`wk
r(`A!7+&rr]l_wrUq"wDdaA&f*T(]dDz"e\\m0<0 xb+?@h(/[9}Eb6aAQ18r(i_DTe_$"KN6D",_3~Iqi:Nj!&"'<',m,l   RC(//wz#|[nmH3/ Nt~}}x,vmtI!B%/[KeI?bc?Kmaq)#{#koeuTuD5[yTmRJ+]FyMe$Ddimq3VnqpjdXPT
DnnhX`1    0v}z?18!{S~]%.[gzg }:!b8
,+&]~LTA9.<<?
{Sz$\l,3|ua=|FGz=>'"jI&h^z8:tHGD<es=z(U^nZi)SOko~4*^xND<rBQQ_)Y=r)tNm<Q\drG,<KD5Z<K\0d{<FD0nvPTf%pbm!'-T7l   [4qFSn-mu:k,8P:4T)Q|<?e<E2YU97lb\I$mbmJKKF*kl&IVxzKbb*U2*sBxKsiY[3vvLcghN]DBkhh?XtnQ~rF md={*g,K?_dUe7nZn]I`bU*2W?Ql!L[I2Rwrlc0_u":!Nm[z='uRzEv2Dt%wF,GV6n9Yyv8z2oP~qW ~eWxJzzEiM^%t}BX   o} Dt>!I
vw/<W&k-U{iyg~]M   a.4J)&|{oO|#<i   5sfbt#)iDr	
G?O~3}RljK;uWNywGOg8;   Z`1|={tSBBXQ#zpIDtP9#Y*,u\lM	   j1}    Ioz#N     p    l!    0["    vPkLZ(m4I5i"]UE/^mH?F]!i.E0mk}SZg.V#GFyzMI7XX0D5_($=~aC}G(qc2I3x:4""7X}o4&_?%,i~9KeVjTRZ:J8^AIV3yb?m|1/qZ  //aBCmEtq$%'u7+"#_~Q}}C_(f8ZJ-V|yqv*?{fzE"?PlJjkHvewl=j0DDY281BfJxQ%%)"iL!CJk7nT0i
Sgi}ZtjADk6le>dy?;^p
DDD1aw6y?3do'}mV;vQ^"G1  0oXh\w>d3?_ZQYq:e6mB*Q^617kH*_k4qANveeXL*56k&uuViM}uVBH"~w	H?Y+);yRWF<Y1"6k.XJ\^1"E'"  [#BgrCeoItkDkl+T{K+KDgk6o:;'r>grn3#F3Oe]Ql,NNcU{!OF.'rh}2%r  ^>gc8sSxq%-
fbVkk4|UvR"
s5kcgV_SG~BN_mcn8ZMKZHHOW,s,KsZZ[353^.PgT;:($urzGGs+&-Z)r1}tKj6M9vs| ^,3~]oi/Y)TI^MiRorfRDq4~A{k&~uH+Vwuvm?-4MivZ1cUr9e\+"%A(C*7r5ziqZ5;`F-jBzbD^up`PzNyfDsY
|/T=rL_)|4O9=\!5  |oUF.!>y+nruj*G?e*%y~u<GF"Q<<N`Y><%1B=yzx$e<&T`5QWZ_g?_M;xPk.?DK^^1BYt"QEL#i "usKU+9r	g+3gJP=Q<J%w#q4gpL$LUuDMy?sF'''s<q0&1,Qpv*\.B={h%p|i4k3T-WR{)g>bJmY\2^47Tm<)	*z([9Oyx[+~9p~#M   |h|9Kl	yLTyE0~~
CD]Y-y#"GG6CJ%>.MDD2U4m.sB[}X6k&y#~&T0xlmrIZqN]'"[[&te:kJ:c:C
Mjyfy>.a 3QZYx87z]MnMH\V;x6PD*&):'"?NZLp,eKR77Q~-[;vhEI)UGud)yVf+5(DDEaH{peJdVR"2W,4QGUGV%*Kt->y~{Y=wS=QjWExw{R5  `<;xzSZ.]jU{aHD5"/5n,!"*,K:/1D)83m()U;y8*cb87TQvMdSQJb#/|bDNNaMtx>N*NDb,,h$3xpYV
e<_ooDdkd1 |LaeHQ*fUI ek8LD\}H"(dCD/U0g<b"r(s|r"&   ;gZgv?-p&6k2%lZ-yyt(mZ*YBD
1&V[#PPoiVz}T9c<5OD[Kox6mmzR)14\~:~I6cZr[mvw;o#yR8-<A.]6)8
x9~\;w2ncd$ttdKDz;gsVU"oJW-Q^:mm2	YZ[>sleWaAQ*1oa-k  }>hl5yyI.JIZqPEFr^OOH2&
poc}SB#[G	LF|1u:<8n][3gkh }:!8
'7	r&5z5lM3eUF'g
fK71bD+0Taao*:^_$s.6DE:)Eg^e9Qb"On ||\`}vDWCkh6L=Ut\{9oKD	MGF=NH|d\"i:{oDDq>"h.h,qF>:|R\qz.%RnzBQ[gSU	   o+Vhd2VMr~Z"S#5Z"A[4 @(j6unkzx|)\M
!"_\3b-5~dZ6,T)ZM&n1WZW[]#E2YU97lbEh. @GbD#F*Tx3S{:+L~gWaY*$5tiQD)gouB.D	mER.';;J5[!{htuvU*^JJR{uDqu}tSxXE,K?_dUe7nZn]I`bU*2B.U7u!JebE*j9`":1MDtlC0zqJ:zN|]1*awe;KS[]!W!Y!<+,l,s\Lu!Zsq\.e
wRAD:-ua8EK4'e  cbaBCm[HbvwT<OH:wyRBCm?m<i;o~f_W   0f.\Hai~V>gM0	^IroHe~6K}J	/5q$}g}!  9310gk6K^O$&srJ8Q>b\`G~O
URljK;uWNywGOg8;  <g\RIwju"{i)iH2v^=r]=cq
u]|,[k  b     _\xf    l!    0[    }     p    l!    0[    }     p    l!    0[    }     p    l!    0[    }     %ao     Lb    g]    l!    0[    }     p    l! xGzPgB_5k/_~I9  e@ -Z,ZH8.^35kc~Dm+wh4v  _  IRCIKK8p`dd)bjeU?~4  2I>
  ,))?((y   > @dC&L}3+WV}''PE:tPv	&w%K		*c+pTTi,GOIIYx7*V\J#F?r  8~a? uss;vq#*Ul><q-[qM6[c71O:koZj%&&VZuD!s*HDM6}KG^[uvvvFN*  @>pww.603>`v2#V>}  H$o6lX/NDaaaDt-"5j07f5QHHJ
 "SVH/_GD$L>}l2'  y  rG?Jg'9s3s>>>}L&;{kLl1-	//^h  <`v Cu}-bG7o^ljz-Z8qQ)bby.HDo2  } ?Uk3<h&Or kkk2e)V2^^^R yC Zt:!<%"	&QjHPyv1uT:o_ttD:_)V...Dtm%K4IE  C<e,1bD~w s;xi_JNN:z *!!a-K[^t+oRW^~=n*U\rEREFFzyy]v@o]j/^<}+.]c
SGR{{ms(f#'Uf  ({W"q    a#N          0[    }     p    l!    0[tww    @e{    X    `    -     f>    B    `    -     f>    B    `    -     f>    B    `    -     f>    B    `    -     f>    B    `    -     f>    25Wnnn)))+?&O,a&s_%sK-1bD,;w=zh6d    _$yS_w-6lGxxx5K~=3gRo-jun9rQQQ:t(mK.:l    0f.\Ha)3Mn%{Sg-[FD/88>>}X9Kg!CFV<x0CH\`$   /I~pp7=mEhhh~?3e]\\(<<,--DJ   1)\zphz___agPPP*UrSZIj_5k_>sF}pk|k
/99YH	8p]v^->y+::k{gggg!CNBBBzqmxSNK.!>g-[V"TR<wzskwo'%%?~V7lSDtM6mQ"H    0|Nq\ayZP[{Uzu"*Wi37Ql1o~c^<X]QNxOKK%v2*UJ/}"d0$RNg!`DDK+gy?"/^4`(]fe;t@DK,yO>D_?7e^3fLe3X,:u*q9~   jD4>`%l6o&K.tk1DYJD
BTFFF<xByfj6lp?%%4e
xrC|2eH$+
RSS(@DR466VZt<{lW!6s4tC<(w&KYl.<?gW%?FR5*`/_[U    _\9,yk&=z8::5kRSSlee$)0L^(((rDtkCOzoIIIDzRmVJDC	2<k9ZXX]bHvYt)7o,tpp 	&/_<dWh3fHKK3gNXlYs   _t)00PX`-(999h7:up/bX!L9=6l{*Tpi!A	Moa}2[TTp`ees4P(ua]JgR$"DwIR_j?{l{n6PB.   !_lGsE9.L9S>lbk+W?]k\RiCDW\w03pN2:GaSu)
_~}m#/_ "o     ##bcc3,p8U(xK(3<5jdx200PghW^%%%e^~5'6jz~9k<(o!%7nkx5>,	}5`(]?CxsgCC]-Z^rHDUV544zh"j-   mUolv|05Onhx]2)S9K+WvMe&=j(H$,k	&|RxyQav0-[tsssvc;|p{{{"9s/L0<m'<occ3aCc v.\`Y`aW}A~   v/bs7o.gL"K*5i$aGJ5}taA=SRRt:]n*tSLqqqeeKm={]$YwW*Lefy=pBR$Iz=;o<"E?8#H$/O_YsfuH$-j?<o<algO%bqz_n9?o]2.\X$yzz.[

7-Y/<[Z5__U7NNvG{k   O]m=psu 666UV{zz
^p    AM~i|RJvyRRbGl)veKaOS(?MB    $^O~G
|
[jeU    th^/69& RhfU^U8qVZ'$$m4iR~7    }.     _k3>    s    |(?=    zG    `'    W)%5-<    >q    |ME~    '<ydi]t^   9C>`vhX%D"7   )]-2k?ko sG|1svX8H"]%"XRuKW^^Z5i\$iZhZV\N(6&cv#gWcZ)b    7O?YbwM7GE'gNNp+bo,Ye9^D]~s3}_YVM54{9?sV0,{K^<wFbb?	IHQ'++<qBzZ/t    _w<T?O]~|4Fu*!#/34kP1'wHhYcYNq,,8{;;kGNVSjuTYD_`bqu-IhZ6-m!:Y+
?BDsTZ?!"YJqS  IDATyZD?V>y8	    u4&)L6y\(Sp<ed"[[r[+:ms2BgDUW}\f)9s\7m)RW3jU.o,r[HQKT*U=zyjV.Z`59-'1QQi,]qUYsH}YY|q.OO_..^_a    Er/#r]\\0]G6kiF)kuhWc2{RZ4mKg!C_Hdy`bE=#eKx,a9z
)Rk4No~Z$^?da	ybi8kk*fW g9No*u)/>3!=vmj    /	#h)!'go'ffOaCQ.pP4u[^nMJg<[iwFR:bEn%KpVi12o$]n-?Ij^zVl\^^Z0^<nd/~=]b-ZTFz),HINUONb   R22Z1]&;2u~k;KDRIZVtcb]:>kK+EZBl_T<~:La[nMuv=xODq)lN:=K,]#,\D}C{uv>>*V\hQR-    @y74+A/4Z|{V3Scs!y~a{'Y>r}_Be9J%R6b;P_;#2uZz	oj9o7$yHBZZ^{K&lfIv1...DR$0,Q;n[i    E#'\=w;W;sZxw?3'B*eVrmtsm?39WX#c<0`5o\z=^KnjJ*V8cRjz+#Uu,-ls9J7a:avBVY"IQi<qfw /V.yl    |RYSJ{y6h6	==:\	iju1,X?,FDgy#Bq;k7:wz:*-[XX"RX*0wFTTqe';u1?'Ryx6tuu%"Vq9/f]JUZ5>UTVsJqqGz}"EsZ-^|}3lE8+V+M   @tA*oI?/Mif/Pu9~#X?g G&ORn%""DDF|awwt+CXe8'tzZ:>]*SF-|!}{*Uk
E4ujn    /AlAJ}~BurHk0vC8pMAmXYXYbyOKS(p)t^?tZVi:N#"!%^r6DTA    ?}Hq~k_GG<+6nr_RbMBR<s<<<	-PzJ(UH{h)    NX-wcQ>X   *tEXtcomment WebSec - A Web Secretary - Mozilla
    IENDB`

Revision-number: 114
Prop-content-length: 138
Content-length: 138

K 7
svn:log
V 37
Add screenshot image to be installed

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-16T13:02:04.932791Z
PROPS-END

Node-path: trunk/site/Makefile
Node-kind: file
Node-action: change
Text-content-length: 760
Text-content-md5: 20cb78d290062c079fcd285c91629a07
Content-length: 760

SRC=index.wml
STATIC_SRC=favicon.ico websec.css websec-homepage.old.html \
	   websec-homepage.new.html websec-homepage.highlighted.html \
	   websec-homepage.highlighted.png
OUT=$(SRC:%.wml=%.html)

all: files_changed run install

run:
	#wmk --optimize=4
	wmk -W2,-X1034 -D"WML_GEN_ISODATE~`date +%Y-%m-%d`"
	validate index.html

files_changed:
	@ls -R ../files | cat - Makefile | md5sum > md5sum.new
	@if [ ! -e "md5sum.old" ]; then \
		echo "No old md5sum file, recompiling everything."; \
		make clean; \
	else \
		if [ "`cat md5sum.old`" != "`cat md5sum.new`" ]; then \
			echo "File list changed, recompiling!"; \
			make clean; \
		fi ;\
	fi
	@mv md5sum.new md5sum.old

clean:
	-rm -f $(OUT)

install:
	install -m 0644 $(OUT) $(STATIC_SRC) ../realsite/


Revision-number: 115
Prop-content-length: 151
Content-length: 151

K 7
svn:log
V 50
Move download upwards and add contact information

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-17T18:48:57.742361Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 7916
Text-content-md5: 35099f03824ce32938978def1a503444
Content-length: 7916

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>Quickie &amp; Download</h2>
<div class=subsection>
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);
			$text = $ver;
			if ($text eq 'snapshot') {
				$text = $file;
				$text =~ s/.*(snapshot-\d+\.\d+\.\d+).*/$1/;
			}

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $text, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
</p>
<ul>
<:listversions():>
</ul>

<p>Contact the Author:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a></li>
 <li>Personal: websec(AT)ev-en.org</li>
</ul>
</div>

<h2>What is WebSec?</h2>
<div class=subsection>
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>
</div>

<h2>Installation</h2>

<h3>Are there any dependencies?</h3>
<div class=subsection>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
</div>

<h3>How to install?</h3>
<div class=subsection>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
</div>

<h3>How to use?</h3>
<div class=subsection>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</div>

<h2>How do I get help? How do I help?</h2>
<div class=subsection>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</div>

<h2>Authors</h2>
<div class=subsection>
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</div>

<h2>"Competitors"</h2>
<div class=subsection>
<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
  <p>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
  </p>
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
  <p>
    Checks only timestamps and not actual page changes. It's main advanges
    is being written in Python.
  </p>
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
  <p>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
  </p>
  <p>
   There apears to be two wrep's, from two different authors, with totally
   different methods of work.
  </p>
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd><p>A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.</p></dd>
</dl>
</div>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Revision-number: 116
Prop-content-length: 129
Content-length: 129

K 7
svn:log
V 28
Add request by Daniel Fraga

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-18T06:17:15.865900Z
PROPS-END

Node-path: trunk/websec/TODO
Node-kind: file
Node-action: change
Text-content-length: 2182
Text-content-md5: 26da53dda5e1e131ce96016e69d37aba
Content-length: 2182

Next Release
------------

http://groups.yahoo.com/group/websec/message/79 has an elisp mode to highlight
url.list.
  + need an equivalent from vim.
  + Find out where to install the emacs mode
  ? Fix it, it seems broken, need user input on this.

Adam Stanley provided patches to add:
 + 'MailFrom', the address from which the mail will come from.
 - 'DiffOnly', send only the difference, as opposed to whole document with highlighted differences
 - Follow Refresh zero, Follow links if the site uses refresh zero.


Short Term
----------

B3450, Inconsitency with the .websec/url.list location finding and using websec with a file parameter.

Run the html output of webdiff through a text mode browser for the benefit of
text MUA users. This, coupled with the DiffOnly patch will provide an
approximate for the request of Daniel Fraga.

B3497, Show also removed content from the html page, possibly use HTML::Diff

Enable running multiple websec instances at the same time:
http://groups.yahoo.com/group/websec/message/70
  - Implement

Send mail from windows:
http://groups.yahoo.com/group/websec/message/21
  - Improve mail sending

Problem with script pages (eg Slashdot):
http://groups.yahoo.com/group/websec/message/35 
  - Check if needed

Patch to remove script/noscript:
http://groups.yahoo.com/group/websec/message/38
  - Check if needed, related to Slashdot issues.

Multi-User patches and RPM spec:
http://groups.yahoo.com/group/websec/message/45
  - Investigate

B3449, Notify when pages in url.list haven't been updated for a long time.

B3472, Provide a method to specify how often to check for changes, mostly for dialup users.
Can also be used for a constantly running program.


Long Term
---------

Keep a website as an image to be put in the desktop background.
http://groups.yahoo.com/group/websec/message/72
  - Investigate

Create GUI for websec, similar to kwebwatch, but implement the GUI with Perl
bindings, this will allow wxWindows bindings for windows and gtk/qt bindings for
Linux.
  - Implement, need to modulrize websec to work as a library and have differt
    GUI, where 'text' is one of them and is the same as now, a simple command
    line.




Revision-number: 117
Prop-content-length: 134
Content-length: 134

K 7
svn:log
V 33
Minor corrections to ignore.list

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-18T07:28:01.459160Z
PROPS-END

Node-path: trunk/websec/ignore.list
Node-kind: file
Node-action: change
Text-content-length: 3740
Text-content-md5: 61bce451341aa4dd1bf54e6d381ca028
Content-length: 3740

[General]
all rights reserved
an error occurred
click here
comments
copyright
daily articles for
details
discussion forum
downloads
in issues
last modified
last updated
maintained
posted
posted at
previous cartoon
search by
special offer
the current week
total votes
visits
votes
copyright

[Date_Time]
\d+ Jan(uary)? \d+
\d+ Feb(ruary)? \d+
\d+ Mar(ch)? \d+
\d+ Apr(il)? \d+
\d+ May \d+
\d+ June? \d+
\d+ July? \d+
\d+ Aug(ust)? \d+
\d+ Sep(tember)? \d+
\d+ Oct(ober)? \d+
\d+ Nov(ember)? \d+
\d+ Dec(ember)? \d+
# 28-03-2005 28/03/2005 28.3.2005 2005-03-28
\d+[\/\-.]\d+[\/\-.]\d+
# 02:24 PST
\d{2}:\d{2} [A-Z]{3}

[Adverts]
http://www.news.com/cgi-bin/acc_clickthru
http://ads2.zdnet.com/adverts/
http://doublclick4.net

[VIM]
[\d,]+ scripts, [\d,]+ downloads
[\d,]+ tips, [\d,]+ tip views

[cvsweb]
\d+ (years?|months?|weeks?|days?|hours?|minutes?)

__END__

=head1 NAME

ignore.list - websec url monitoring configuration

=head1 DESCRIPTION

=head2 IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.list" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


=head2 IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.list". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut



Revision-number: 118
Prop-content-length: 142
Content-length: 142

K 7
svn:log
V 41
Enable generation of HTML files from POD

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-18T07:28:23.489980Z
PROPS-END

Node-path: trunk/websec/Makefile
Node-kind: file
Node-action: change
Text-content-length: 1575
Text-content-md5: 26cda176a74627cd9b25c9f03f2d4063
Content-length: 1575

PREFIX=/usr/local
BINDIR=$(DESTDIR)$(PREFIX)/bin
MANDIR=$(DESTDIR)$(PREFIX)/share/man
MAN1DIR=$(MANDIR)/man1
MAN5DIR=$(MANDIR)/man5
DOCDIR=$(DESTDIR)$(PREFIX)/share/doc/websec
EMACSDIR=$(DESTDIR)$(PREFIX)/share/emacs/site-lisp
VIMSYNDIR=$(DESTDIR)$(PREFIX)/share/vim/vim61/syntax

PROGSRC=websec webdiff
CONFSRC=url.list ignore.list

PROGMAN=$(PROGSRC:%=%.1)
CONFMAN=$(CONFSRC:%=%.5)

PROGMANHTML=$(addsuffix (1).html,$(PROGSRC))
CONFMANHTML=$(addsuffix (5).html,$(CONFSRC))

# Generated files
GENFILES=$(PROGMAN) $(CONFMAN)
HTMLFILES=$(addprefix ",$(addsuffix ",$(PROGMANHTML) $(CONFMANHTML)))

all: $(GENFILES)

install: all
	install -d $(BINDIR)
	install websec $(BINDIR)/
	install webdiff $(BINDIR)/

	install -d $(MAN1DIR)
	install websec.1 $(MAN1DIR)
	install webdiff.1 $(MAN1DIR)

	install -d $(MAN5DIR)
	install url.list.5 $(MAN5DIR)
	install ignore.list.5 $(MAN5DIR)

	install -d $(DOCDIR)
	install -d $(DOCDIR)/examples
	install -m 0644 url.list $(DOCDIR)/examples/
	install -m 0644 ignore.list $(DOCDIR)/examples/
	install -m 0644 README $(DOCDIR)/

	install -d $(EMACSDIR)
	install -m 0644 websec.el $(EMACSDIR)
	
	install -d $(VIMSYNDIR)
	install -m 0644 websec.vim $(VIMSYNDIR)

clean:
	rm -f $(GENFILES) pod2htm* $(HTMLFILES)

$(PROGMAN) : %.1 : %
	pod2man $< > $@

$(CONFMAN) : %.5 : %
	pod2man --section 5 $< > $@

html: $(CONFMANHTML) $(PROGMANHTML)

$(CONFMANHTML) : %(5).html : %
	pod2html --podroot . --podpath . --infile "$<" --outfile "$@"

$(PROGMANHTML) : %(1).html : %
	-rm -f pod2htm*
	pod2html --podroot . --podpath . --infile "$<" --outfile "$@"


Revision-number: 119
Prop-content-length: 124
Content-length: 124

K 7
svn:log
V 23
Install HTML man pages

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-18T07:37:00.696055Z
PROPS-END

Node-path: trunk/websec/Makefile
Node-kind: file
Node-action: change
Text-content-length: 1659
Text-content-md5: 50984b05f81639cd23dfe4dd67234d35
Content-length: 1659

PREFIX=/usr/local
BINDIR=$(DESTDIR)$(PREFIX)/bin
MANDIR=$(DESTDIR)$(PREFIX)/share/man
MAN1DIR=$(MANDIR)/man1
MAN5DIR=$(MANDIR)/man5
DOCDIR=$(DESTDIR)$(PREFIX)/share/doc/websec
EMACSDIR=$(DESTDIR)$(PREFIX)/share/emacs/site-lisp
VIMSYNDIR=$(DESTDIR)$(PREFIX)/share/vim/vim61/syntax

PROGSRC=websec webdiff
CONFSRC=url.list ignore.list

PROGMAN=$(PROGSRC:%=%.1)
CONFMAN=$(CONFSRC:%=%.5)

PROGMANHTML=$(addsuffix (1).html,$(PROGSRC))
CONFMANHTML=$(addsuffix (5).html,$(CONFSRC))

# Generated files
GENFILES=$(PROGMAN) $(CONFMAN)
HTMLFILES=$(addprefix ",$(addsuffix ",$(PROGMANHTML) $(CONFMANHTML)))

all: $(GENFILES)

install: all
	install -d $(BINDIR)
	install websec $(BINDIR)/
	install webdiff $(BINDIR)/

	install -d $(MAN1DIR)
	install websec.1 $(MAN1DIR)
	install webdiff.1 $(MAN1DIR)

	install -d $(MAN5DIR)
	install url.list.5 $(MAN5DIR)
	install ignore.list.5 $(MAN5DIR)

	install -d $(DOCDIR)
	install -d $(DOCDIR)/examples
	install -m 0644 url.list $(DOCDIR)/examples/
	install -m 0644 ignore.list $(DOCDIR)/examples/
	install -m 0644 README $(DOCDIR)/

	install -d $(EMACSDIR)
	install -m 0644 websec.el $(EMACSDIR)
	
	install -d $(VIMSYNDIR)
	install -m 0644 websec.vim $(VIMSYNDIR)

clean:
	rm -f $(GENFILES) pod2htm* $(HTMLFILES)

$(PROGMAN) : %.1 : %
	pod2man $< > $@

$(CONFMAN) : %.5 : %
	pod2man --section 5 $< > $@

install_html: html
	install -m 0644 $(HTMLFILES) $(DEST)

html: $(CONFMANHTML) $(PROGMANHTML)

$(CONFMANHTML) : %(5).html : %
	pod2html --htmlroot . --podroot . --podpath . --infile "$<" --outfile "$@"

$(PROGMANHTML) : %(1).html : %
	-rm -f pod2htm*
	pod2html --htmlroot . --podroot . --podpath . --infile "$<" --outfile "$@"


Revision-number: 120
Prop-content-length: 120
Content-length: 120

K 7
svn:log
V 19
Add HTML man pages

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-18T07:37:18.785964Z
PROPS-END

Node-path: trunk/site
Node-kind: dir
Node-action: change
Prop-content-length: 123
Content-length: 123

K 10
svn:ignore
V 22
index.html
md5sum.old

K 13
svn:externals
V 44
source file:///home/svn/websec/trunk/websec

PROPS-END


Node-path: trunk/site/Makefile
Node-kind: file
Node-action: change
Text-content-length: 815
Text-content-md5: 521d556c0421a88ec6ad1db64d1aab88
Content-length: 815

SRC=index.wml
STATIC_SRC=favicon.ico websec.css websec-homepage.old.html \
	   websec-homepage.new.html websec-homepage.highlighted.html \
	   websec-homepage.highlighted.png
OUT=$(SRC:%.wml=%.html)

all: files_changed run install

run:
	#wmk --optimize=4
	wmk -W2,-X1034 -D"WML_GEN_ISODATE~`date +%Y-%m-%d`"
	validate index.html

files_changed:
	@ls -R ../files | cat - Makefile | md5sum > md5sum.new
	@if [ ! -e "md5sum.old" ]; then \
		echo "No old md5sum file, recompiling everything."; \
		make clean; \
	else \
		if [ "`cat md5sum.old`" != "`cat md5sum.new`" ]; then \
			echo "File list changed, recompiling!"; \
			make clean; \
		fi ;\
	fi
	@mv md5sum.new md5sum.old

clean:
	-rm -f $(OUT)

install:
	make -C source html install_html DEST=../../realsite/
	install -m 0644 $(OUT) $(STATIC_SRC) ../realsite/


Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 8143
Text-content-md5: cc4ce6a309cdee57cba79439bfe8311c
Content-length: 8143

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>Quickie &amp; Download</h2>
<div class=subsection>
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);
			$text = $ver;
			if ($text eq 'snapshot') {
				$text = $file;
				$text =~ s/.*(snapshot-\d+\.\d+\.\d+).*/$1/;
			}

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $text, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
</p>
<ul>
<:listversions():>
</ul>

<p>Contact the Author:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a></li>
 <li>Personal: websec(AT)ev-en.org</li>
</ul>
</div>

<h2>What is WebSec?</h2>
<div class=subsection>
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>
</div>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>

<h2>Installation</h2>

<h3>Are there any dependencies?</h3>
<div class=subsection>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
</div>

<h3>How to install?</h3>
<div class=subsection>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
</div>

<h3>How to use?</h3>
<div class=subsection>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</div>

<h2>How do I get help? How do I help?</h2>
<div class=subsection>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</div>

<h2>Authors</h2>
<div class=subsection>
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</div>

<h2>"Competitors"</h2>
<div class=subsection>
<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
  <p>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
  </p>
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
  <p>
    Checks only timestamps and not actual page changes. It's main advanges
    is being written in Python.
  </p>
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
  <p>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
  </p>
  <p>
   There apears to be two wrep's, from two different authors, with totally
   different methods of work.
  </p>
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd><p>A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.</p></dd>
</dl>
</div>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Revision-number: 121
Prop-content-length: 148
Content-length: 148

K 7
svn:log
V 47
Put the manpages in the same block as the rest

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-18T10:39:26.902568Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 8143
Text-content-md5: 922e771a8bacab38099cbe931923df8d
Content-length: 8143

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<h2>Quickie &amp; Download</h2>
<div class=subsection>
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);
			$text = $ver;
			if ($text eq 'snapshot') {
				$text = $file;
				$text =~ s/.*(snapshot-\d+\.\d+\.\d+).*/$1/;
			}

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $text, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
</p>
<ul>
<:listversions():>
</ul>

<p>Contact the Author:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a></li>
 <li>Personal: websec(AT)ev-en.org</li>
</ul>
</div>

<h2>What is WebSec?</h2>
<div class=subsection>
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</div>

<h2>Installation</h2>

<h3>Are there any dependencies?</h3>
<div class=subsection>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>
</div>

<h3>How to install?</h3>
<div class=subsection>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>
</div>

<h3>How to use?</h3>
<div class=subsection>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</div>

<h2>How do I get help? How do I help?</h2>
<div class=subsection>
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</div>

<h2>Authors</h2>
<div class=subsection>
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</div>

<h2>"Competitors"</h2>
<div class=subsection>
<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
  <p>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
  </p>
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
  <p>
    Checks only timestamps and not actual page changes. It's main advanges
    is being written in Python.
  </p>
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
  <p>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
  </p>
  <p>
   There apears to be two wrep's, from two different authors, with totally
   different methods of work.
  </p>
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd><p>A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.</p></dd>
</dl>
</div>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Revision-number: 122
Prop-content-length: 126
Content-length: 126

K 7
svn:log
V 25
Update links in manpages

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-18T10:42:43.084298Z
PROPS-END

Node-path: trunk/websec/ignore.list
Node-kind: file
Node-action: change
Text-content-length: 3774
Text-content-md5: 99a00a432a955af3f5abd3d700b82a77
Content-length: 3774

[General]
all rights reserved
an error occurred
click here
comments
copyright
daily articles for
details
discussion forum
downloads
in issues
last modified
last updated
maintained
posted
posted at
previous cartoon
search by
special offer
the current week
total votes
visits
votes
copyright

[Date_Time]
\d+ Jan(uary)? \d+
\d+ Feb(ruary)? \d+
\d+ Mar(ch)? \d+
\d+ Apr(il)? \d+
\d+ May \d+
\d+ June? \d+
\d+ July? \d+
\d+ Aug(ust)? \d+
\d+ Sep(tember)? \d+
\d+ Oct(ober)? \d+
\d+ Nov(ember)? \d+
\d+ Dec(ember)? \d+
# 28-03-2005 28/03/2005 28.3.2005 2005-03-28
\d+[\/\-.]\d+[\/\-.]\d+
# 02:24 PST
\d{2}:\d{2} [A-Z]{3}

[Adverts]
http://www.news.com/cgi-bin/acc_clickthru
http://ads2.zdnet.com/adverts/
http://doublclick4.net

[VIM]
[\d,]+ scripts, [\d,]+ downloads
[\d,]+ tips, [\d,]+ tip views

[cvsweb]
\d+ (years?|months?|weeks?|days?|hours?|minutes?)

__END__

=head1 NAME

ignore.list - websec url monitoring configuration

=head1 DESCRIPTION

=head2 IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.list" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


=head2 IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.list". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


=head1 SEE ALSO

L<url.list(5)>


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut



Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 8908
Text-content-md5: 06de0167c915fc9c3eb3b5a0adb867a1
Content-length: 8908

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM

URL = http://www.joelonsoftware.com/
Name = Joel on Software
Prefix = joelonsoftware

URL = http://www.openbsd.org/cgi-bin/cvsweb/src/sys/sys/tree.h
Name = OpenBSD tree implementation
Prefix = openbsd-tree
Ignore = cvsweb

URL = http://slashdot.org/
Name = Slashdot
Prefix = slashdot

URL = http://freshmeat.net/
Name = FreshMeat
Prefix = freshmeat

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    AsciiMarker - If set to 1 it will add ascii markers around the changes so
	             that highlighting is noticeable in text mode too. Useful for
				 text MUAs users.

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


=head1 SEE ALSO

L<ignore.list(5)>


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut

vim:set filetype=websec:


Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12957
Text-content-md5: f76730b757ad918ef4054d8376a3bc11
Content-length: 12957

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.6.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$asciimarker = 0;
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "asciimarker"  => \$asciimarker,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage = &MangleHTML($oldpage, @tags);
$newpage = &MangleHTML($newpage, @tags);

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            if ($asciimarker) {
                $token = "###>>>". $token ."<<<###";
            }
            $token =
                    "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR>".
                    "<TD BGCOLOR=$hicolor>" . $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m/^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$/i ) {
            $token = $2;
            if ( !$1 =~ m/^\s*$/ ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup = &ReduceSpaces($token);
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m/^.*?(\b$keyword\b).*?$/i
            || $tokdup =~ m/^.*?(\b$keyword\b).*?$/i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    # If this token contains <= tmin no. of words, don't check
    $tokdup = &ReduceSpaces($token);
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m/~~~~A.*?HREF=.*?$url.*?\@\@\@\@/i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m/~~~~\/A\@\@\@\@/i;
    return 0;
}

sub MangleHTML() {
    my $page = shift(@_);
    my @tags = shift(@_);

    $page =~ s/[\r\n]|\s\s/ /sig;    # Handle MSDOS-style line separators
    $page =~ s/&nbsp;/\@\@\@\@&nbsp;~~~~/sig;   # Handle non-breaking white space
    $page =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>/~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@/sig;    # Handle nested brackets
    foreach (@tags) {
        $tag = $_;
        $page =~ s/<(\/*$tag)>/~~~~$1\@\@\@\@/sig;
        if ( $tag =~ s/\*/ / ) { # XXX WTF is going here with the re?
            $page =~ s/<(\/*$tag.*?)>/~~~~$1\@\@\@\@/sig;
        }
    }

    return $page;
}

sub ReduceSpaces() {
    my $token = shift(@_);
    
    $token =~ s/\@\@\@\@&nbsp;~~~~/ /sig;
    $token =~ s/~~~~/</sig;
    $token =~ s/\@\@\@\@/>/sig;
    $token =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>//sig;
    $token =~ s/<[^>]*>//sig;
    $token =~ s/^\s*//sig;
    $token =~ s/\s*$//sig;
    $token =~ s/\s+/ /sig;

    return $token;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.


=head1 SEE ALSO

L<websec(1)>


=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut

vim:set et ts=4:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 15778
Text-content-md5: c702433ff1d4c7f9922dcf1ec9bb1f8e
Content-length: 15778

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.6.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use LWP::UserAgent;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.6.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.6.0",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 123
Prop-content-length: 130
Content-length: 130

K 7
svn:log
V 29
Update site before update it

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-18T11:41:00.465676Z
PROPS-END

Node-path: trunk/utils/update
Node-kind: file
Node-action: change
Text-content-length: 514
Text-content-md5: d879e6d0f083b3bac8deff7884e1ca30
Content-length: 514

#!/bin/sh

PROJ=websec

#TODO: Check if everything is committed in $PROJ and site

# Create a dump of the repository for online backup
# bzip2 create very small files
# Best after that is gzip --rsyncable not for size as much as for smaller rsync uploads
echo "Dump source control repository"
svnadmin dump -q /home/svn/$PROJ | bzip2 -9 > files/svn/$PROJ.svn-dump.bz2

# Make new site
echo "Update site with new release"
cd site && svn up && make -s
if [ $? != 0 ];
then
	echo "Error while making site"
	exit 1
fi


Revision-number: 124
Prop-content-length: 259
Content-length: 259

K 7
svn:log
V 157
Handle "Content-Encoding: gzip" (compressed data from server), this requires Compress:Gzip but code is added to simply complain if the module isn't present.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-18T17:06:52.475890Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 8668
Text-content-md5: 8feb703ce095884a2b8941a08901a06f
Content-length: 8668

1.7 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

    * Handle Content-Encoding: gzip, but if the Compress::Gzip module is not
      available don't just die.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 16751
Text-content-md5: cef016a851e8236483eb99515e9e5fca
Content-length: 16751

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.6.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
eval { "load Compress::Zlib;" } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.6.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.6.0",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    if (not $feature_compress) {
        print "No feature compress\n";
        $req->push_header("Accept-Encoding" => "identity");
    } else { 
        print "YES feature compress\n";
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 125
Prop-content-length: 159
Content-length: 159

K 7
svn:log
V 58
Wrap a subsection in a <topic> tag for easier maintenance

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-19T10:08:23.493196Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 8061
Text-content-md5: e66b7743ff4fc1154273682b6786e804
Content-length: 8061

<define-tag topic endtag=required>
<h2>%0</h2>
<div class=subsection>
%body
</div>
</define-tag>

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<topic "Quickie &amp; Download">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.
</p>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);
			$text = $ver;
			if ($text eq 'snapshot') {
				$text = $file;
				$text =~ s/.*(snapshot-\d+\.\d+\.\d+).*/$1/;
			}

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $text, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>

<p>
The latest versions are:
</p>
<ul>
<:listversions():>
</ul>

<p>Contact the Author:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a></li>
 <li>Personal: websec(AT)ev-en.org</li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</topic>

<topic '"Competitors"'>
<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
  <p>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
  </p>
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
  <p>
    Checks only timestamps and not actual page changes. It's main advanges
    is being written in Python.
  </p>
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
  <p>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
  </p>
  <p>
   There apears to be two wrep's, from two different authors, with totally
   different methods of work.
  </p>
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd><p>A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.</p></dd>
</dl>
</topic>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Revision-number: 126
Prop-content-length: 173
Content-length: 173

K 7
svn:log
V 72
Move templates to different file and fix bug with snapshot file listing

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-19T13:45:02.546576Z
PROPS-END

Node-path: trunk/site/Makefile
Node-kind: file
Node-action: change
Text-content-length: 832
Text-content-md5: 9396a95decd730375d931b7928e52d96
Content-length: 832

SRC=index.wml
STATIC_SRC=favicon.ico websec.css websec-homepage.old.html \
	   websec-homepage.new.html websec-homepage.highlighted.html \
	   websec-homepage.highlighted.png
OUT=$(SRC:%.wml=%.html)

all: files_changed run install

run:
	wml -n -I . -W "2,-X1034" -D "WML_GEN_ISODATE=`date +%Y-%m-%d`" -o index.html index.wml
	validate index.html

files_changed:
	@ls -R ../files | cat - Makefile | md5sum > md5sum.new
	@if [ ! -e "md5sum.old" ]; then \
		echo "No old md5sum file, recompiling everything."; \
		make clean; \
	else \
		if [ "`cat md5sum.old`" != "`cat md5sum.new`" ]; then \
			echo "File list changed, recompiling!"; \
			make clean; \
		fi ;\
	fi
	@mv md5sum.new md5sum.old

clean:
	-rm -f $(OUT)

install:
	make -C source html install_html DEST=../../realsite/
	install -m 0644 $(OUT) $(STATIC_SRC) ../realsite/


Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 6532
Text-content-md5: 22d020d6ba1da173ff2de06e62f5196a
Content-length: 6532

#use wml::tmpl::defs

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
<:listversions():>
</ul>

<p>Contact the Author:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a></li>
 <li>Personal: websec(AT)ev-en.org</li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<ul>
<li><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></li>
<li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
<li><a href="http://savannah.nongnu.org/files/?group=websec">Download Websec</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></li>
<li><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></li>
</ul>
</topic>

<topic '"Competitors"'>
<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
  <p>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
  </p>
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
  <p>
    Checks only timestamps and not actual page changes. It's main advanges
    is being written in Python.
  </p>
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
  <p>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
  </p>
  <p>
   There apears to be two wrep's, from two different authors, with totally
   different methods of work.
  </p>
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd><p>A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.</p></dd>
</dl>
</topic>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Node-path: trunk/site/tmpl
Node-kind: dir
Node-action: add
Prop-content-length: 10
Content-length: 10

PROPS-END


Node-path: trunk/site/tmpl/defs.wml
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 1596
Text-content-md5: 5d54bc8cba82c2ef6bb7cc4ed555ca01
Content-length: 1606

PROPS-END
<define-tag topic endtag=required>
<h2>%0</h2>
<div class=subsection>
%body
</div>
</define-tag>

<:
	$dirbase="../files/websec.pkg";
	$dlbase="http://savannah.nongnu.org/download/websec/websec.pkg";
	$archive=".tar.gz";
	
	sub basefile
	{
		my $dir = shift(@_);
		my $ver = $dir;
		my $sign = '';
		my $readme = '';
		my $changelog = '';
		my $file = '';

		foreach (`ls $dirbase/$dir`)
		{
			s/^\s+//;
			s/\s+$//;

			# Skip symbolic links
			if ( -l "$dirbase/$dir/$_" ) { next; }

			if (m/.asc$/)
			{
				$sign = $_;
			}
			elsif (m/README$/)
			{
				$readme = $_;
			}
			elsif (m/ChangeLog$/)
			{
				$changelog = $_;
			}
			else {
				$file = $_;
			}
		}

		return ($file, $sign, $readme, $changelog);
	}
	
	sub dlurl
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		return "$dlbase/$ver/$file";
	}

	sub extra
	{
		my $ver = shift(@_);
		my $file = shift(@_);
		my $text= shift(@_);

		if (! $file eq '')
		{
			print ' - <a href="', dlurl($ver, $file), '">', $text, "</a>\n";
		}
	}
	
	sub listversions
	{
		my @vers = ();

		# list all files in $dirbase
		foreach $dir (`ls $dirbase`) 
		{ 
			$dir =~ s/\s//g;
			push @vers, $dir;
		}

		foreach $ver (@vers) {
			my ($file, $sign, $readme, $changelog) = basefile($ver);
			$text = $ver;
			if ($text eq 'snapshot') {
				$text = $file;
				$text =~ s/.*(snapshot-\d+\.\d+\.\d+).*/$1/;
			}

			print "<li>\n";
			print '<a href="', dlurl($ver,$file), '">', $text, "</a>\n";
			extra($ver, $sign, "GPG Signature");
			extra($ver, $readme, "README");
			extra($ver, $changelog, "ChangeLog");
			print "</li>\n";
		}
	}
:>




Revision-number: 127
Prop-content-length: 127
Content-length: 127

K 7
svn:log
V 26
Improve the links section

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-19T15:39:26.034275Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 7392
Text-content-md5: 29ff650ed8f1e0f522a8125ef81eac24
Content-length: 7392

#use wml::tmpl::defs
#use wml::std::href

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
<:listversions():>
</ul>

<p>Contacts:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://savannah.nongnu.org/mail/?group=websec">mailing list</a></li>
 <li>Baruch Even (Author): websec(AT)ev-en.org</li>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
 <li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the config file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dialup, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<dl>
  <dt><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></dt>
  <dd>
    The website of the former developer, frozen from the 1.3.4 days.	 
  </dd>
  
  <dt><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></dt>
  <dd>
	The project page at <href url="http://savannah.gnu.org/" name=Savannah/>,
	they provide us with numerous services and hosting, <strong>Thanks Folks!</strong>.
  </dd>

  <dt><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></dt>
  <dd>
    Old releases are moved to a secondary place so they will be out of the way. They are kept
	for historical fun.
  </dd>

  <dt><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></dt>
  <dd>
    Development is done in a local Subversions Repository since it's easier than having constant online
	connection, so I'm providing the repository dump online for backup and for others benefit.
  </dd>
</dl>
</topic>

<topic '"Competitors"'>
<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
  <p>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
  </p>
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
  <p>
    Checks only timestamps and not actual page changes. It's main advanges
    is being written in Python.
  </p>
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
  <p>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
  </p>
  <p>
   There apears to be two wrep's, from two different authors, with totally
   different methods of work.
  </p>
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd><p>A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.</p></dd>
</dl>
</topic>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Revision-number: 128
Prop-content-length: 198
Content-length: 198

K 7
svn:log
V 97
Fix spelling mistakes (thanks to vimspell and ispell)
Add a paragraph before competitors section

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-19T16:05:48.670278Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 7660
Text-content-md5: 43bd681032d4872278e97cdff5e00f2d
Content-length: 7660

#use wml::tmpl::defs
#use wml::std::href

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
<:listversions():>
</ul>

<p>Contacts:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://savannah.nongnu.org/mail/?group=websec">mailing list</a></li>
 <li>Baruch Even (Author): websec(AT)ev-en.org</li>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
 <li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the configuration file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dial-up, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<dl>
  <dt><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></dt>
  <dd>
    The website of the former developer, frozen from the 1.3.4 days.	 
  </dd>
  
  <dt><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></dt>
  <dd>
	The project page at <href url="http://savannah.gnu.org/" name=Savannah/>,
	they provide us with numerous services and hosting, <strong>Thanks Folks!</strong>.
  </dd>

  <dt><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></dt>
  <dd>
    Old releases are moved to a secondary place so they will be out of the way. They are kept
	for historical fun.
  </dd>

  <dt><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></dt>
  <dd>
    Development is done in a local Subversions Repository since it's easier than having constant online
	connection, so I'm providing the repository dump online for backup and for others benefit.
  </dd>
</dl>
</topic>

<topic '"Competitors"'>
<p>
  Maybe you are not happy with Web Secretary (please tell me why!), or maybe you want to look at
  other options, it's a Free-Software world(!) after-all. Following is a list of programs I've found
  that are somewhat related to what WebSec does.
</p>

<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
  <p>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
  </p>
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
  <p>
    Checks only timestamps and not actual page changes. It's main advantages
    is being written in Python.
  </p>
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
  <p>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
  </p>
  <p>
   There appears to be two wrep's, from two different authors, with totally
   different methods of work.
  </p>
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd><p>A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.</p></dd>
</dl>
</topic>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Revision-number: 129
Prop-content-length: 143
Content-length: 143

K 7
svn:log
V 42
Remove extra spacing caused by paragraphs

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-24T10:37:41.111625Z
PROPS-END

Node-path: trunk/site/index.wml
Node-kind: file
Node-action: change
Text-content-length: 7616
Text-content-md5: e89c1df92d88acf10288a352f9e6e39c
Content-length: 7616

#use wml::tmpl::defs
#use wml::std::href

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet href="websec.css" type="text/css"?>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>
<title>WebSec - A Web Secretary</title>
<meta name=description content="Web Secretary, tracks web page changes for you" />
<link rel="parent" title="Project Page" href="http://nongnu.org/projects/websec/" />
<link rel="author" href="http://baruch.ev-en.org/" />
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="stylesheet" href="websec.css" />
</head>
<body>

<h1>WebSec - A Web Secretary</h1>

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
<:listversions():>
</ul>

<p>Contacts:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://savannah.nongnu.org/mail/?group=websec">mailing list</a></li>
 <li>Baruch Even (Author): websec(AT)ev-en.org</li>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
 <li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the configuration file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dial-up, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</div>

<h2>Links</h2>
<div class=subsection>
<dl>
  <dt><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></dt>
  <dd>
    The website of the former developer, frozen from the 1.3.4 days.	 
  </dd>
  
  <dt><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></dt>
  <dd>
	The project page at <href url="http://savannah.gnu.org/" name=Savannah/>,
	they provide us with numerous services and hosting, <strong>Thanks Folks!</strong>.
  </dd>

  <dt><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></dt>
  <dd>
    Old releases are moved to a secondary place so they will be out of the way. They are kept
	for historical fun.
  </dd>

  <dt><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></dt>
  <dd>
    Development is done in a local Subversions Repository since it's easier than having constant online
	connection, so I'm providing the repository dump online for backup and for others benefit.
  </dd>
</dl>
</topic>

<topic '"Competitors"'>
<p>
  Maybe you are not happy with Web Secretary (please tell me why!), or maybe you want to look at
  other options, it's a Free-Software world(!) after-all. Following is a list of programs I've found
  that are somewhat related to what WebSec does.
</p>

<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
    Checks only timestamps and not actual page changes. It's main advantages
    is being written in Python.
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
   <br/>
   There appears to be two wrep's, from two different authors, with totally
   different methods of work.
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd>
   A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.
 </dd>
</dl>
</topic>

<hr/>
<p class="bottomtext">Last modified by <get-var WML_GEN_REALNAME /> on <get-var WML_GEN_ISODATE /></p>

</body>
</html>


Revision-number: 130
Prop-content-length: 145
Content-length: 145

K 7
svn:log
V 44
Change version numbers for release of 1.7.0

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-31T12:12:55.725974Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 8670
Text-content-md5: 64d5a420e45fddba6b2b258a00e163a8
Content-length: 8670

1.7.0 - Under construction

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

    * Handle Content-Encoding: gzip, but if the Compress::Gzip module is not
      available don't just die.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Node-path: trunk/websec/README
Node-kind: file
Node-action: change
Text-content-length: 4016
Text-content-md5: e842d128a751bca200c3d1dce81686a9
Content-length: 4016

WEB SECRETARY


1. OVERVIEW

Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. Not only does it detect
changes based on content analysis (instead of date/time stamp or simple
textual comparison), it will email the changed page to you WITH THE NEW
CONTENT HIGHLIGHTED!

Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.

Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.


2. DEPENDENCIES

Web Secretary should be able to run on all Unix systems with a Perl
interpreter (and LWP module) installed. At present, it has only been tested
on Linux. Users report that it works on Windows too using ActiveState Perl.


3. INSTALLATION AND CONFIGURATION

Installing Web Secretary is easy.

- Un-tar the distribution. The files will be uncompressed into a directory
  called websec/.

- Change directory to websec/.

- Edit the first lines in websec and webdiff to reflect the actual location
  of the Perl interpreter on your system.

- Edit the URL list called url.list. Please refer to SECTION 5 for more
  information on this.

- Edit the ignore keyword/URL file "ignore.list". Please refer to SECTION 6
  and 7 for more information on this file.


4. USAGE 

You can run Web Secretary whenever you want to monitor the changes in your
URL list by typing 'websec'.

Alternatively, you can add Web Secretary to your crontab and run it on a
regular basis (eg. daily). You can even have different URL list files and
run them at different intervals (eg. hourly, daily, weekly etc.)

It goes without saying that you can use Web Secretary to monitor its own
homepage so that you can be informed of the latest news and updates.

Web Secretary is available at:   http://www.nongnu.org/websec/


5. DIGEST

This feature is contributed by Matti Airas:

> Hi,

> I noticed a small bug in websec. Since I'm using a text-based MUA, I'm
> not interested in getting the HTML pages, but only the notifications.
> However, on line 214 the recipient email address is $email, not
> $emailLink.

> Also, since I found the separate notification mails inconvenient, I
> implemented a new configuration variable "Digest = false|true", which
> makes the notifications to come in a single email.

> The modified file is attached to this mail. I hope you find it useful.

> --
> Matti Airas


6. INSTALLING LWP

Websec requires the Perl module 'LWP' to work. The quickest way to install
this module is to type:

> perl -MCPAN -e 'install Bundle::LWP'

at the command line.

Thanks to Jeff for contributing this tip!


7. ACKNOWLEDGEMENT

I would like to thank the GNU people. I don't know them personally, but they
have blessed us with free and great tools such as Linux, gcc, emacs, Perl,
fetchmail etc. which I now use on a daily basis. In the trails of their
selfless spirit, I will also like to share Web Secretary in the same way,
and hope many people besides me find it useful.

I would also like to thank Chng Tiak Jung, a friend and mentor who inspires
me to learn at least one new thing everyday. I am sure if he continues at
his current pace, I will never be able to catch up with him!

The article "Some Simpler Applications Using LWP" (http://webreview.com/wr/
pub/97/12/12/bookshelf/index.html) by Clinton Wong in webreview.com inspired
me to modify Web Secretary to use the LWP library for HTTP retrieval and
email transmission.



Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12957
Text-content-md5: cf8778da497ed7597c648803755778f4
Content-length: 12957

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.7.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$asciimarker = 0;
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "asciimarker"  => \$asciimarker,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage = &MangleHTML($oldpage, @tags);
$newpage = &MangleHTML($newpage, @tags);

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            if ($asciimarker) {
                $token = "###>>>". $token ."<<<###";
            }
            $token =
                    "<TABLE BORDER=0 CELLPADDING=0 CELLSPACING=0><TR>".
                    "<TD BGCOLOR=$hicolor>" . $token . "</TD></TR></TABLE>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m/^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$/i ) {
            $token = $2;
            if ( !$1 =~ m/^\s*$/ ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup = &ReduceSpaces($token);
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m/^.*?(\b$keyword\b).*?$/i
            || $tokdup =~ m/^.*?(\b$keyword\b).*?$/i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    # If this token contains <= tmin no. of words, don't check
    $tokdup = &ReduceSpaces($token);
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m/~~~~A.*?HREF=.*?$url.*?\@\@\@\@/i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m/~~~~\/A\@\@\@\@/i;
    return 0;
}

sub MangleHTML() {
    my $page = shift(@_);
    my @tags = shift(@_);

    $page =~ s/[\r\n]|\s\s/ /sig;    # Handle MSDOS-style line separators
    $page =~ s/&nbsp;/\@\@\@\@&nbsp;~~~~/sig;   # Handle non-breaking white space
    $page =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>/~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@/sig;    # Handle nested brackets
    foreach (@tags) {
        $tag = $_;
        $page =~ s/<(\/*$tag)>/~~~~$1\@\@\@\@/sig;
        if ( $tag =~ s/\*/ / ) { # XXX WTF is going here with the re?
            $page =~ s/<(\/*$tag.*?)>/~~~~$1\@\@\@\@/sig;
        }
    }

    return $page;
}

sub ReduceSpaces() {
    my $token = shift(@_);
    
    $token =~ s/\@\@\@\@&nbsp;~~~~/ /sig;
    $token =~ s/~~~~/</sig;
    $token =~ s/\@\@\@\@/>/sig;
    $token =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>//sig;
    $token =~ s/<[^>]*>//sig;
    $token =~ s/^\s*//sig;
    $token =~ s/\s*$//sig;
    $token =~ s/\s+/ /sig;

    return $token;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.


=head1 SEE ALSO

L<websec(1)>


=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut

vim:set et ts=4:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 16749
Text-content-md5: 76cc1a1fb6914a681af2c62a6d52f209
Content-length: 16749

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
eval { "load Compress::Zlib;" } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.7",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    if (not $feature_compress) {
        print "No feature compress\n";
        $req->push_header("Accept-Encoding" => "identity");
    } else { 
        print "YES feature compress\n";
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 131
Prop-content-length: 138
Content-length: 138

K 7
svn:log
V 37
Fix release of non-snapshot versions

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-31T12:16:27.740111Z
PROPS-END

Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 1156
Text-content-md5: 9199dcaff0d5b7a293d893244c51fba2
Content-length: 1156

#!/bin/sh

PROJ=websec
VERSION=$1
DIR=files/$PROJ.pkg/$VERSION/
TAG=version-$VERSION
TAGFILE=file:///home/svn/$PROJ/tags/$PROJ/$TAG
DOLINK=0

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

if [ "$VERSION" = "snapshot" ]
then
	echo "Doing snapshot"
	DOLINK=1
	VERSION=snapshot-`date +%Y.%m.%d`
	TAGFILE=file:///home/svn/$PROJ/trunk/$PROJ
	DIR=files/$PROJ.pkg/snapshot
	rm -f $DIR/*
fi

FILE=$PROJ-$VERSION.tar.gz
BASEDIR=$PROJ-$VERSION/

if [ $DOLINK != 0 ]
then
	ln -s $FILE $DIR/websec-snapshot.tar.gz
fi

# Export clean copy of the repository
svn export $TAGFILE $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	rm -rf $BASEDIR
	exit 1
fi

# Prepare version for release
if [ -f $BASEDIR/autogen.sh ]
then
	pushd $BASEDIR
	./autogen.sh
	popd
elif [ -f $BASEDIR/Makefile ]
then
	pushd $BASEDIR
	make
	popd
fi

# Generate ChangeLog
utils/extractNEWS $BASEDIR/NEWS > $DIR/ChangeLog

# Create tar.gz file
tar cvzf $FILE $BASEDIR
mkdir $DIR
mv $FILE $DIR
rm -rf $BASEDIR

# Sign the tar.gz
echo "Now we GPG sign the release:"
pushd $DIR
gpg --detach --armor --sign $FILE
popd

# Locally update the site
utils/update


Revision-number: 132
Prop-content-length: 119
Content-length: 119

K 7
svn:log
V 18
Tag version 1.7.0

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-31T12:17:46.172621Z
PROPS-END

Node-path: tags/websec/version-1.7.0
Node-kind: dir
Node-action: add
Node-copyfrom-rev: 131
Node-copyfrom-path: trunk/websec


Revision-number: 133
Prop-content-length: 131
Content-length: 131

K 7
svn:log
V 30
Set date of release for 1.7.0

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-31T12:23:04.164799Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 8675
Text-content-md5: 9e4c061a1143f7534d99d0d09024df81
Content-length: 8675

1.7.0 - Released on 31-May-2003

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

    * Handle Content-Encoding: gzip, but if the Compress::Gzip module is not
      available don't just die.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Revision-number: 134
Prop-content-length: 156
Content-length: 156

K 7
svn:log
V 55
Remove tag of 1.7.0, there was an error in the release

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-31T12:23:48.304390Z
PROPS-END

Node-path: tags/websec/version-1.7.0
Node-action: delete


Revision-number: 135
Prop-content-length: 130
Content-length: 130

K 7
svn:log
V 29
Remove needless debug prints

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-31T12:27:39.235707Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 16735
Text-content-md5: 7bace2fb863683b0b8267ae3b9bf78b3
Content-length: 16735

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
eval { "load Compress::Zlib;" } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.7",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    if (not $feature_compress) {
        $req->push_header("Accept-Encoding" => "identity");
    } else {
        $req->post_header("Accept-Encoding" => "gzip, identity");
    }

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 136
Prop-content-length: 134
Content-length: 134

K 7
svn:log
V 33
Fix compression handling feature

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-31T12:41:11.410168Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 16749
Text-content-md5: e41704701bda1137f2ec007d03a2e842
Content-length: 16749

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
eval { "load Compress::Zlib;" } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.7",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    my $compress_options = "identity";
    if ($feature_compress) {
        $compress_options = "gzip, $compress_options";
    }
    $req->push_header("Accept-Encoding" => $compress_options);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 137
Prop-content-length: 156
Content-length: 156

K 7
svn:log
V 55
Create release directory before the changelog creation

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-31T12:42:59.026356Z
PROPS-END

Node-path: trunk/utils/release
Node-kind: file
Node-action: change
Text-content-length: 1325
Text-content-md5: 3f39fa3edf97053183b5a0c41754c62c
Content-length: 1325

#!/bin/sh

PROJ=websec
VERSION=$1
DIR=files/$PROJ.pkg/$VERSION/
TAG=version-$VERSION
TAGFILE=file:///home/svn/$PROJ/tags/$PROJ/$TAG
DOLINK=0

if [ -z "$VERSION" ]
then
	echo "No version given."
	exit 1
fi

if [ "$VERSION" = "snapshot" ]
then
	echo "Doing snapshot"
	DOLINK=1
	VERSION=snapshot-`date +%Y.%m.%d`
	TAGFILE=file:///home/svn/$PROJ/trunk/$PROJ
	DIR=files/$PROJ.pkg/snapshot
	rm -f $DIR/*
fi

FILE=$PROJ-$VERSION.tar.gz
BASEDIR=$PROJ-$VERSION/

if [ $DOLINK != 0 ]
then
	ln -s $FILE $DIR/websec-snapshot.tar.gz
fi

# Export clean copy of the repository
svn export $TAGFILE $BASEDIR
if [ $? != 0 ]
then
	echo "Error while checking out the version."
	rm -rf $BASEDIR
	exit 1
fi

# Prepare version for release
if [ -f $BASEDIR/autogen.sh ]
then
	pushd $BASEDIR
	./autogen.sh
	popd
elif [ -f $BASEDIR/Makefile ]
then
	pushd $BASEDIR
	make
	popd
fi

# Create directory for version
echo "Creating directory $DIR"
mkdir $DIR

# Generate ChangeLog
echo "Generating ChangeLog from NEWS"
utils/extractNEWS $BASEDIR/NEWS > $DIR/ChangeLog

# Create tar.gz file
tar cvzf $DIR/$FILE $BASEDIR
rm -rf $BASEDIR

# Sign the tar.gz
echo "Now we GPG sign the release:"
pushd $DIR
gpg --detach --armor --sign $FILE
popd

# Locally update the site
echo "Update website for the release"
utils/update

echo "Done releasing $VERSION"
exit 0


Revision-number: 138
Prop-content-length: 156
Content-length: 156

K 7
svn:log
V 55
Remove duplicates in url.list
Add ignores for slashdot

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-31T12:49:59.190584Z
PROPS-END

Node-path: trunk/websec/ignore.list
Node-kind: file
Node-action: change
Text-content-length: 3797
Text-content-md5: 0120b8dcda123332370befe01ae34124
Content-length: 3797

[General]
all rights reserved
an error occurred
click here
comments
copyright
daily articles for
details
discussion forum
downloads
in issues
last modified
last updated
maintained
posted
posted at
previous cartoon
search by
special offer
the current week
total votes
visits
votes
copyright

[Date_Time]
\d+ Jan(uary)? \d+
\d+ Feb(ruary)? \d+
\d+ Mar(ch)? \d+
\d+ Apr(il)? \d+
\d+ May \d+
\d+ June? \d+
\d+ July? \d+
\d+ Aug(ust)? \d+
\d+ Sep(tember)? \d+
\d+ Oct(ober)? \d+
\d+ Nov(ember)? \d+
\d+ Dec(ember)? \d+
# 28-03-2005 28/03/2005 28.3.2005 2005-03-28
\d+[\/\-.]\d+[\/\-.]\d+
# 02:24 PST
\d{2}:\d{2} [A-Z]{3}

[Adverts]
http://www.news.com/cgi-bin/acc_clickthru
http://ads2.zdnet.com/adverts/
http://doublclick4.net

[VIM]
[\d,]+ scripts, [\d,]+ downloads
[\d,]+ tips, [\d,]+ tip views

[cvsweb]
\d+ (years?|months?|weeks?|days?|hours?|minutes?)

[Slashdot]
\d+ of \d+

__END__

=head1 NAME

ignore.list - websec url monitoring configuration

=head1 DESCRIPTION

=head2 IGNORE KEYWORDS

When determining which parts of a particular web page has changed, you may
want to skip those paragraphs that contains certain predefined words. For
example, pages like InfoWorld, PC Magazine and PC Week often contain the
current date/time regardless of whether there is new or changed content. In
such cases, you can use IGNORE KEYWORDS to skip those paragraphs which
contains date/time information.

Ignore keywords are stored in a file called "ignore.list" in the same
directory as websec. Like the URL list, the ignore keywords are partitioned
into different sections. Each section has a user-defined name. An example is
shown below:

        [General]
        all rights reserved
        an error occurred
        click here
        comments
        copyright

        [Date_Time]
        January\s+\d{1,2}
        February\s+\d{1,2}
        March\s+\d{1,2}
        April\s+\d{1,2}
        May\s+\d{1,2}
    
In the example above, there are two sections: "General" and "Date_Time".
You can use them in the URL list as follows:

    Ignore = General

You can also use multiple sections at one go:

    Ignore = General,Date_Time

If you use certain ignore keywords regularly, you might want to add them to
a defaults section in the URL list.

Ignore keywords can contain regular expressions. For example, the ignore
keyword "January\s+\d{1,2}" tells websec to look for the string "January",
followed by one or more spaces, followed by at least one but not more than
two digits.

Two sections of ignore keywords are supplied in this distribution. "General"
contains some general ignore keywords which you may want to use. "Date_Time"
contains date/time detectors coded using regular expressions. Feel free to
add your own!


=head2 IGNORE URLS

Most advertisements in webpages are of the following form:

        <A HREF="http://page.url.com/advert/cgi-bin/" ...>
        <IMG SRC="advert.animated.gif" ...>
        Click here for free beer!
        </A>

Such advertisements can be ignored when running webdiff using ignore URLs.

Ignore URLs are also stored in "ignore.list". They contain all of parts of
the URL referred to by the <A HREF> tag which you want to ignore. An example
is shown below:

        [Adverts]
        page.url.com/advert/cgi-bin/
    
Use the "Adverts" section in the URL list as follows:

    IgnoreURL = Adverts

You can also use multiple sections at one go:

    IgnoreURL = Adverts1,Adverts2

If you use certain ignore URLs regularly, you might want to add them
to a defaults section in the URL list.

Like ignore keywords, ignore URLs can contain regular expressions.

An "Adverts" section is supplied in this distribution. Feel free to add your
own!


=head1 SEE ALSO

L<url.list(5)>


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut



Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 8833
Text-content-md5: 152a242f2671edb19a79469f71f2eb21
Content-length: 8833

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1
AsciiMarker = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot
Ignore = Slashdot,General,Date_Time

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM

URL = http://www.joelonsoftware.com/
Name = Joel on Software
Prefix = joelonsoftware

URL = http://www.openbsd.org/cgi-bin/cvsweb/src/sys/sys/tree.h
Name = OpenBSD tree implementation
Prefix = openbsd-tree
Ignore = cvsweb

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    AsciiMarker - If set to 1 it will add ascii markers around the changes so
	             that highlighting is noticeable in text mode too. Useful for
				 text MUAs users.

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - Application to run with the diff-output-file. If "mozilla" is 
                 specified here, the diffs are opened in new tabs of the Mozilla
                 Browser.

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


=head1 SEE ALSO

L<ignore.list(5)>


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut

vim:set filetype=websec:


Revision-number: 139
Prop-content-length: 148
Content-length: 148

K 7
svn:log
V 47
Tag version 1.7.0 after checking that it works

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-05-31T12:50:40.816188Z
PROPS-END

Node-path: tags/websec/version-1.7.0
Node-kind: dir
Node-action: add
Node-copyfrom-rev: 138
Node-copyfrom-path: trunk/websec


Revision-number: 140
Prop-content-length: 178
Content-length: 178

K 7
svn:log
V 77
Add special handling of konqueror in the Program command, similar to Mozilla

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-06-03T05:59:43.497168Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 8759
Text-content-md5: f2760b10f8b35045e5b1f7481c18f8af
Content-length: 8759

2.0.0 - Release on ...

    * Special support for konqueror for Program directive.

1.7.0 - Released on 31-May-2003

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

    * Handle Content-Encoding: gzip, but if the Compress::Gzip module is not
      available don't just die.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 8991
Text-content-md5: 87798a901259048f2e3231dd6ca4adff
Content-length: 8991

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences; or "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file, special cases:
#              "mozilla", pages are opened in new tabs,
#              "konqueror", pages are opened using "kfmclient openURL"
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1
AsciiMarker = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot
Ignore = Slashdot,General,Date_Time

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM

URL = http://www.joelonsoftware.com/
Name = Joel on Software
Prefix = joelonsoftware

URL = http://www.openbsd.org/cgi-bin/cvsweb/src/sys/sys/tree.h
Name = OpenBSD tree implementation
Prefix = openbsd-tree
Ignore = cvsweb

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    AsciiMarker - If set to 1 it will add ascii markers around the changes so
	             that highlighting is noticeable in text mode too. Useful for
				 text MUAs users.

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - application to call with the diff-file, special cases:
                 "mozilla", pages are opened in new tabs,
                 "konqueror", pages are opened using "kfmclient openURL"

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


=head1 SEE ALSO

L<ignore.list(5)>


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut

vim:set filetype=websec:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 17165
Text-content-md5: 740368a9733c5c04536ebca0b1517066
Content-length: 17165

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
eval { "load Compress::Zlib;" } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$outgoing     = "$base/index.html";
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

# prepare digest
@digest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.7",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($outgoing);
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    my $compress_options = "identity";
    if ($feature_compress) {
        $compress_options = "gzip, $compress_options";
    }
    $req->push_header("Accept-Encoding" => $compress_options);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        ShowDocument( $program, $outgoing );
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 141
Prop-content-length: 151
Content-length: 151

K 7
svn:log
V 50
Add support for HTML::Diff for diffing HTML files

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-06-26T06:14:55.725244Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 8894
Text-content-md5: c60f28a8243d87b94790ab7c0b4c0370
Content-length: 8894

2.0.0 - Release on ...

    * Support for diffing with HTML::Diff from CPAN, this has support to show
      the removed text in addition to the changed text.

    * Special support for konqueror for Program directive.

1.7.0 - Released on 31-May-2003

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

    * Handle Content-Encoding: gzip, but if the Compress::Gzip module is not
      available don't just die.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Node-path: trunk/websec/htmldiff
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 990
Text-content-md5: 6bcb60ec1f7c1e0cb175135e41cb4987
Content-length: 1000

PROPS-END
#!/usr/bin/perl

#
# htmldiff uses HTML::Diff to create an HTML file that shows the difference
# between two HTML files, given on the command line.
#
# Contributed by Maurice Aubrey <maurice@redweek.com>
#

use strict;
use HTML::Diff;

@ARGV == 2 or die "Usage: $0 <file1> <file2>\n";


my @txt;
foreach (@ARGV) {
  open my $fh, $_ or die "unable to read '$_': $!";
  local $/;
  push @txt, scalar <$fh>;
}

my $changeStatus = 0;

print qq{<style type="text/css"><!-- ins{color: green} del{color:red}--></style>\n};
foreach (@{ html_word_diff(@txt) }) {
  my($type, $left, $right) = @$_;

  # debug
  #$left =~ s/\n/ /g;
  #$right =~ s/\n/ /g;
  #print "TYPE:$type\nLEFT: $left\nRIGHT: $right\n\n";
  #next;

  if ($type eq 'u') {
    print $left;
  } else {
    print "<del>$left</del>" if length $left;
    print "<ins>$right</ins>" if length $right;
    $changeStatus = 1 if (length $left or length $right);
  }
}

# print "exiting with status: ".$changeStatus."\n";
exit $changeStatus;


Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 9744
Text-content-md5: fbb0cd2a969d87d0d1e5d805f4cfab98
Content-length: 9744

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences,
#              "htmldiff" to use HTML::Diff from cpan
#              "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file, special cases:
#              "mozilla", pages are opened in new tabs,
#              "konqueror", pages are opened using "kfmclient openURL"
# ProgramDigest = if specified "true", the application specified under "Program" is
#              invoked with a summary page that provides links to all changed pages.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1
AsciiMarker = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

URL = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot
Ignore = Slashdot,General,Date_Time

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM

URL = http://www.joelonsoftware.com/
Name = Joel on Software
Prefix = joelonsoftware

URL = http://www.openbsd.org/cgi-bin/cvsweb/src/sys/sys/tree.h
Name = OpenBSD tree implementation
Prefix = openbsd-tree
Ignore = cvsweb

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.
                 Put "htmldiff" to use a different implementation of HTML
                 diff output. Note that you need to install the perl-modules
                 Algorithm::Diff and HTML::Diff which are availabe on 
                 http://www.cpan.org/ for this to work.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    AsciiMarker - If set to 1 it will add ascii markers around the changes so
	             that highlighting is noticeable in text mode too. Useful for
				 text MUAs users.

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - application to call with the diff-file, special cases:
                 "mozilla", pages are opened in new tabs,
                 "konqueror", pages are opened using "kfmclient openURL"

    ProgramDigest - If specified "true", websec does not open all changed pages 
                 separately with the application specified in "Program", but opens
                 a summary page that contains links to all changed pages. 

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


=head1 SEE ALSO

L<ignore.list(5)>


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut

vim:set filetype=websec:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 20669
Text-content-md5: fc748e51bd1a2a9e29f959aa35f7b629
Content-length: 20669

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
eval { "load Compress::Zlib;" } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$page_current = "$base/retrieve.html";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

$htmldiffbin = "$FindBin::Bin/htmldiff";
if ( !-e $htmldiffbin ) {
  $htmldiffbin = "./htmldiff";
  if ( !-e $htmldiffbin ) {
      $htmldiffbin = "htmldiff";
  }
}

# prepare digest
@digest = ();
@htmldigest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    ProgramDigest => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.7",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

if (@htmldigest) {
    open( OUTPAGE, "> $base/index.html" ) or die "Cannot open $base/index.html: $!\n";
    print OUTPAGE "<HTML><BODY>The contents of the following URLs has changed:<P><P>";
    foreach (@htmldigest) { print OUTPAGE "$_<P>\n"; }
    print OUTPAGE "<P></BODY></HTML>";
    close(OUTPAGE);

    &ShowDocument( $program, $base."/index.html");
}


# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $programdigest = $siteinfo{ProgramDigest};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $outgoing     = "$pagebase.diff.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";
     $htmldiff = "$htmldiffbin $page_previous $page_current > $outgoing";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    my $compress_options = "identity";
    if ($feature_compress) {
        $compress_options = "gzip, $compress_options";
    }
    $req->push_header("Accept-Encoding" => $compress_options);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        elsif ( $diff eq "htmldiff" )
        {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage using htmldiff...\n";
                $rc = system($htmldiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 142
Prop-content-length: 154
Content-length: 154

K 7
svn:log
V 53
Use SPAN tags instead of TABLE tags for highlighting

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-06-26T06:18:58.862418Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 9015
Text-content-md5: 3538ad4516fa6dd814392008f62b7178
Content-length: 9015

2.0.0 - Release on ...

    * Support for diffing with HTML::Diff from CPAN, this has support to show
      the removed text in addition to the changed text.

    * Special support for konqueror for Program directive.

    * Replace TABLE tags with SPAN tags to highlight differences, this is
      reported to work better for some users.

1.7.0 - Released on 31-May-2003

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

    * Handle Content-Encoding: gzip, but if the Compress::Gzip module is not
      available don't just die.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12916
Text-content-md5: b35b1608886b5c7247713b66ad015484
Content-length: 12916

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.7.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$asciimarker = 0;
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "asciimarker"  => \$asciimarker,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage = &MangleHTML($oldpage, @tags);
$newpage = &MangleHTML($newpage, @tags);

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            if ($asciimarker) {
                $token = "###>>>". $token ."<<<###";
            }
            $token =
                    "<span style=\"background-color: $hicolor\">"
                    . $token . "</span>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m/^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$/i ) {
            $token = $2;
            if ( !$1 =~ m/^\s*$/ ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup = &ReduceSpaces($token);
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m/^.*?(\b$keyword\b).*?$/i
            || $tokdup =~ m/^.*?(\b$keyword\b).*?$/i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    # If this token contains <= tmin no. of words, don't check
    $tokdup = &ReduceSpaces($token);
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m/~~~~A.*?HREF=.*?$url.*?\@\@\@\@/i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m/~~~~\/A\@\@\@\@/i;
    return 0;
}

sub MangleHTML() {
    my $page = shift(@_);
    my @tags = shift(@_);

    $page =~ s/[\r\n]|\s\s/ /sig;    # Handle MSDOS-style line separators
    $page =~ s/&nbsp;/\@\@\@\@&nbsp;~~~~/sig;   # Handle non-breaking white space
    $page =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>/~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@/sig;    # Handle nested brackets
    foreach (@tags) {
        $tag = $_;
        $page =~ s/<(\/*$tag)>/~~~~$1\@\@\@\@/sig;
        if ( $tag =~ s/\*/ / ) { # XXX WTF is going here with the re?
            $page =~ s/<(\/*$tag.*?)>/~~~~$1\@\@\@\@/sig;
        }
    }

    return $page;
}

sub ReduceSpaces() {
    my $token = shift(@_);
    
    $token =~ s/\@\@\@\@&nbsp;~~~~/ /sig;
    $token =~ s/~~~~/</sig;
    $token =~ s/\@\@\@\@/>/sig;
    $token =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>//sig;
    $token =~ s/<[^>]*>//sig;
    $token =~ s/^\s*//sig;
    $token =~ s/\s*$//sig;
    $token =~ s/\s+/ /sig;

    return $token;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.


=head1 SEE ALSO

L<websec(1)>


=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut

vim:set et ts=4:


Revision-number: 143
Prop-content-length: 126
Content-length: 126

K 7
svn:log
V 25
Make htmldiff executable

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-06-28T20:20:38.979051Z
PROPS-END

Node-path: trunk/websec/htmldiff
Node-kind: file
Node-action: change
Prop-content-length: 36
Content-length: 36

K 14
svn:executable
V 1
*
PROPS-END


Revision-number: 144
Prop-content-length: 183
Content-length: 183

K 7
svn:log
V 82
Use sendmail from /usr/sbin if it's not in /usr/lib/ (patch from Michael Wittman)

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-11-22T06:35:56.759620Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 9156
Text-content-md5: 04b6bbaa7879305c708e34543975c34b
Content-length: 9156

2.0.0 - Release on ...

    * Support for diffing with HTML::Diff from CPAN, this has support to show
      the removed text in addition to the changed text.

    * Special support for konqueror for Program directive.

    * Replace TABLE tags with SPAN tags to highlight differences, this is
      reported to work better for some users.

    * Use sendmail from /usr/sbin/ if it's no available in /usr/lib/, this
      fixes a problem with RedHat 9. (Thanks to Michael Wittman)

1.7.0 - Released on 31-May-2003

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

    * Handle Content-Encoding: gzip, but if the Compress::Gzip module is not
      available don't just die.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 21091
Text-content-md5: 270f2a51ae5b14b8a01d705d798d849f
Content-length: 21091

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
eval { "load Compress::Zlib;" } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$page_current = "$base/retrieve.html";

# Red Hat has sendmail in /usr/sbin/sendmail, but LWP::Protocol::mailto
# expects it in /usr/lib/sendmail.  Revert to the Red Hat location if there
# is no executable at the expected location.
use vars qw($LWP::Protocol::mailto::SENDMAIL);
$LWP::Protocol::mailto::SENDMAIL = "/usr/sbin/sendmail"
  unless -x "/usr/lib/sendmail";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

$htmldiffbin = "$FindBin::Bin/htmldiff";
if ( !-e $htmldiffbin ) {
  $htmldiffbin = "./htmldiff";
  if ( !-e $htmldiffbin ) {
      $htmldiffbin = "htmldiff";
  }
}

# prepare digest
@digest = ();
@htmldigest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    ProgramDigest => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.7",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

if (@htmldigest) {
    open( OUTPAGE, "> $base/index.html" ) or die "Cannot open $base/index.html: $!\n";
    print OUTPAGE "<HTML><BODY>The contents of the following URLs has changed:<P><P>";
    foreach (@htmldigest) { print OUTPAGE "$_<P>\n"; }
    print OUTPAGE "<P></BODY></HTML>";
    close(OUTPAGE);

    &ShowDocument( $program, $base."/index.html");
}


# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $programdigest = $siteinfo{ProgramDigest};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $outgoing     = "$pagebase.diff.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";
     $htmldiff = "$htmldiffbin $page_previous $page_current > $outgoing";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = new HTTP::Request( 'GET', $url );
    if ( $auth ne "none" ) {
        $req->authorization_basic( split ( /:/, $auth, 2 ) );
    }

    if ( $proxyAuth ne "none" ) {
        $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) );
    }
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    my $compress_options = "identity";
    if ($feature_compress) {
        $compress_options = "gzip, $compress_options";
    }
    $req->push_header("Accept-Encoding" => $compress_options);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);
        if ( $resp->is_success ) { last; }
        else {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
        }
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        elsif ( $diff eq "htmldiff" )
        {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage using htmldiff...\n";
                $rc = system($htmldiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        my $resp = $ua->request($req);
        die "Error mailing document: ".$resp->message()."\n" if $resp->is_error;
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 145
Prop-content-length: 152
Content-length: 152

K 7
svn:log
V 51
Add an attempt on how to provide patches document.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-11-22T06:47:12.658931Z
PROPS-END

Node-path: trunk/websec/HACKING
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 1348
Text-content-md5: d51f2c23bfc5b38732987929568b328b
Content-length: 1358

PROPS-END
How to hack on WebSec
---------------------

So you've had an itch with WebSec, and you scratched it, Cool! You now want
everyone to enjoy your improvement and get the world renowned status of WebSec
hacker, so you send it over to the maintainer or to the mailing list. Since the
maintainer tries to have a semblence of life and since the work his work is not
only to apply a patch it takes a lot of time to get the patch applied.

The way to make patches be applied faster try to make sure the maintainer needs
to do as little work as possible. Here are a few hints:

- Follow the coding style of the current code.
  The maintainer is not really a Perl coder and doesn't have a well defined
  style for Perl programming he just follows his aesthetics sense, follow his
  and he'll have less to modify in the code.
  Suggestions for better Perl Coding Style are always welcome, but don't force
  it down his throat :-)

- Add a few lines to the NEWS file, put them at the very start of the file or
  you could add them in the text of your patch submission, this way the
  maintainer won't need to actually make his function to apply the patch.

- Is it a new feature you did? Great, But did you add to the documentation the
  new feature? Who else is better qualified to fix the now outdated docs than
  the author of the patch.

That's all folks.


Revision-number: 146
Prop-content-length: 169
Content-length: 169

K 7
svn:log
V 68
Import messages from Yahoo Groups to this file for easier reference

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-11-22T07:21:52.749403Z
PROPS-END

Node-path: trunk/websec/TODO
Node-kind: file
Node-action: change
Text-content-length: 4757
Text-content-md5: dd0cc74777125241f29ddf61ae97d61c
Content-length: 4757

Next Release
------------

http://groups.yahoo.com/group/websec/message/79 has an elisp mode to highlight
url.list.
  + need an equivalent from vim.
  + Find out where to install the emacs mode
  ? Fix it, it seems broken, need user input on this.

Adam Stanley provided patches to add:
 + 'MailFrom', the address from which the mail will come from.
 - 'DiffOnly', send only the difference, as opposed to whole document with highlighted differences
 - Follow Refresh zero, Follow links if the site uses refresh zero.


Short Term
----------

B3450, Inconsistency with the .websec/url.list location finding and using websec with a file parameter.

Run the html output of webdiff through a text mode browser for the benefit of
text MUA users. This, coupled with the DiffOnly patch will provide an
approximate for the request of Daniel Fraga.

B3497, Show also removed content from the html page, possibly use HTML::Diff

Enable running multiple websec instances at the same time:
MSG70 - see below
  - Implement

Send mail from windows:
http://groups.yahoo.com/group/websec/message/21
  - Improve mail sending

Problem with script pages (eg Slashdot):
http://groups.yahoo.com/group/websec/message/35 
  - Check if needed

Patch to remove script/noscript:
http://groups.yahoo.com/group/websec/message/38
  - Check if needed, related to Slashdot issues.

Multi-User patches and RPM spec:
http://groups.yahoo.com/group/websec/message/45
  - Investigate

B3449, Notify when pages in url.list haven't been updated for a long time.

B3472, Provide a method to specify how often to check for changes, mostly for dialup users.
Can also be used for a constantly running program.


Long Term
---------

Keep a website as an image to be put in the desktop background.
MSG72 - see below
  - Investigate

Create GUI for websec, similar to kwebwatch, but implement the GUI with Perl
bindings, this will allow wxWindows bindings for windows and gtk/qt bindings for
Linux.
  - Implement, need to modulrize websec to work as a library and have differt
    GUI, where 'text' is one of them and is the same as now, a simple command
    line.


Messages from Yahoo Groups
==========================

MSG70
-----

From seijit@bigfoot.com Sun Jul 29 19:18:14 2001
Return-Path: <toku@nic.nec.co.jp>
X-Sender: toku@nic.nec.co.jp
X-Apparently-To: websec@yahoogroups.com
Date: Mon, 30 Jul 2001 11:17:39 +0900
Message-ID: <m3wv4rp3cs.wl@kux7.isc.nws.nesic.nec.co.jp>
To: websec@yahoogroups.com
Subject: race condition fix.
Content-Type: text/plain; charset=US-ASCII
From: seijit@bigfoot.com
X-Yahoo-Message-Num: 70

Hi,

I use a lot of cron'ed websec and sometimes 2 or more websec runs at
the same time creating (I think) a race condition. Sometimes I get
weird HTML email where the subject and the body doesn't match.

So I think this should fix it.

Seiji T.

P.S. Bottom part of the diff adds no caching keyword to the request
which isn't related to this issue. BTW, "max-age=..." seems to be
automatically added to "Cache-Control" probably by LWP, so I don't
know if it's valid.

diff -c websec.ORG websec
*** websec.ORG	Tue Jun 26 11:04:36 2001
--- websec	Fri Jul 27 18:29:03 2001
***************
*** 42,49 ****
# Prepare pathnames.
($base = $0) =~ s:[^/]+$::;
$archive = "archive/";
! $outgoing = $base . "index.html";
! $page_current = $base . "retrieve.html";

# prepare digest
@digest = ();
--- 42,49 ----
# Prepare pathnames.
($base = $0) =~ s:[^/]+$::;
$archive = "archive/";
! $outgoing = $base . $$ . ".index.html";
! $page_current = $base . $$ . ".retrieve.html";

# prepare digest
@digest = ();
***************
*** 198,203 ****
--- 198,207 ----
$ua->env_proxy;
if ($proxy ne "") { $ua->proxy(http => $proxy); }
$req = new HTTP::Request('GET', $url);
+ $req->remove_header( "Cache-Control" );
+ $req->remove_header( "Pragma" );
+ $req->push_header( "Cache-Control" => "no-cache" );
+ $req->push_header( "Pragma" => "no-cache" );
if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
if ($proxyAuth ne "none") {
$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));

MSG72
-----

From f.gassauer@aon.at Sun Sep 02 09:29:02 2001
Return-Path: <f.gassauer@aon.at>
X-Sender: f.gassauer@aon.at
X-Apparently-To: websec@egroups.com
To: websec@yahoogroups.com
Subject: Web Secretary Ver 1.3.4
Date: Sun, 2 Sep 2001 18:26:46 +0200
Message-Id: <20010902162647.NFSV10991.viefep15-int.chello.at@there>
From: Ferdinand Gassauer <f.gassauer@aon.at>
X-Yahoo-Message-Num: 72

Hi!
I just wanted to know if (and how) it is possible to save an hourly changed 
web site (gif image) to my local hard disk (and use it as desktop background) 
instead of letting it by emailed to me.
thanx
-- 

regards
Ferdinand Gassauer
mailto:f.gassauer@aon.at
http://www.goesing.at



Revision-number: 147
Prop-content-length: 127
Content-length: 127

K 7
svn:log
V 26
Follow zero refresh pages

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-11-22T08:10:27.212952Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 9267
Text-content-md5: 9693e3be348f3c8c73c0cccee93a4f64
Content-length: 9267

2.0.0 - Release on ...

    * Support for diffing with HTML::Diff from CPAN, this has support to show
      the removed text in addition to the changed text.

    * Special support for konqueror for Program directive.

    * Replace TABLE tags with SPAN tags to highlight differences, this is
      reported to work better for some users.

    * Use sendmail from /usr/sbin/ if it's no available in /usr/lib/, this
      fixes a problem with RedHat 9. (Thanks to Michael Wittman)

    * Follow refresh links, this is useful to pass automatic forwarding pages.
      (Thanks to Adam Stanley)

1.7.0 - Released on 31-May-2003

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

    * Handle Content-Encoding: gzip, but if the Compress::Gzip module is not
      available don't just die.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 21822
Text-content-md5: 5626fbd7da3d2efe68df7a699a19a2d4
Content-length: 21822

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
use URI;
eval { "load Compress::Zlib;" } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters
if ( -e "url.list" ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$page_current = "$base/retrieve.html";

# Red Hat has sendmail in /usr/sbin/sendmail, but LWP::Protocol::mailto
# expects it in /usr/lib/sendmail.  Revert to the Red Hat location if there
# is no executable at the expected location.
use vars qw($LWP::Protocol::mailto::SENDMAIL);
$LWP::Protocol::mailto::SENDMAIL = "/usr/sbin/sendmail"
  unless -x "/usr/lib/sendmail";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

$htmldiffbin = "$FindBin::Bin/htmldiff";
if ( !-e $htmldiffbin ) {
  $htmldiffbin = "./htmldiff";
  if ( !-e $htmldiffbin ) {
      $htmldiffbin = "htmldiff";
  }
}

# prepare digest
@digest = ();
@htmldigest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    ProgramDigest => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.7",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/url.list" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

if (@htmldigest) {
    open( OUTPAGE, "> $base/index.html" ) or die "Cannot open $base/index.html: $!\n";
    print OUTPAGE "<HTML><BODY>The contents of the following URLs has changed:<P><P>";
    foreach (@htmldigest) { print OUTPAGE "$_<P>\n"; }
    print OUTPAGE "<P></BODY></HTML>";
    close(OUTPAGE);

    &ShowDocument( $program, $base."/index.html");
}


# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $programdigest = $siteinfo{ProgramDigest};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $outgoing     = "$pagebase.diff.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";
     $htmldiff = "$htmldiffbin $page_previous $page_current > $outgoing";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = PrepareRequest($url);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);

        if ( ! $resp->is_success ) {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
            next;
        }

        # Leave if there is no refresh header
        if (!$resp->header("Refresh")) { last; }

        # Handle it if the refresh is for zero seconds
        ( $time, $refresh_url ) = split(/;/, $resp->header("Refresh"), 2);
        if ($time > 0) { last; }
        
        # Convert to absolute URL and refetch the page
        ( undef, $refresh_to ) = split(/=/, $refresh_url, 2);
        $newurl = URI->new_abs($refresh_to, $url)->as_string();

        $req = PrepareRequest($newurl);
        # Don't reset the counter, we still want to protect from endless loops
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        elsif ( $diff eq "htmldiff" )
        {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage using htmldiff...\n";
                $rc = system($htmldiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

sub PrepareRequest() {
    my $url = shift (@_);

    $req = new HTTP::Request( 'GET', $url );

    my $auth = $siteinfo{Auth};
    if ( $auth ne "none" ) { $req->authorization_basic( split ( /:/, $auth, 2 ) ); }

    my $proxyAuth = $siteinfo{$proxyAuth};
    if ( $proxyAuth ne "none" ) { $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) ); }
    
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    
    my $compress_options = "identity";
    if ($feature_compress) { $compress_options = "gzip, $compress_options"; }
    $req->push_header("Accept-Encoding" => $compress_options);

    return $req;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        my $resp = $ua->request($req);
        die "Error mailing document: ".$resp->message()."\n" if $resp->is_error;
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 148
Prop-content-length: 138
Content-length: 138

K 7
svn:log
V 37
Implemented the follow refresh patch

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-11-22T08:13:13.452185Z
PROPS-END

Node-path: trunk/websec/TODO
Node-kind: file
Node-action: change
Text-content-length: 4757
Text-content-md5: 86544fa4ee9a7301431fcb138bd32e0f
Content-length: 4757

Next Release
------------

http://groups.yahoo.com/group/websec/message/79 has an elisp mode to highlight
url.list.
  + need an equivalent from vim.
  + Find out where to install the emacs mode
  ? Fix it, it seems broken, need user input on this.

Adam Stanley provided patches to add:
 + 'MailFrom', the address from which the mail will come from.
 - 'DiffOnly', send only the difference, as opposed to whole document with highlighted differences
 + Follow Refresh zero, Follow links if the site uses refresh zero.


Short Term
----------

B3450, Inconsistency with the .websec/url.list location finding and using websec with a file parameter.

Run the html output of webdiff through a text mode browser for the benefit of
text MUA users. This, coupled with the DiffOnly patch will provide an
approximate for the request of Daniel Fraga.

B3497, Show also removed content from the html page, possibly use HTML::Diff

Enable running multiple websec instances at the same time:
MSG70 - see below
  - Implement

Send mail from windows:
http://groups.yahoo.com/group/websec/message/21
  - Improve mail sending

Problem with script pages (eg Slashdot):
http://groups.yahoo.com/group/websec/message/35 
  - Check if needed

Patch to remove script/noscript:
http://groups.yahoo.com/group/websec/message/38
  - Check if needed, related to Slashdot issues.

Multi-User patches and RPM spec:
http://groups.yahoo.com/group/websec/message/45
  - Investigate

B3449, Notify when pages in url.list haven't been updated for a long time.

B3472, Provide a method to specify how often to check for changes, mostly for dialup users.
Can also be used for a constantly running program.


Long Term
---------

Keep a website as an image to be put in the desktop background.
MSG72 - see below
  - Investigate

Create GUI for websec, similar to kwebwatch, but implement the GUI with Perl
bindings, this will allow wxWindows bindings for windows and gtk/qt bindings for
Linux.
  - Implement, need to modulrize websec to work as a library and have differt
    GUI, where 'text' is one of them and is the same as now, a simple command
    line.


Messages from Yahoo Groups
==========================

MSG70
-----

From seijit@bigfoot.com Sun Jul 29 19:18:14 2001
Return-Path: <toku@nic.nec.co.jp>
X-Sender: toku@nic.nec.co.jp
X-Apparently-To: websec@yahoogroups.com
Date: Mon, 30 Jul 2001 11:17:39 +0900
Message-ID: <m3wv4rp3cs.wl@kux7.isc.nws.nesic.nec.co.jp>
To: websec@yahoogroups.com
Subject: race condition fix.
Content-Type: text/plain; charset=US-ASCII
From: seijit@bigfoot.com
X-Yahoo-Message-Num: 70

Hi,

I use a lot of cron'ed websec and sometimes 2 or more websec runs at
the same time creating (I think) a race condition. Sometimes I get
weird HTML email where the subject and the body doesn't match.

So I think this should fix it.

Seiji T.

P.S. Bottom part of the diff adds no caching keyword to the request
which isn't related to this issue. BTW, "max-age=..." seems to be
automatically added to "Cache-Control" probably by LWP, so I don't
know if it's valid.

diff -c websec.ORG websec
*** websec.ORG	Tue Jun 26 11:04:36 2001
--- websec	Fri Jul 27 18:29:03 2001
***************
*** 42,49 ****
# Prepare pathnames.
($base = $0) =~ s:[^/]+$::;
$archive = "archive/";
! $outgoing = $base . "index.html";
! $page_current = $base . "retrieve.html";

# prepare digest
@digest = ();
--- 42,49 ----
# Prepare pathnames.
($base = $0) =~ s:[^/]+$::;
$archive = "archive/";
! $outgoing = $base . $$ . ".index.html";
! $page_current = $base . $$ . ".retrieve.html";

# prepare digest
@digest = ();
***************
*** 198,203 ****
--- 198,207 ----
$ua->env_proxy;
if ($proxy ne "") { $ua->proxy(http => $proxy); }
$req = new HTTP::Request('GET', $url);
+ $req->remove_header( "Cache-Control" );
+ $req->remove_header( "Pragma" );
+ $req->push_header( "Cache-Control" => "no-cache" );
+ $req->push_header( "Pragma" => "no-cache" );
if ($auth ne "none") { $req->authorization_basic(split(/:/, $auth, 2)); }
if ($proxyAuth ne "none") {
$req->proxy_authorization_basic(split(/:/, $proxyAuth, 2));

MSG72
-----

From f.gassauer@aon.at Sun Sep 02 09:29:02 2001
Return-Path: <f.gassauer@aon.at>
X-Sender: f.gassauer@aon.at
X-Apparently-To: websec@egroups.com
To: websec@yahoogroups.com
Subject: Web Secretary Ver 1.3.4
Date: Sun, 2 Sep 2001 18:26:46 +0200
Message-Id: <20010902162647.NFSV10991.viefep15-int.chello.at@there>
From: Ferdinand Gassauer <f.gassauer@aon.at>
X-Yahoo-Message-Num: 72

Hi!
I just wanted to know if (and how) it is possible to save an hourly changed 
web site (gif image) to my local hard disk (and use it as desktop background) 
instead of letting it by emailed to me.
thanx
-- 

regards
Ferdinand Gassauer
mailto:f.gassauer@aon.at
http://www.goesing.at



Revision-number: 149
Prop-content-length: 176
Content-length: 176

K 7
svn:log
V 75
Allow using a URL list file other than url.list (Thanks to Sacha Fournier)

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-11-22T08:25:17.376640Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 9355
Text-content-md5: 7201007ff9687d54eeefb9fd11d399ef
Content-length: 9355

2.0.0 - Release on ...

    * Support for diffing with HTML::Diff from CPAN, this has support to show
      the removed text in addition to the changed text.

    * Special support for konqueror for Program directive.

    * Replace TABLE tags with SPAN tags to highlight differences, this is
      reported to work better for some users.

    * Use sendmail from /usr/sbin/ if it's no available in /usr/lib/, this
      fixes a problem with RedHat 9. (Thanks to Michael Wittman)

    * Follow refresh links, this is useful to pass automatic forwarding pages.
      (Thanks to Adam Stanley)

    * Allow using a URL list file other than url.list (Thanks to Sacha
      Fournier)

1.7.0 - Released on 31-May-2003

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

    * Handle Content-Encoding: gzip, but if the Compress::Gzip module is not
      available don't just die.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 21973
Text-content-md5: 0a36e8db8b0598312aab1be865f95ac2
Content-length: 21973

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
use URI;
eval { "load Compress::Zlib;" } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters

$urllist = "url.list";
GetOptions("urllist=s" => \$urllist);

if ( -e $urllist ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$page_current = "$base/retrieve.html";

# Red Hat has sendmail in /usr/sbin/sendmail, but LWP::Protocol::mailto
# expects it in /usr/lib/sendmail.  Revert to the Red Hat location if there
# is no executable at the expected location.
use vars qw($LWP::Protocol::mailto::SENDMAIL);
$LWP::Protocol::mailto::SENDMAIL = "/usr/sbin/sendmail"
  unless -x "/usr/lib/sendmail";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

$htmldiffbin = "$FindBin::Bin/htmldiff";
if ( !-e $htmldiffbin ) {
  $htmldiffbin = "./htmldiff";
  if ( !-e $htmldiffbin ) {
      $htmldiffbin = "htmldiff";
  }
}

# prepare digest
@digest = ();
@htmldigest = ();

# Set default values
local %defaults = (
    URL        => "",
    Auth       => "none",
    Name       => "",
    Prefix     => "",
    Diff       => "webdiff",
    Hicolor    => "blue",
    AsciiMarker => 0,
    Ignore     => "none",
    IgnoreURL  => "none",
    Email      => "",
    EmailLink  => "",
    EmailError => 1,
    Program    => "",
    ProgramDigest => "",
    Proxy      => "",
    ProxyAuth  => "none",
    Randomwait => 0,
    Retry      => 3,
    Retrywait  => 0,
    Timeout    => 20,
    Tmin       => 0,
    Tmax       => 99999,
    AddSubject => "",
    Digest     => "false",
    UserAgent  => "WebSec/1.7",
    DateFMT    => " - %d %B %Y (%a)",
    MailFrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/" . "$urllist" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{URL} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{MailFrom} );
}

if (@htmldigest) {
    open( OUTPAGE, "> $base/index.html" ) or die "Cannot open $base/index.html: $!\n";
    print OUTPAGE "<HTML><BODY>The contents of the following URLs has changed:<P><P>";
    foreach (@htmldigest) { print OUTPAGE "$_<P>\n"; }
    print OUTPAGE "<P></BODY></HTML>";
    close(OUTPAGE);

    &ShowDocument( $program, $base."/index.html");
}


# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{URL};
    $auth       = $siteinfo{Auth};
    $name       = $siteinfo{Name};
    $prefix     = $siteinfo{Prefix};
    $diff       = $siteinfo{Diff};
    $hicolor    = $siteinfo{Hicolor};
    $ignore     = $siteinfo{Ignore};
    $ignoreurl  = $siteinfo{IgnoreURL};
    $email      = $siteinfo{Email};
    $emailLink  = $siteinfo{EmailLink};
    $program    = $siteinfo{Program};
    $programdigest = $siteinfo{ProgramDigest};
    $proxy      = $siteinfo{Proxy};
    $proxyAuth  = $siteinfo{ProxyAuth};
    $randomwait = $siteinfo{Randomwait};
    $retry      = $siteinfo{Retry};
    $retrywait  = $siteinfo{Retrywait};
    $timeout    = $siteinfo{Timeout};
    $tmin       = $siteinfo{Tmin};
    $tmax       = $siteinfo{Tmax};
    $addsubject = $siteinfo{AddSubject};
    $digest     = $siteinfo{Digest};
    $useragent  = $siteinfo{UserAgent};
    $datefmt    = $siteinfo{DateFMT};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $outgoing     = "$pagebase.diff.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";
     $htmldiff = "$htmldiffbin $page_previous $page_current > $outgoing";

    if ($siteinfo{AsciiMarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = PrepareRequest($url);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);

        if ( ! $resp->is_success ) {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
            next;
        }

        # Leave if there is no refresh header
        if (!$resp->header("Refresh")) { last; }

        # Handle it if the refresh is for zero seconds
        ( $time, $refresh_url ) = split(/;/, $resp->header("Refresh"), 2);
        if ($time > 0) { last; }
        
        # Convert to absolute URL and refetch the page
        ( undef, $refresh_to ) = split(/=/, $refresh_url, 2);
        $newurl = URI->new_abs($refresh_to, $url)->as_string();

        $req = PrepareRequest($newurl);
        # Don't reset the counter, we still want to protect from endless loops
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{MailFrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{MailFrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        elsif ( $diff eq "htmldiff" )
        {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage using htmldiff...\n";
                $rc = system($htmldiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{MailFrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{EmailError} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{MailFrom} );
        }
        if ( $emailLink ne "" && $siteinfo{EmailError} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{MailFrom} );
            }
        }
    }

    return 0;
}

sub PrepareRequest() {
    my $url = shift (@_);

    $req = new HTTP::Request( 'GET', $url );

    my $auth = $siteinfo{Auth};
    if ( $auth ne "none" ) { $req->authorization_basic( split ( /:/, $auth, 2 ) ); }

    my $proxyAuth = $siteinfo{$proxyAuth};
    if ( $proxyAuth ne "none" ) { $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) ); }
    
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    
    my $compress_options = "identity";
    if ($feature_compress) { $compress_options = "gzip, $compress_options"; }
    $req->push_header("Accept-Encoding" => $compress_options);

    return $req;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        my $resp = $ua->request($req);
        die "Error mailing document: ".$resp->message()."\n" if $resp->is_error;
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=item B<--urllist>

Use another file for the url list, by default it is "url.list".

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 150
Prop-content-length: 120
Content-length: 120

K 7
svn:log
V 19
Commit old changes

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-11-22T08:43:06.838933Z
PROPS-END

Node-path: trunk/site/Makefile
Node-kind: file
Node-action: change
Text-content-length: 881
Text-content-md5: e06954c79950d1424e6297c38305e0b3
Content-length: 881

SRC=index.wml
STATIC_SRC=favicon.ico websec.css websec-homepage.old.html \
	   websec-homepage.new.html websec-homepage.highlighted.html \
	   websec-homepage.highlighted.png
OUT=$(SRC:%.wml=%.html)

all: files_changed run

run: index.html

index.html: index.wml tmpl/defs.wml Makefile
	wml -n -I . -W "2,-X1034" -D "WML_GEN_ISODATE=`date +%Y-%m-%d`" -o index.html index.wml
	validate index.html

files_changed:
	@ls -R ../files | cat - Makefile | md5sum > md5sum.new
	@if [ ! -e "md5sum.old" ]; then \
		echo "No old md5sum file, recompiling everything."; \
		make clean; \
	else \
		if [ "`cat md5sum.old`" != "`cat md5sum.new`" ]; then \
			echo "File list changed, recompiling!"; \
			make clean; \
		fi ;\
	fi
	@mv md5sum.new md5sum.old

clean:
	-rm -f $(OUT)

install:
	make -C source html install_html DEST=../../realsite/
	install -m 0644 $(OUT) $(STATIC_SRC) ../realsite/


Revision-number: 151
Prop-content-length: 124
Content-length: 124

K 7
svn:log
V 23
Ignore generated files

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-11-22T08:43:53.840690Z
PROPS-END

Node-path: trunk/websec
Node-kind: dir
Node-action: change
Prop-content-length: 59
Content-length: 59

K 10
svn:ignore
V 27
*.swp
*.[1-9]
*.html
*.tmp

PROPS-END


Revision-number: 152
Prop-content-length: 169
Content-length: 169

K 7
svn:log
V 68
Ignore case of commands, no reason to force users to a special case

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2003-11-22T08:49:56.118998Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 9409
Text-content-md5: d1dbf3507aec78ea4465d65d613d4ca8
Content-length: 9409

2.0.0 - Release on ...

    * Support for diffing with HTML::Diff from CPAN, this has support to show
      the removed text in addition to the changed text.

    * Special support for konqueror for Program directive.

    * Replace TABLE tags with SPAN tags to highlight differences, this is
      reported to work better for some users.

    * Accept keywords for url.list in all cases.
    
    * Use sendmail from /usr/sbin/ if it's no available in /usr/lib/, this
      fixes a problem with RedHat 9. (Thanks to Michael Wittman)

    * Follow refresh links, this is useful to pass automatic forwarding pages.
      (Thanks to Adam Stanley)

    * Allow using a URL list file other than url.list (Thanks to Sacha
      Fournier)

1.7.0 - Released on 31-May-2003

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

    * Handle Content-Encoding: gzip, but if the Compress::Gzip module is not
      available don't just die.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Node-path: trunk/websec/url.list
Node-kind: file
Node-action: change
Text-content-length: 9744
Text-content-md5: c4920c8c75b5942ed44b6f3532d9a122
Content-length: 9744

#############################################################################################
# Default configuration block.
#
# These configuration values will be used for the sites that follow, unless they are
# overridden by the site configuration values. Parameter names are case-sensitive.
#
# Auth       = "none"; or "username:password"
# Diff       = "webdiff" to detect and highlight differences,
#              "htmldiff" to use HTML::Diff from cpan
#              "none" to mail page as is.
# HiColor    = "blue"; "yellow"; "pink"; or "grey".
# Ignore     = list of comma-delimited section names containing ignore keywords.
# IgnoreURL  = list of comma-delimited section names containing ignore URLs.
# Tmin       = every token containing <= Tmin words will not be highlighted for differences.
# Tmax       = every token containing >= Tmax words will not be checked for ignore keywords.
# Proxy      = specify proxy "http://your.proxy.here:portnum" if you are behind one.
# MailFrom   = Address from which the mail will appear from.
# Email      = email address to send page to.
# EmailLink  = email address to send URL to.
# EmailError = 0 or 1, zero will disable emailing on errors and 1 will enable (default 1)
# Program    = application to call with the diff-file, special cases:
#              "mozilla", pages are opened in new tabs,
#              "konqueror", pages are opened using "kfmclient openURL"
# ProgramDigest = if specified "true", the application specified under "Program" is
#              invoked with a summary page that provides links to all changed pages.
#############################################################################################

Auth      = none
Diff      = webdiff
Hicolor   = blue
Ignore    = General,Date_Time
IgnoreURL = Adverts
Tmin      = 0
Tmax      = 99999
Email     = ${USER}@example.com
EmailLink = ${USER}@example.com
#MailFrom = ${USER}@example.com
EmailError = 1
AsciiMarker = 1

# enable this to open the webdiff-output in a new Mozilla Tab or specify a 
# different application to run with the changed page
#Program   = mozilla

##########################################################################################
# Sites to monitor
#
# Configuration values for each site is separated by a newline.
# Parameter names are case-sensitive.
# Parameter values override default values (for given site only) if specified.
# URL, Name and Prefix parameters must be specified for each site.
##########################################################################################

url = http://browserwatch.iworld.com/news.html 
Name = Browser Watch
Prefix = browse-watch

URL = http://www.freshmeat.net/
Name = Freshmeat
Prefix = freshmeat
Hicolor = grey

URL = http://www.javaworld.com/
Name = Java World
Prefix = java-world

URL = http://www.linuxgazette.com/
Name = Linux Gazette
Prefix = linux-gazette

URL = http://www.pcmag.com/
Name = PC Magazine
Prefix = pcmag

URL = http://www.pcweek.com/
Name = PC Week
Prefix = pcweek

URL = http://www.sun.com/sunworldonline/
Name = SunWorld Online
Prefix = sunworld-online

URL = http://www.winmag.com/
Name = Windows Magazine
Prefix = winmag

URL = http://www.linuxresources.com/
Name = Linux Resources
Prefix = linux-resources

URL = http://www.news.com/
Name = CNET News
Prefix = cnet-news

URL = http://www.slashdot.org/
Name = Slashdot.org
Prefix = slashdot
Ignore = Slashdot,General,Date_Time

URL = http://www.linuxworld.com/
Name = Linux World
Prefix = linux-world

URL = http://www.theregister.co.uk/
Name = The Register
Prefix = the-register

URL = http://headlines.yahoo.com/Full_Coverage/Tech/Linux/
Name = Yahoo Linux News
Prefix = yahoo-linux-news

URL = http://www.betanews.com/
Name = Betanews
Prefix = betanews
Hicolor = grey

URL = http://www.wired.com/news/
Name = Wired News
Prefix = wired-news

URL = http://www.nongnu.org/websec/
Name = Web Secretary
Prefix = websec-homepage
DateFMT = " - %Y-%m-%d"

URL = http://www.vim.org/
Name = VIM
Prefix = vim
Ignore = General,Date_Time,VIM

URL = http://www.joelonsoftware.com/
Name = Joel on Software
Prefix = joelonsoftware

URL = http://www.openbsd.org/cgi-bin/cvsweb/src/sys/sys/tree.h
Name = OpenBSD tree implementation
Prefix = openbsd-tree
Ignore = cvsweb

__END__

=head1 NAME

url.list - websec url monitoring configuration

=head1 DESCRIPTION

The URL list consists of one or more sections separated by newlines.

The following parameters (case-sensitive) are recognized in each section:

    URL        - URL of web page to monitor

    Auth       - Authentication information in "username:password" format. 
                 Put "none" if no authentication needed.

    Name       - Name of web site. Pages delivered to you will have the
                 following format: "Name - Date (Day)" eg. "PC Magazine - 4
                 Sep 98 (Fri)"

    Prefix     - Prefix of filenames for archive files of web pages created
                 by Web Secretary.

    Diff       - Put "none" if you want Web Secretary to always mail this
                 page to you instead of checking for and highlighting
                 changes in the page.  Put "webdiff" if you want Web
                 Secretary to check for changes.
                 Put "htmldiff" to use a different implementation of HTML
                 diff output. Note that you need to install the perl-modules
                 Algorithm::Diff and HTML::Diff which are availabe on 
                 http://www.cpan.org/ for this to work.

    Hicolor   -  Color used to highlight new or changed content. Currently,
                 four colors are defined. They are: blue, pink, yellow and
                 grey. You can also supply your own HTML color tag in the
                 form "#rrggbb".

    Ignore     - Comma-delimited List of section names containing ignore
                 keywords. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    IgnoreURL  - Comma-delimited List of section names containing ignore
                 URLs. There must be NO SPACES between delimiters and
                 section names. The ignore sections and keywords are stored
                 in a file called "ignore.list".

    AsciiMarker - If set to 1 it will add ascii markers around the changes so
	             that highlighting is noticeable in text mode too. Useful for
				 text MUAs users.

    Tmin       - Every token containing <= Tmin words will not be highlighted
                 for differences.

    Tmax       - Every token containing >= Tmax words will not be checked for
                 ignore keywords.

    Proxy      - Specify proxy "http://your.proxy.here:portnum" if you are
                 using one. (Alternatively, you can make use of the
                 "http_proxy" environment variable)

    ProxyAuth  - Specify proxy authentication in "username:password" format.
                 The code for this feature was contributed by Volker Stampa.

    MailFrom   - The E-Mail address to send mail from, this can be left empty
                 and the user used to run websec will be used.

    Email      - Comma-delimited list of email addresses to send highlighted
                 pages to.

    EmailLink  - Comma-delimited list of email addresses to send URL of
                 changed pages to.

    Program    - application to call with the diff-file, special cases:
                 "mozilla", pages are opened in new tabs,
                 "konqueror", pages are opened using "kfmclient openURL"

    ProgramDigest - If specified "true", websec does not open all changed pages 
                 separately with the application specified in "Program", but opens
                 a summary page that contains links to all changed pages. 

    Digest     - true|false or yes|no. This works only if EmailLink is
                 specified. It consolidates all the changed URLs and sends
                 them in one email.

    UserAgent  - The User-Agent that will be sent by the web client. This can
                 be used to bypass servers that prevent access based on the user
                 agent.

    DateFMT    - Date format to use in e-mail messages, can be empty for no date.
                 Set it to " - %Y-%m-%d" for ISO dates. This is perl format for dates.

    RandomWait - Websec waits for a random number of seconds between retries up
                 to the value specified by the RandomWait keyword. This is to
                 prevent websec from being blocked by websites that perform log
                 analysis to find time similarities between requests.

Any line which begins with a '#' is treated as comment and ignored.

If a section does not contain a URL entry, the values provided will be
treated as the default for the following sections.

For example,

    # Defaults
    Auth = none
    Diff = webdiff
    Hicolor = blue
    Ignore = General,Date_Time
    IgnoreURL = Adverts
    Tmin = 1
    Tmax = 10
    Proxy = http://proxy.nus.edu.sg:8080
    Email = vchew@post1.com

    # Web page to monitor which does not require authentication
    URL = http://browserwatch.iworld.com/news.html 
    Name = Browser Watch
    Prefix = browsewatch

    # New defaults with authentication information
    Auth = user:password

    # More web pages to monitor which requires authentication
    URL = http://www.infoworld.com
    Name = Infoworld
    Prefix = infoworld

    URL = http://developer.javasoft.com/
    Name = Java Developer Central
    Prefix = jdc


=head1 SEE ALSO

L<ignore.list(5)>


=head1 AUTHOR

Baruch Even <websec@ev-en.org> is maintaining this program.

=cut

vim:set filetype=websec:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 21926
Text-content-md5: 76d307d765075dd789e80402fc8e7cf0
Content-length: 21926

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
use URI;
eval { "load Compress::Zlib;" } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters

$urllist = "url.list";
GetOptions("urllist=s" => \$urllist);

if ( -e $urllist ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$page_current = "$base/retrieve.html";

# Red Hat has sendmail in /usr/sbin/sendmail, but LWP::Protocol::mailto
# expects it in /usr/lib/sendmail.  Revert to the Red Hat location if there
# is no executable at the expected location.
use vars qw($LWP::Protocol::mailto::SENDMAIL);
$LWP::Protocol::mailto::SENDMAIL = "/usr/sbin/sendmail"
  unless -x "/usr/lib/sendmail";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

$htmldiffbin = "$FindBin::Bin/htmldiff";
if ( !-e $htmldiffbin ) {
  $htmldiffbin = "./htmldiff";
  if ( !-e $htmldiffbin ) {
      $htmldiffbin = "htmldiff";
  }
}

# prepare digest
@digest = ();
@htmldigest = ();

# Set default values
local %defaults = (
    url        => "",
    auth       => "none",
    name       => "",
    prefix     => "",
    diff       => "webdiff",
    hicolor    => "blue",
    asciimarker => 0,
    ignore     => "none",
    ignoreurl  => "none",
    email      => "",
    emaillink  => "",
    emailerror => 1,
    program    => "",
    programdigest => "",
    proxy      => "",
    proxyauth  => "none",
    randomwait => 0,
    retry      => 3,
    retrywait  => 0,
    timeout    => 20,
    tmin       => 0,
    tmax       => 99999,
    addsubject => "",
    digest     => "false",
    useragent  => "WebSec/1.7",
    datefmt    => " - %d %B %Y (%a)",
    mailfrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/" . "$urllist" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{url} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{mailfrom} );
}

if (@htmldigest) {
    open( OUTPAGE, "> $base/index.html" ) or die "Cannot open $base/index.html: $!\n";
    print OUTPAGE "<HTML><BODY>The contents of the following URLs has changed:<P><P>";
    foreach (@htmldigest) { print OUTPAGE "$_<P>\n"; }
    print OUTPAGE "<P></BODY></HTML>";
    close(OUTPAGE);

    &ShowDocument( $program, $base."/index.html");
}


# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $keyword =~ tr/A-Z/a-z/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{url};
    $name       = $siteinfo{name};
    $prefix     = $siteinfo{prefix};
    $diff       = $siteinfo{diff};
    $hicolor    = $siteinfo{hicolor};
    $ignore     = $siteinfo{ignore};
    $ignoreurl  = $siteinfo{ignoreurl};
    $email      = $siteinfo{email};
    $emailLink  = $siteinfo{emaillink};
    $program    = $siteinfo{program};
    $programdigest = $siteinfo{programdigest};
    $proxy      = $siteinfo{proxy};
    $randomwait = $siteinfo{randomwait};
    $retry      = $siteinfo{retry};
    $retrywait  = $siteinfo{retrywait};
    $timeout    = $siteinfo{timeout};
    $tmin       = $siteinfo{tmin};
    $tmax       = $siteinfo{tmax};
    $addsubject = $siteinfo{addsubject};
    $digest     = $siteinfo{digest};
    $useragent  = $siteinfo{useragent};
    $datefmt    = $siteinfo{datefmt};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $outgoing     = "$pagebase.diff.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";
     $htmldiff = "$htmldiffbin $page_previous $page_current > $outgoing";

    if ($siteinfo{asciimarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = PrepareRequest($url);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);

        if ( ! $resp->is_success ) {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
            next;
        }

        # Leave if there is no refresh header
        if (!$resp->header("Refresh")) { last; }

        # Handle it if the refresh is for zero seconds
        ( $time, $refresh_url ) = split(/;/, $resp->header("Refresh"), 2);
        if ($time > 0) { last; }
        
        # Convert to absolute URL and refetch the page
        ( undef, $refresh_to ) = split(/=/, $refresh_url, 2);
        $newurl = URI->new_abs($refresh_to, $url)->as_string();

        $req = PrepareRequest($newurl);
        # Don't reset the counter, we still want to protect from endless loops
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{mailfrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{mailfrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        elsif ( $diff eq "htmldiff" )
        {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage using htmldiff...\n";
                $rc = system($htmldiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{mailfrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{emailerror} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{mailfrom} );
        }
        if ( $emailLink ne "" && $siteinfo{emailerror} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{mailfrom} );
            }
        }
    }

    return 0;
}

sub PrepareRequest() {
    my $url = shift (@_);

    $req = new HTTP::Request( 'GET', $url );

    my $auth = $siteinfo{auth};
    if ( $auth ne "none" ) { $req->authorization_basic( split ( /:/, $auth, 2 ) ); }

    my $proxyAuth = $siteinfo{proxyauth};
    if ( $proxyAuth ne "none" ) { $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) ); }
    
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    
    my $compress_options = "identity";
    if ($feature_compress) { $compress_options = "gzip, $compress_options"; }
    $req->push_header("Accept-Encoding" => $compress_options);

    return $req;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        my $resp = $ua->request($req);
        die "Error mailing document: ".$resp->message()."\n" if $resp->is_error;
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=item B<--urllist>

Use another file for the url list, by default it is "url.list".

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Node-path: trunk/websec/websec.vim
Node-kind: file
Node-action: change
Text-content-length: 1674
Text-content-md5: 1e81da3aa434e42de48e81379fe1115d
Content-length: 1674

" Vim syntax file
" Language: 	Web Secretary url.list & ignore.list
" Maintainer:	Baruch Even <websec@ev-en.org>
" URL:			With websec itself
" Last Change:	Tue May 13 07:41:52 IDT 2003

" Place this file as ~/.vim/syntax/websec.vim
"
" Then add the following lines to ~/.vimrc
"
" au BufNewFile,BufRead  url.list,ignore.list setf svn

" For version 5.x: Clear all syntax items
" For version 6.x: Quit when a syntax file was already loaded
if version < 600
	syntax clear
elseif exists("b:current_syntax")
	finish
endif

syn case ignore
syn keyword urllistCommand  Auth Diff Hicolor Ignore IgnoreURL Tmin Tmax ProxyAuth
syn keyword urllistCommand  Email EmailLink EmailError Program MailFrom Proxy con
syn keyword urllistCommand  Program URL Name Prefix AsciiMarker Digest
syn keyword urllistCommand  UserAgent DateFMT RandomWait

syn region	urllistString	start=+"+ skip=+\\\\\|\\"+ end=+"+ oneline
syn region	urllistString	start=+'+ skip=+\\\\\|\\'+ end=+'+ oneline

syn region  urllistEND    start="^\s*__END__" skip="." end="." contains=perlPOD

syn match	urllistComment	"^#.*"
syn match	urllistComment	"\s#.*"ms=s+1

" Define the default highlighting.
" For version 5.7 and earlier: only when not done already
" For version 5.8 and later: only when an item doesn't have highlighting yet
if version >= 508 || !exists("did_svn_syn_inits")
	if version < 508
		let did_svn_syn_inits = 1
		command -nargs=+ HiLink hi link <args>
	else
		command -nargs=+ HiLink hi def link <args>
	endif

    HiLink urllistComment	Comment
    HiLink urllistEND		Comment
    HiLink urllistString	String
	HiLink urllistCommand	Statement

	delcommand HiLink
endif

let b:current_syntax = "websec"


Revision-number: 153
Prop-content-length: 124
Content-length: 124

K 7
svn:log
V 23
Update source location

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2004-12-02T16:10:51.084764Z
PROPS-END

Node-path: trunk/site
Node-kind: dir
Node-action: change
Prop-content-length: 126
Content-length: 126

K 10
svn:ignore
V 22
index.html
md5sum.old

K 13
svn:externals
V 47
source svn://svn.ev-en.org/websec/trunk/websec

PROPS-END


Revision-number: 154
Prop-content-length: 186
Content-length: 186

K 7
svn:log
V 85
Prepare the site to be incorporated into my personal site (http://baruch.ev-en.org/)

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2004-12-02T16:23:30.357031Z
PROPS-END

Node-path: trunk/site/index.html.wml
Node-kind: file
Node-action: add
Node-copyfrom-rev: 152
Node-copyfrom-path: trunk/site/index.wml


Node-path: trunk/site/Makefile
Node-action: delete


Node-path: trunk/site/websec.css
Node-action: delete


Node-path: trunk/site/index.wml
Node-action: delete


Revision-number: 155
Prop-content-length: 136
Content-length: 136

K 7
svn:log
V 35
Add release binaries to repository

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2004-12-02T16:30:36.456540Z
PROPS-END

Node-path: trunk/site/websec-1.7.0.tar.gz
Node-kind: file
Node-action: add
Prop-content-length: 59
Text-content-length: 32568
Text-content-md5: 63d5062d9dcede7ab4b1a9c75451f843
Content-length: 32627

K 13
svn:mime-type
V 24
application/octet-stream
PROPS-END
 )> [{wGO ,[`$EkI\	=0j^Y?UuqWWUrI2fi6w{{Us'VKx$c!q6>i}9Vdw#v{y_pdi;=o)f2JG:['2&k ;vDK{_1LG\LiiKqact" 2@=h5^i*4y-c^H"{"Ee1XH1r*D
hH{Wp3<lE*R{bL$iq<0N]JP$AI
$MHN,w $bRyjJ ,C@76$$;c@Th')#r <A 
x%X",p'Xtw{];rS!3[	i)`uR1(XNXEHz|/[fd.x[niMa:%U-@p>U_*[B;KmnZ\((iuP?F8 RFj"W}A7+bfo+0o++9t2((l+t":JJ/U80w`GL Qqo92zSR
q	Y)b0I/y'd J$#R7Q1%>9Txxzo@ _aMS1	3X9a(('2Ihe*(K	
P+@'0fg'e7GQ(-UbOE0A,M5O`#oy),aoa/u|#s3	cV%X
H9KpdKK9^L l=]-*~b=qph18=8U5[UO$;V1 QyH!~SdL
3$3Uq&hNs^kaa>	}%T^OoZY_|[5^lX`38?~.~{#M<W4f.i<.CO;+RcE*T Pch2~U8 t%cIz\J?56m^_cffKw5 R0@3I=Z ej(Nx~E;`jYOS/9<hUK!M(31 /WL"C`5y*+g[*VH`sv44fWq$66.+;^)$|$]-Ee$:#.2eM^ty@zg$a^WFEkG_XO(:$_b-Ou3n/Wayx-zb2''m!Bt}rp"VSow,f_WVvC
e}>{968qP
1k9F4%p~Szau.*4E>^$N\23bPt%=HqTG!O&0Qsx5~Sw^Qy05g#.3/*=&6:FdPXm#+aVH)*"iczJ%!xd_96x$r0GQ)!zt"R|sw)s#mtrAnNfJ$3T=zSm33zaxQm:;dz6R3=D+z(t1EHi/)0>8; ^x"G_RHwC:3Px|%_	[rL4AKdWS7a |$s(R^i'Gh^!	-8HyZn0'K	Vca)7p`Y<	5!N"9qK8dvBbYo$b,fd$2MaH`mc=)` *MLdWU\22E&!MT
AAiHtc/(lNv"fX-
IX)\mL&=jmdIHp9<_Y^Y w:UmVU^\nvEw.k3X3x=,UD2rjGpw0ba<#{9rdsG=sMj<ts_tO4wgw?S]_$Kw%
W[qzWV|"F[Ew\iw.5zf%)O][LRX;{-;
  9cFSHP)QB
QoNCyQ
xQ	xMkx q5zQTDP0gEP0Fe|BQD%G[$vW}.40% =VJ$(Rg27qS@{iU)8F'NI]8G31?4(9_z;cYlR	vOb56"~,$=Rz5.|e/Gv*OB<x mIK"qm*Fh #$)o(Q8k$"_
KQySRJfo
!`!#|a#q 0H MLgD&Y;\Q:A"pi`*@PaLOG4LP5w/8MN2ck Vz,09]r97950X?FE~ :n!v^[rpR58SRNgGO}s.Rq)5	a4j78\04a!OuUkSH5DId`
r-RE[I  
K	pMMCMGai]ZrGhb !<)xjT+Tu2EN0Q,{B0BuXCWvF&4	2ky#m	]BH<ac0TeEx^PsYV=UT$Wtb^~k!
]`ATnF#-HxAr 3OTqFw&:#r(+iE G>8mGvJO6SOk|\bj,rRUO1(UMDq,{Gy.%*RKgZmS=:WTTCY!I3-MtB
:+9$RfG"*r&0~;{,g@ T/;C+'_p8#p+/d3
&	{-4aZ5qTGR4aUQQB,AH!/M)y8ZSy'I0I=uEn~0*||+"n5K%vjjx zr!n&#1'71?LZEPMDmA1D+T5C^]+9N7/F@<37[4bC='ocnv][rlJqf'f+e.jTPWt(%~dcv;Ov!GbK?^cdun#{r7y{+CrT\[t$ Yl,]|yuI4wR"0ZnUXOu~w-];g>0$c)W'xcB
~DU4as*DdDFU/#{=w"^ i{D.6d>=!
$J	*Y-Dc	l0[XT|twFR?0{ ;V(<1BXSpaI#qv|=7<?~r^\ebpLu%)1R#SZ3+H@XNK>E2_\Z1,|AeH7"bky41z-ON{Wu*R {x@:B4[e&AWo|/t6ozBg2MRJsrR5^9>P-c/jSWCD:.@RZ"ThMXCh&7wlVR'*W
SVWsJ3+m`_"Sz/@Ak)'dbp/t0):une-hi Mw1(Ij -"hI4by>~HA
h $Cw>t/+*icvIc R9
VFRiVOrECo{w!&D\! 7d4u[Vg?h&}vc[uuu]kl8&pi`6$4+!W3~x2&y;Mq$aS,=qhp#;[:	X<OBEV,0G;T3_`@I5E\[>Sa'XtDV3zx@{0sa8xy+h~a*8+jmp>?!Bff1_N	JNw%An]tNOjG_yjx7CcNBK
<trv:T+1tO1-&R,Ch?L"y-Tj-OK-^GG[kGloKy\:|OS_kviKJQWM6Y>{RGU2//b&S6zi[8({-GKL8Ih G,_cH+fltLj'@yt>tD715**=Y7.ol`UkRn""}X^m^ K+ULU}z&gS]MiZ;="RiB0d6~>~iTA*GAG0%MoMN"n!?.H\m$Kw{#NwT 9(?q!t:f/p7b7aP4 RtvWd<Ru7QPSbB'uKT\X Y^J+REYOsf;dtLX{@@U2,3#_}qLT:;`?7j>w{ZG\sO_o;@[68Qa<KO)1}n`pq3Geti)`tFxM@@<(W>n|DI!6o3y='+v#&x{MCSan y.3+S#o,Go&19=vFI+$AqnU=:4z 3g}:yr:11ZJ#J9<w3GWhQ*IAloz,x!Jx~{1by}6^qrf/*<"	(<>ZU1*NN{w9xgu5|]9vrwp	uJo'LwCFpAd!|@W~v$ Sa7xo!eo 6L mu>#v{mzp/p@' q(M+PORH1fktuM&@z7n+g\Q:f^%{475xa472NTjvQ(:[n2m,in{^~C&ZGHqBWbaVq\n*k ]%Ju^.;zjz=9x
lV{:_OEp7 |ERU*8ZZ];/o>-{XP[=fj}~MES{\mxo7c9D5p8fW\!W:%Mf.qItsbr#dJ}##'e?au"J#wG;r;iy$FIk7Wap| `Wn<Jff=AGU>>1O*4:*YnG9Yuhb$4B)#	9s+g6Km ."f41CdL9ag`ECSS#MQ[D$p!SrV.x(o8h=["5T[,&87{i-7rspx~'t]$Xr]7^JtRtGI_D.;[{5i!
5DlyO6W?}/+OwV\+$e{ue"&u{Sqz]E*L P_$fg	(h
2Tt!;GP>/un<)~g^j\XP-z+Zzs2)IU=e!rp3Dt 0o5kd~brl!nG+'n=a4;q*jdMQ?&yK.Rekli^b=V< 'RXkOo3V|SU5TU,$!9K	,Z)MK?	BwD3Y5@-6|0PZ-r!(|_6(_2Ot[w`x8Rw(xEYs(Z kI<F0|v	C$TZ,ea@:I\$4Ip-{u6Z;?cxaV3k#m.radKt2
1G].cuec;zZ@g]u	8%Gt:?@AhS#>m |V	[O`s*jQ`>Wjja6Z(ZB<F	%s{so:dUs\hao]^JJRjl:AR$frWsbW%6zbp <A4`VN{}[b6tkruA):hL\^9~Av&D"Hv,qxp<?J)W>|S^*GH:{H{%c1YL'KA	"qp9\Jb2k:LIYW%="X#GI%rh!gADc>RKs_(sb`@\	J;8d,H*?6;OI 6d6 %Hj?8??k[dwjyk>(%2ppqzC{WPM@0<Cv,a"w8'Zxop&\E,pY(I =)2c1z!4 9-,!n2A%'K{,eWw1e*=VGqSd5OX@J<e$ARPJ8tuGREg:eG(	
:@`aE

bXy2k3(h7w2Z4p/-etZW"Z-O%l(yjMK{PSeh(MG6Rj(4 s(4ia*sTN30b33.$dE;s}_c	sUR/hop:`k?_~	R  @|^>M8&rsu}J7+sO9n+k!/_x1S5h6MO^lEzw~{*aiK:?ZDu|,C.>Zr<^u	seB4"\0z~Eg: LUQUd~4yTR6)B51		-B2n)AqG;{em<U;Wc5?/b8\IQ89$	zcl!Xds._RP!6XG'Lz F1YIB1!J
z9huo4vuCDtxz8
'b^`8XXxt&8^^&VuaD5$K(Y$%.vApn-\I]QY54G ZHXg
dfZl,mRT&Uo~g#4Q|7 l[OW1dPfPx8# Fp*ek`J}wkTYk|ScgBW+}jJ9,cp7M*<U~9 fx/oj{~}NRRxmkfnnp<dCEd<TJu:o*y{Jb627De70:9[c{C0lda[5Q`h1;GfF}}h,"{7S}"+MKq+#824R?8ASd^p"6B]]
4E2\&sL{Bc)eipdfWRf2lk3XOHmAqr2>z ^)zx!"Ewe.<6Ua-0~Os#TcyZ2X2Ll
Rmzx{%e	e!==o,b9*4$NoG0BSC!+3saGE20ngf?7Kf=i5n,j?IY<$s,-:Hcc#G3=C_)s"h-W1WF
{
88=	9V40m:fzhyIMV33kAX5nZ;\/9ZVFkNx.w]3g Wb&SNR8CNk9RdaH_+h]&/ueGZ+;=)/9(gw}CI~XV{$[N8:^H&ruTM23M@AY+!z$i"|0l\zs,\
k9FV+,.!B)n%s(*X'[%#`^\_QuUZD4IL`@Yry)lIP=/DE1?!)c*X[O7"Ki
\}p='r]$k9ew^=AYru268s 58B}n'.fzrVGB#Fvcc%B4]DlJ9cy`;*L\]g%.ES t/{It1pqzNU4q[1v1z#*gcenD ZV95k\.C\SX~a	JI:[8,=V1XGztjx\``j rars, m<!6qk?6/g[}6_|f8t	*@9\	WJ
%)zfGa-@5YL'e0y&EE_7xX}p^Y;hqF>},p>\0jvS|C)g$8(>28Eg6#x%0gPNv
{0P	t=rW W+T/U<yXuww7h78Yf	p2%
sbsJFr^vnklwf6OGDA^ r>lSIfZtMbK[X_R&",<Vz/;Rjf\M76y;sh qCCYwN^2|~#s^x*-n(pQHqTl}ohaX` vQl^jZTq	GBQ4N2nX* *hs^H|EP2*R'1oEUd5rnwl#_=1?N4pp oNUw;zOY_.k v~zZP
)D-u]
cO_H-c
Lt21c_d0v)I!c#dz`Jvr:qrFfmNoi=IKB<E8zQ56#<[*EA|rz 7`DVXXcqx\AHY9f1x
[@t
uG(b;,WPHjN<rx@UWyXN{=\|tBO|Ie%\
 k5kwCPQ4CooY)vH599p7\Od@5;M4/ joPzGHC iP!G#}M2/-xlPU83[BArrZSO
!Q3pN_bIv$[yD yp\U ijZ8JJGQ+ol +ATJ\gA4RYa{N-/%otbpYj1NVn[DA`wW,jB?$S}	\'4h6TiV:L7 4:2"l$haT\1%l^5^]h7"b )	 %XnLX+^zwFj*i_['XZ}iCclHWuM f:$Yb
TegyP@%;H3g6@"$C^Mt^ZL	t{tN~k5~bV^lHuHWtumop|>En|D:=Z7Qo\`%Zxv:|3>((mll/n7C{
 }^cn~]X|ok]-T6"@}=!$xO&P1	 ^~Sgxg0'z&kK2'7N,e;+e4B|7LG%X(sXbz# gBC4B|ZqUSx>jToskJG
8`iC>E8;8<?Fpx/9*3;9dH3%G]*"vT[uxNN`d&K%tgik aslGo1X/Uc5O'0},=M%0Kln;ewg
Em7! q'fgc21ag1T@cUE6Ru0@,y,HdEXX,95dni^Pv.1fJ3xKW&Ox[]H8P:}+XqlH:4
.#1Jer{dpE`=:6O3]@D!WZ]
3\Uy&zv @Dl$wmue\7TejQC/0laTo&qBx&Cq`{8.]NQ"PIYY^>W4N&bR 7G^_a	eo{A*|.f(('U0e^.	)e,EYF23kO|qb\R .rs=JmpDOm"Y2AVM%U<cPqG(bJ)05Q6g$nON}	]i)mxDLwpMOclu%r-2WmR~eHGClCu5`1l;'}'B5Er6(Nz Kop-LoZ;Vf!U5j+.~qqz/`1[&}ajpj W$8s>pI}@Ob?ZP4:V^eKL["9F"y}>1FRFZ#(sSC4N.ht]1
q'z
\;C`>I?X9eK(#(]O^LH8H&qQQ(F ~!P},XH,seItEt*i*mr:i@C@|	&NI )7heLF	2bbYe!eA+u$
`+E+c9	"j
f),\m$>~\)%8ZNKO4$<v/GEI_Gx-P.j9Tr`'),Y8a~1DVt(y1Djgi _uMZoP0}gnifYqCQPLlP6*1s/]Hz
&,!T_PE\#1DF9*j.h 1-O,[Qa@({1Ue"K?N	`W[ sI3el<q)k,@]XX_#D8x7&2PXC0GS1k58;kRR3D8;L%p_Ra	N'V(4=]:i|1T4O(K_1>)^ O<m~eY] *N`5~XD^.0z98?_vE_h"Cq^9F*t"
fYT8H /RAfB2fC]9`
qFne"T`&_M=2wCxVPf%N($E`!:CKs7agS4AxxT8tV$wDii[!!G`4BbT/f HlmCK`K2T(8Dyf
'e$,X1LO<q|9mi";:;SpliI=p1A14pgE@J|qpM&c NwAaSi!+@cnA">(]tvY2`#
Hw^{CLP	?wB6qK)<\=rgi:	qMJ\"&DfyWB?mt86`&)r^>"SO;CpsGf%{Z?zbNPpxu:m0n1Mard#~bHt YYQ 1AU!Wj0|f3F27~b}!U<q>Vg"g'|5X<2X"opaxwVO`?Z0d(,=z)1) vs#F>uZ5Xk9X;'2?uDE$bVrBP6{3d34zKF3/zNLqWJnv=?$Ja'1108ZqBH48s~s.^3nD4tv3[]XSCqrz,SW+.BLst7 Tq;9>a4se!z8	kCAEGxc@xMfU3smSP4gg/c[v [CFhD1gZS+N>n33*ViPVff'q	9MGyA>t<HA"aW 4YcJzC=VL<y8S,h~q+i6	aS$Bf
>4:DdV<FphLB2v7cuxTh88hf0|$Kgwm3W;X@(i:jnL18tg6Uy$>7;ON8a	r\Z^#ppB7}2+2vNx_VB!{E;P8v,H
u3k!83QIm$8ju](l+U5#F/	4GM%z
~/c=:;^5&J:Try:*cpw0'T_
eS k"Av'pIt;X'UcGFn' s=cY0)Ttq8qbST &9MiV<2" wjAIcaG+mrM2?=Bu_?Tn<W=(DAg0?`b.{LZoF4gtM`Z!'lpV*Dlo4"SX'ze&Iseit1-+EyJ9}={2Kl<9z6cg&2
w	}%j BIO1<DrU(B,4m43+L|_=U@O:SDpp8l)88<eezh|jIAD0;W|5$r<`E0:CZ["Sm^/}s]v,wFTz>;e9(az 9U;te)Z9A>3W?Gb~b%I(|0Ox5>I:V gK$4pZI 'i%j:#|nky9EC^s+I=	q(_Ho'3B;2gKW4k StvKH@}5!zF/t..BlRhqq>]7B:6wq$D9th11CPL.FUx2Y0]-ac-VF+Ln&P6EJn_{:JlnLTk{z(q!QAZ2#nZPM 6!6=<%5AM	qsnlpzd]$Tzuz@3iHKUw#hxq 5P#1!1j Tq X49%onzMW'CLy46`^amV2;8TLr>jC5Gd*qj&.\8J>%Q7b<&{-|&gNOoF %#z*gBOz3gS7Vx=?	*v+|vj{[U;h6 *[?vN=)'O[knThG/{[mP\QVvPCv6cojj3C@;lnnbCawFF<1eta9||<&}p0.-${{S 
ZZGWT9|><	w[&!8hoia#}p ({B[VX<+m?/i< 	08V(UB/Dhl$?)zP7v+)vn<<[LhkcgyX$6B	heVhK"=P -'<%b{Fmc]/N/hkjPo_fk/O<9h
q.AJq l?6_]SimljH;Byd4>._`y!fR|	']bvKz2v`Gt8+7(=6?I@wMS{No2@2s{9qJ&A,3:e\AGN&kdy(bStvf>Mt,.8pTgECnah/Yv$(R8Mq:{7gY XxD4TZ1guw>,z%{>f[\~rRlyUy6FkMrnVnv0RHWzXglX.T|sPE!w{l/h:)M{C0f,	{k/D~[VCXjgFYX#<%_ s .]M/%xj$szf52UCOIy[{^l}L4fV*3WAS=}!mV"8/mb<L!DzTw"O'AC;'ClqDG6owDF|qlmE_tq,\!VE!=p O|EcT5uc!=o8	dC=x2~<j |N(u:`.\e8jen-v?sO*Kq{c 3M9#9OYG0=XT\-~ 6[6$AC(vqtcl.H1o4T#+O/[E'XhF;m8XMbIcUH0H7?`#`ma(~v<zJXL`L9T_*<JOt}[D3&)7#!
Bxtw33.@rtX?Ocb%@o=ZiyOW?){Am&r40wnV>00._/mSl+yV\_Aj pCS^K0oxz}quo"
w Dl4k;@3_t3)09&U3C|.c~.BhgXr< 1#rZI NYQ]Cb|b@d )b4o1:.TBN=x""YMG
mM s	Dg{oNj=exWq!v%jLLuLb>\Yu9(MO#X4ZmF";[aKY#]%9?-Ep`8#HY}fn%cJNWR_,m8 d*	;+,yo4q"lJx]5q/VgV p"7Di{-0
VC'z
gZni'h{$=v]N$Ry7AksjY#Vn.vBMjCD4Ter!'O4f',d'/m-RJ1&vL)5gqe,|^}R2LFN/-?@0rrv R1ui1kR \dn#5bVCAHdH^~jkTuQhlIk_m4P6zR	U]Zm^u:-3#.SS\3S0+r\|HY,`i/>Cd!	:`+Stba-Lc-n9_#- /gv'mV2d@"|l7,_&6KOeiv{qfZ!YN4$F2|Gen3kT4b?mL(d^#p<$.aJ8Q,7di 1q`Yq2V5k
N=B97V:>Dv%7BVi{X,2!4grLGOlOMq(m]><^R)F8Ovk)OtOdvBeM3b<3/=,a%]P2s0,lW7f^BCXDCDmBwIZ,7O0{.ydX8 }
7?oQ$01/X[T!%X1h+fo2$vM7<GiU'}l6[/W51VAnWCDmV%lPptBFVM.TF uC9HN&t$8eS5PsS0KKEE]}K.>Tz,7+yz|3CP~0?<?FWK-*?&m6i3v7~"eM1S/)O&7;Xh;D/KhKV3(XMjy2|3>8xwjJ+:hB9RTLp;
=%g	 Myq_%Ch*9cSL\lOe_F-H&B->qK3a7vcSz
L	]5l! VisX@|Oz5-7N0h4iOq4vNjhCy<vg8<Pv,abI VMW b%8p "wk8]^;.#>'(uC=%Q&b`}~YgDAZHGJObcg-yh%[zab@qQ]*/PS3-1]l:?C
	/M,
d\|i;VF}K`>U^<&t]e[,~k@2jOd8gVHL**+u	b4dWa<u*V/"  
8"&u(#k`)5t9G

a)_1j.t;9[F!;-0zG+GtWt0ez#"K2`9s_cT,Cq6&NL]|]DrXs5C/cR?k?7';yMQB&^>h3,\m4.//.9SG^WmLDcaBbbq?RDflRujcp{F	I!Ob';$X&#xr+1 OYqwr0	p\n17PD]w0JCp/.p
i36{FC>u8fTR[D9v#V7n{Avx14e}T-8RvI<G8ldBqr6W)Y*<;{ZA;W@QtVg*f{3|5O ql&}~^X ~D#jbr|lSOE1^f<&XEd&$bJ7Ynw([Ed7n|N<zJWFPMm!r|46RvcegP==yYx4Rms;ed_#~8N PINOFU;[r}pX%XO5}M[YHbajDp.=a6H+	'P}9H+-rFpZ^naj<#8~az44L;YD
qF+exoI,&#3H@FMs/z2=;3P%a\a685@`F%xhmqksl8=-7BL(` 9G,^iAYMY'3?F1u6&4Wyoypgg|>j#{jJ99#m=@#{9pSAkYM/}ljj}{ad+wPtRz$HO7DJqFUUq*.I,;[ e"JL^p\/t0m	>aPOJo#8P-wZwj
%	j"]Z$f ' <XkfKmdMCOdw__b(@T"W&KENW"y*idy	1c74{T.0VzF[uK-,Vrk2YC40M)%aQ4+2s@7~yXhX8qUA?NF[3nUAr,,$[	r%,~6DcVGLbf><71P;_i(:L62	;36gsF\%^134Wv>6N)6NJ2{1w+-iyMKn* *6{<U]r?ap4h6BK9@6I?KL?xkS)F@b5)Q~Mx0-hwg0]D&~k<v@QEL
mBQe y7VM*_=.TNIyt8{4kR e25P*M
/kZa*yo[ *#kAPhzQzN+7Q5\uY0yVk!oLA5S!gX>CifQ9op'm+?v<;&_0'1B>hyr-)DXhK<==.cjjjAJIKcLw~S~s7u3KasUEz8{0LHp1S *}%d\4F:4YMq\?7Q[0dN<l
Bn;inykf
k[3fj0
	Vp/9^<)AyTP'{q;Yci[9YKGwg	@Mak?IO8UZ^j0c=dyU~=pS*\K&hV)Y=,2Zpq0x6N*%"Y  i/dAL[F|[E6(]=g.!VY>+^Mm2M!3>&Vhh${=ip7$fj7a5c(GzjpMpQ+HB.SENx-o 2pva>erYU0)d7j	jjUsf)+9{76[Vd<LrQmPQvQvN[j_><aFKbk%s;3H4aamO+nloX/:w^n&QHX@s&x	k
[8[7\H.<EvA<p0s/7
w01}YkU)>'IOa1f;f;mho<<-26v]FoHDn-O[~Q @%@1
(YBPD'6'>hn?z%i"STqnv&'rd_I;5b5U[)D&2v(H` Z@e=Wif|co;KxUNM87w8Mkg:R~$6kZzz@2\_5"{@(GX Ax=uuy2,$SV ['\YEKz4zT ld/>=nLdSW'Y2LKrq1j\5bD@N`R)^;+/i?l2Z"e\2~JN~j"<Z,]1NW@3#?"1`e'i=N.{ZxA\a:i:{|uY9HQP/CHU/G.A@,a#e\^;bH?y-t.A 1<p2MmN`QV}O@Qexyb 8AD(E-@ .NUv\0X'}OCljo<{7w_g=z~fOMM"RpanON3FN9faP3xs\ &gz-N%RoP	I &GAPC"i)zL_"9r2v,>E:ASpc
~/q+xV>Y|RDr{1^~t2W0'1K^gRf/b[q'^mm}=
Z70?<2_7jo,*KP45J9a$J.l$uw8=qPSS;?w|eGWCNL{	~%yS;;k_64wn9d0%BuBVj @QO(|w `c"=1#?D/.q/7XA=p0C&i^X'mcheP~x=lpM>yl(.2Vp{(I T5u94:zDC8,{~C	rU3K
Q]=d[9]T;hUqcx}d"oG3"WDW;8Y
sy(A[VLYt2nScd`5,$MGI$5 u8 k0=i&,c:. [G>5Fg!-0L$H`fXLtALX9N^/b"]zj s.>$I&	+\vJyVm*?{[lrd5H9,L8JSP;t{@uK=IP Y ":XL42l.#H6R@cXj{yxxVrh\MK<gZc17$nL|17{B *G'LV'Zj+F$)pmslD<:vxbpD eVCKPEDb
>f4e*Z+s#:	nB%weh6h}C}fM~6/>6|rMNg|^<hm D
[1
t%]N5F6!fCNLLWhX0#Dr@Z4(cgbzs)?cHRAL%1,TsrWs7NQ4M{Em$lm}V4wi<@CTJan}i,g&8@*L']DqBCbuG,@~t\]l+x&!pgDx5=ag"_lj34Q:T(;1_wtQ2tc!3v95u0Q6pi%|xhRWMDOAmu5)_9tZt9K64}T=7Ia/NzG6M$!@{K0VTrY0@QhTDZh3S	8^lI65u2!`z P`'s;@xhc{{6Y9r-O;Xh55b3PG65t4R	LE&3C'}S"h9F6[d":\XZgOS+h>?W[6ZYg.}}\$W}(	G6a r,@ssE}7.bA&6hD[8 sahs ^do2vpqZ%Y%zy9S&/&0dgMs;BN,h{'	PE ]60QeWD<QNLP:=|knfavC)0/6}bV269}hrx	5ltq	GD[GR$&s^ oO/*4|$,bZ $aKp259|w^nT 1v&by
|$VT8r0/ ;\wgPh|8UBW9"g$0%a%6{;H	 faIB6|
5CXzadE
g26C&;|4<5 j=n9x)X'	-'
92CHlu6	|th$4 @	Boqf$*aDx8bP
ByF[fx9UFE1'q6eb;-09w&F5H?r4IW(N2R('v&3h#BN2%axf.A]#LS24GI38nv?1xtY5rpX#+M20[KB'3"g>v8T'Mjc8%="-	K-^hzFs?"`i1~'y@7{`=vpuu$r5%0ro_)-6<h~3*~'>gtFV`~yV_BI2v6f BM)%+xGs[{W;aK3nF(x9N_$}u!``9T<_C-yqc.mk&/ xTU[b)5
U`?>v,J<ouu$PPk =h7U(hUzl'GBp.t:M#4E9p,\([\0"["XR:{AF]u*h?W6o+uu^X!^_vcUI-SWC7
#%'!K~@1\QXf`\K(t$5;&'Gcg<{^~s("^	rOJ7IVML6>>Ovk;9O!VOY<fpGG}[.C4=1'W	'ap6(O7Og.`F 
6HVYa10|4R1`QXte`:phC>$|M9RJ-	B==M  ^r%E.,c4$:\A!N*Fo%jWQp_o\h,bgg.'DEciaPp-@;LU f4J`Ori"gsSdrr'\W0UByr*qDD6dVn2_~\> Gn_,ZstV_{KvaoD0hI}@K7JfxPX{8|QnVKXwt^AR:DBpWVYp+>8F(U*__ %Z: Ml?oZZ2\E,4X:X%0Pia .en<4=nL)^T>?3z{TjUj@O'TioeH0l$Ig
v3.;&%}b	9m,yU&X)1B8<\41nassie,QPQd~L34gEc	>G)|~	DO$f?=vX|OQ6>N8P|>TH?pL1C,p;yx'l\FO]$XL.|3*w?-\nNkc?~2T_uLk_dB!&7?Kpry/m4kD,.NoV%+^I%:Fi*NR
h:jh\3_i.X%FSoV}o=:\}TwBRa/~;kK*tn*Y?01n#O\mq|F{Ig[p*of	hMt_5?4eN _4T\4(Fpd`_u4-XC5-DT!#c6pZa4W/G/^b\xXhX3k0a71H6lL%E+Bz8jijw(&{8g10`t2rl'X'0V%wq&LD(8Q[`m
K[OMzz"?&qrxTL`0Nu[z33@_%Nudx,hi{8
12|QI^@9iwo}(=!i>ZgvFt,r6Pj]N~72m)FOX0]Hmq.-\@WPY:WC{_.2]%$B,Y00uP[bS\RJ $	[g at_ X*:/iC
:1n*v;]!@nL{ s{l@B6(swW7kN>q	>'2y^YyqFNbN/!2-Ykl	%7I,L%; bb>H9bow{YfJRS4=4	LIXwnNj:]KGWU?6|?l>|w;N'w;N'ZV`><
U>Yq{z'l]otl_`o*uWm%hMSyzn#WA>!Wlvd_;`{;
_p>&XR\hnn+%X!}lR`c`Er+y
La>!=Vd-OLVJ_5C^FI'+"xf3-|>5YJR?"WYd++>o)yYA>%}YQ_o/YAg?,YQn-VD-l-[6%@+ MV4[v(>8+ZhE#4iE'&J+*o7sZ!o}xLVL|Z:B	LF^5.'c2$kV%\rk+]o"Y{!c_k|
m)CryB|V`#:}`fm+miprS47IihA:\HX>YD}oix^GM}}-k'	3&GjG9}C@v bp2	Tn\<&I2_"t7O33Ogm6=Kp%qv4CdR9LI_|_KwyQ,DK-xmq/N Z3MRB^XBz*6;-J\OSt7ZXiETsx)K-TGNt/}JHFUA:jWzqBj"RRmz(9uIz$6/Uh'++W5)y
D4<kp$}Qme.Z,jZOI}s-qiS}>w(  

Node-path: trunk/site/websec-1.7.0.tar.gz.sig
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 189
Text-content-md5: 1f315050d43dea98e58fa39eb69c77c2
Content-length: 199

PROPS-END
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.2 (GNU/Linux)

iD8DBQA+2KUtHCar6qtHRZgRAqPKAJ9Jz/+nPpA88NgtGgao1Q7iCOl4nwCfZJPW
OapFxHOriyc35iSUib2jXuI=
=4r7j
-----END PGP SIGNATURE-----


Revision-number: 156
Prop-content-length: 152
Content-length: 152

K 7
svn:log
V 51
Update webpage to incorporate into the site design

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2004-12-02T16:42:40.218813Z
PROPS-END

Node-path: trunk/site/index.html.wml
Node-kind: file
Node-action: change
Text-content-length: 6960
Text-content-md5: 592f609ec4dff70f4af556bf9aacbfb8
Content-length: 6960

#use wml::templates::be-tmpl
<subject "WebSec - A Web Secretary"/>

#<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
#<:listversions():>
<li><a href="websec-1.7.0.tar.gz">1.7.0</a> - <a href="websec-1.7.0.tar.gz.sig">GPG signature</a></ul>
</ul>

<p>Contacts:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://savannah.nongnu.org/mail/?group=websec">mailing list</a></li>
 <li>Baruch Even (Author): websec(AT)ev-en.org</li>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
 <li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the configuration file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dial-up, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</topic>

<topic "Links">
<dl>
  <dt><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></dt>
  <dd>
    The website of the former developer, frozen from the 1.3.4 days.	 
  </dd>
  
  <dt><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></dt>
  <dd>
	The project page at <href url="http://savannah.gnu.org/" name=Savannah/>,
	they provide us with numerous services and hosting, <strong>Thanks Folks!</strong>.
  </dd>

  <dt><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></dt>
  <dd>
    Old releases are moved to a secondary place so they will be out of the way. They are kept
	for historical fun.
  </dd>

  <dt><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></dt>
  <dd>
    Development is done in a local Subversions Repository since it's easier than having constant online
	connection, so I'm providing the repository dump online for backup and for others benefit.
  </dd>
</dl>
</topic>

<topic '"Competitors"'>
<p>
  Maybe you are not happy with Web Secretary (please tell me why!), or maybe you want to look at
  other options, it's a Free-Software world(!) after-all. Following is a list of programs I've found
  that are somewhat related to what WebSec does.
</p>

<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
    Checks only timestamps and not actual page changes. It's main advantages
    is being written in Python.
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
   <br/>
   There appears to be two wrep's, from two different authors, with totally
   different methods of work.
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd>
   A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.
 </dd>
</dl>
</topic>


Revision-number: 157
Prop-content-length: 130
Content-length: 130

K 7
svn:log
V 29
End tag for li was incorrect

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2004-12-02T17:05:00.617090Z
PROPS-END

Node-path: trunk/site/index.html.wml
Node-kind: file
Node-action: change
Text-content-length: 6960
Text-content-md5: 9792997ba75cbe2a9c9179c6ef24ea30
Content-length: 6960

#use wml::templates::be-tmpl
<subject "WebSec - A Web Secretary"/>

#<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
#<:listversions():>
<li><a href="websec-1.7.0.tar.gz">1.7.0</a> - <a href="websec-1.7.0.tar.gz.sig">GPG signature</a></li>
</ul>

<p>Contacts:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://savannah.nongnu.org/mail/?group=websec">mailing list</a></li>
 <li>Baruch Even (Author): websec(AT)ev-en.org</li>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
 <li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the configuration file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dial-up, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</topic>

<topic "Links">
<dl>
  <dt><a href="http://homemade.hypermart.net/websec/">Old Homepage</a></dt>
  <dd>
    The website of the former developer, frozen from the 1.3.4 days.	 
  </dd>
  
  <dt><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></dt>
  <dd>
	The project page at <href url="http://savannah.gnu.org/" name=Savannah/>,
	they provide us with numerous services and hosting, <strong>Thanks Folks!</strong>.
  </dd>

  <dt><a href="http://savannah.nongnu.org/download/websec/old">Obsolete Releases</a></dt>
  <dd>
    Old releases are moved to a secondary place so they will be out of the way. They are kept
	for historical fun.
  </dd>

  <dt><a href="http://savannah.nongnu.org/download/websec/svn">Subversions Repository Dump</a></dt>
  <dd>
    Development is done in a local Subversions Repository since it's easier than having constant online
	connection, so I'm providing the repository dump online for backup and for others benefit.
  </dd>
</dl>
</topic>

<topic '"Competitors"'>
<p>
  Maybe you are not happy with Web Secretary (please tell me why!), or maybe you want to look at
  other options, it's a Free-Software world(!) after-all. Following is a list of programs I've found
  that are somewhat related to what WebSec does.
</p>

<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
    Checks only timestamps and not actual page changes. It's main advantages
    is being written in Python.
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
   <br/>
   There appears to be two wrep's, from two different authors, with totally
   different methods of work.
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd>
   A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.
 </dd>
</dl>
</topic>


Revision-number: 158
Prop-content-length: 119
Content-length: 119

K 7
svn:log
V 18
Remove dead links

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2004-12-02T18:21:00.977669Z
PROPS-END

Node-path: trunk/site/index.html.wml
Node-kind: file
Node-action: change
Text-content-length: 6262
Text-content-md5: 2390341505aff65e9fe11264d29a076c
Content-length: 6262

#use wml::templates::be-tmpl
<subject "WebSec - A Web Secretary"/>

#<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
#<:listversions():>
<li><a href="websec-1.7.0.tar.gz">1.7.0</a> - <a href="websec-1.7.0.tar.gz.sig">GPG signature</a></li>
</ul>

<p>Contacts:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://savannah.nongnu.org/mail/?group=websec">mailing list</a></li>
 <li>Baruch Even (Author): websec(AT)ev-en.org</li>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
 <li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the configuration file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dial-up, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</topic>

<topic "Links">
<dl>
  <dt><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></dt>
  <dd>
	The project page at <href url="http://savannah.gnu.org/" name=Savannah/>,
	they provide us with numerous services and hosting, <strong>Thanks Folks!</strong>.
  </dd>
</dl>
</topic>

<topic '"Competitors"'>
<p>
  Maybe you are not happy with Web Secretary (please tell me why!), or maybe you want to look at
  other options, it's a Free-Software world(!) after-all. Following is a list of programs I've found
  that are somewhat related to what WebSec does.
</p>

<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
    Checks only timestamps and not actual page changes. It's main advantages
    is being written in Python.
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
   <br/>
   There appears to be two wrep's, from two different authors, with totally
   different methods of work.
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd>
   A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.
 </dd>
</dl>
</topic>


Revision-number: 159
Prop-content-length: 159
Content-length: 159

K 7
svn:log
V 58
Remove links section, the project page is linked elsewhere
K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2004-12-02T18:22:30.191346Z
PROPS-END

Node-path: trunk/site/index.html.wml
Node-kind: file
Node-action: change
Text-content-length: 5968
Text-content-md5: ead7f58cb452b324df478f00ff1fd036
Content-length: 5968

#use wml::templates::be-tmpl
<subject "WebSec - A Web Secretary"/>

#<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
#<:listversions():>
<li><a href="websec-1.7.0.tar.gz">1.7.0</a> - <a href="websec-1.7.0.tar.gz.sig">GPG signature</a></li>
</ul>

<p>Contacts:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://savannah.nongnu.org/mail/?group=websec">mailing list</a></li>
 <li>Baruch Even (Author): websec(AT)ev-en.org</li>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
 <li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the configuration file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dial-up, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</topic>

<topic '"Competitors"'>
<p>
  Maybe you are not happy with Web Secretary (please tell me why!), or maybe you want to look at
  other options, it's a Free-Software world(!) after-all. Following is a list of programs I've found
  that are somewhat related to what WebSec does.
</p>

<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
    Checks only timestamps and not actual page changes. It's main advantages
    is being written in Python.
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
   <br/>
   There appears to be two wrep's, from two different authors, with totally
   different methods of work.
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd>
   A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.
 </dd>
</dl>
</topic>


Revision-number: 160
Prop-content-length: 148
Content-length: 148

K 7
svn:log
V 47
Use the real e-mail, dont bother obfuscating it
K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2004-12-02T18:24:02.766637Z
PROPS-END

Node-path: trunk/site/index.html.wml
Node-kind: file
Node-action: change
Text-content-length: 6003
Text-content-md5: ced4c32cef3f617fcda686e9242c77f7
Content-length: 6003

#use wml::templates::be-tmpl
<subject "WebSec - A Web Secretary"/>

#<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
#<:listversions():>
<li><a href="websec-1.7.0.tar.gz">1.7.0</a> - <a href="websec-1.7.0.tar.gz.sig">GPG signature</a></li>
</ul>

<p>Contacts:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://savannah.nongnu.org/mail/?group=websec">mailing list</a></li>
 <li>Baruch Even (Author): <a href="mailto:websec@ev-en.org">websec@ev-en.org</a></li>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
 <li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the configuration file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dial-up, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</topic>

<topic '"Competitors"'>
<p>
  Maybe you are not happy with Web Secretary (please tell me why!), or maybe you want to look at
  other options, it's a Free-Software world(!) after-all. Following is a list of programs I've found
  that are somewhat related to what WebSec does.
</p>

<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
    Checks only timestamps and not actual page changes. It's main advantages
    is being written in Python.
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
   <br/>
   There appears to be two wrep's, from two different authors, with totally
   different methods of work.
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd>
   A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.
 </dd>
</dl>
</topic>


Revision-number: 161
Prop-content-length: 141
Content-length: 141

K 7
svn:log
V 40
Add makefile to create the manpage htmls
K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2004-12-02T20:30:01.371199Z
PROPS-END

Node-path: trunk/site/Makefile
Node-kind: file
Node-action: add
Prop-content-length: 10
Text-content-length: 140
Text-content-md5: 9f9dce281cde70ff5baa0338038a3e3b
Content-length: 150

PROPS-END
all:
	$(MAKE) -C source install_html DEST=`pwd`

clean:
	-rm -f "ignore.list(5).html" "url.list(5).html" "webdiff(1).html" "websec(1).html"


Revision-number: 162
Prop-content-length: 153
Content-length: 153

K 7
svn:log
V 52
Add a link to an advanced usage of cron and websec.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2004-12-04T01:35:46.917058Z
PROPS-END

Node-path: trunk/site/index.html.wml
Node-kind: file
Node-action: change
Text-content-length: 6229
Text-content-md5: 19abb49dd77962917f5277487623b87d
Content-length: 6229

#use wml::templates::be-tmpl
<subject "WebSec - A Web Secretary"/>

#<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
#<:listversions():>
<li><a href="websec-1.7.0.tar.gz">1.7.0</a> - <a href="websec-1.7.0.tar.gz.sig">GPG signature</a></li>
</ul>

<p>Contacts:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://savannah.nongnu.org/mail/?group=websec">mailing list</a></li>
 <li>Baruch Even (Author): <a href="mailto:websec@ev-en.org">websec@ev-en.org</a></li>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
 <li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the configuration file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dial-up, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
<p>
If you want to have different sites checked at different intervals you can
check the way Jani Uusitalo made <a
href="http://www.mummila.net/varasto/tekstit/websec-cron.html">an advanced
setup for websec and cron</a>.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</topic>

<topic '"Competitors"'>
<p>
  Maybe you are not happy with Web Secretary (please tell me why!), or maybe you want to look at
  other options, it's a Free-Software world(!) after-all. Following is a list of programs I've found
  that are somewhat related to what WebSec does.
</p>

<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
	Based on Web Secretary, but written in C++ and with a KDE GUI. Wasn't
	updated since January 2002 and is written for KDE 2 and not the latest KDE
	3.
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
    Checks only timestamps and not actual page changes. It's main advantages
    is being written in Python.
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
   <br/>
   There appears to be two wrep's, from two different authors, with totally
   different methods of work.
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd>
   A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.
 </dd>
</dl>
</topic>


Revision-number: 163
Prop-content-length: 127
Content-length: 127

K 7
svn:log
V 26
Update status of KWebWatch
K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2004-12-14T12:36:03.882105Z
PROPS-END

Node-path: trunk/site/index.html.wml
Node-kind: file
Node-action: change
Text-content-length: 6203
Text-content-md5: dbfbe0cec446ad6456486bb4968fcd25
Content-length: 6203

#use wml::templates::be-tmpl
<subject "WebSec - A Web Secretary"/>

#<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
#<:listversions():>
<li><a href="websec-1.7.0.tar.gz">1.7.0</a> - <a href="websec-1.7.0.tar.gz.sig">GPG signature</a></li>
</ul>

<p>Contacts:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://savannah.nongnu.org/mail/?group=websec">mailing list</a></li>
 <li>Baruch Even (Author): <a href="mailto:websec@ev-en.org">websec@ev-en.org</a></li>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
 <li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the configuration file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dial-up, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
<p>
If you want to have different sites checked at different intervals you can
check the way Jani Uusitalo made <a
href="http://www.mummila.net/varasto/tekstit/websec-cron.html">an advanced
setup for websec and cron</a>.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</topic>

<topic '"Competitors"'>
<p>
  Maybe you are not happy with Web Secretary (please tell me why!), or maybe you want to look at
  other options, it's a Free-Software world(!) after-all. Following is a list of programs I've found
  that are somewhat related to what WebSec does.
</p>

<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
  Based on Web Secretary, but written in C++ and with a KDE GUI. Has
  renewed development and offers a KDE3 user interface.
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
    Checks only timestamps and not actual page changes. It's main advantages
    is being written in Python.
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
   <br/>
   There appears to be two wrep's, from two different authors, with totally
   different methods of work.
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd>
   A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.
 </dd>
</dl>
</topic>


Revision-number: 164
Prop-content-length: 187
Content-length: 187

K 7
svn:log
V 86
Check for the existence of Compress::Zlib correctly, fixes bug #271658 in Debian BTS.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2005-01-01T22:16:03.510832Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 21927
Text-content-md5: 8a02b26a1056c3af9c2a2c4c0a62a2e0
Content-length: 21927

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
use URI;
eval { require Compress::Zlib; } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters

$urllist = "url.list";
GetOptions("urllist=s" => \$urllist);

if ( -e $urllist ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$page_current = "$base/retrieve.html";

# Red Hat has sendmail in /usr/sbin/sendmail, but LWP::Protocol::mailto
# expects it in /usr/lib/sendmail.  Revert to the Red Hat location if there
# is no executable at the expected location.
use vars qw($LWP::Protocol::mailto::SENDMAIL);
$LWP::Protocol::mailto::SENDMAIL = "/usr/sbin/sendmail"
  unless -x "/usr/lib/sendmail";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

$htmldiffbin = "$FindBin::Bin/htmldiff";
if ( !-e $htmldiffbin ) {
  $htmldiffbin = "./htmldiff";
  if ( !-e $htmldiffbin ) {
      $htmldiffbin = "htmldiff";
  }
}

# prepare digest
@digest = ();
@htmldigest = ();

# Set default values
local %defaults = (
    url        => "",
    auth       => "none",
    name       => "",
    prefix     => "",
    diff       => "webdiff",
    hicolor    => "blue",
    asciimarker => 0,
    ignore     => "none",
    ignoreurl  => "none",
    email      => "",
    emaillink  => "",
    emailerror => 1,
    program    => "",
    programdigest => "",
    proxy      => "",
    proxyauth  => "none",
    randomwait => 0,
    retry      => 3,
    retrywait  => 0,
    timeout    => 20,
    tmin       => 0,
    tmax       => 99999,
    addsubject => "",
    digest     => "false",
    useragent  => "WebSec/1.7",
    datefmt    => " - %d %B %Y (%a)",
    mailfrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/" . "$urllist" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{url} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{mailfrom} );
}

if (@htmldigest) {
    open( OUTPAGE, "> $base/index.html" ) or die "Cannot open $base/index.html: $!\n";
    print OUTPAGE "<HTML><BODY>The contents of the following URLs has changed:<P><P>";
    foreach (@htmldigest) { print OUTPAGE "$_<P>\n"; }
    print OUTPAGE "<P></BODY></HTML>";
    close(OUTPAGE);

    &ShowDocument( $program, $base."/index.html");
}


# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $keyword =~ tr/A-Z/a-z/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{url};
    $name       = $siteinfo{name};
    $prefix     = $siteinfo{prefix};
    $diff       = $siteinfo{diff};
    $hicolor    = $siteinfo{hicolor};
    $ignore     = $siteinfo{ignore};
    $ignoreurl  = $siteinfo{ignoreurl};
    $email      = $siteinfo{email};
    $emailLink  = $siteinfo{emaillink};
    $program    = $siteinfo{program};
    $programdigest = $siteinfo{programdigest};
    $proxy      = $siteinfo{proxy};
    $randomwait = $siteinfo{randomwait};
    $retry      = $siteinfo{retry};
    $retrywait  = $siteinfo{retrywait};
    $timeout    = $siteinfo{timeout};
    $tmin       = $siteinfo{tmin};
    $tmax       = $siteinfo{tmax};
    $addsubject = $siteinfo{addsubject};
    $digest     = $siteinfo{digest};
    $useragent  = $siteinfo{useragent};
    $datefmt    = $siteinfo{datefmt};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $outgoing     = "$pagebase.diff.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";
     $htmldiff = "$htmldiffbin $page_previous $page_current > $outgoing";

    if ($siteinfo{asciimarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = PrepareRequest($url);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);

        if ( ! $resp->is_success ) {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
            next;
        }

        # Leave if there is no refresh header
        if (!$resp->header("Refresh")) { last; }

        # Handle it if the refresh is for zero seconds
        ( $time, $refresh_url ) = split(/;/, $resp->header("Refresh"), 2);
        if ($time > 0) { last; }
        
        # Convert to absolute URL and refetch the page
        ( undef, $refresh_to ) = split(/=/, $refresh_url, 2);
        $newurl = URI->new_abs($refresh_to, $url)->as_string();

        $req = PrepareRequest($newurl);
        # Don't reset the counter, we still want to protect from endless loops
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{mailfrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{mailfrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        elsif ( $diff eq "htmldiff" )
        {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage using htmldiff...\n";
                $rc = system($htmldiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{mailfrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{emailerror} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{mailfrom} );
        }
        if ( $emailLink ne "" && $siteinfo{emailerror} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{mailfrom} );
            }
        }
    }

    return 0;
}

sub PrepareRequest() {
    my $url = shift (@_);

    $req = new HTTP::Request( 'GET', $url );

    my $auth = $siteinfo{auth};
    if ( $auth ne "none" ) { $req->authorization_basic( split ( /:/, $auth, 2 ) ); }

    my $proxyAuth = $siteinfo{proxyauth};
    if ( $proxyAuth ne "none" ) { $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) ); }
    
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    
    my $compress_options = "identity";
    if ($feature_compress) { $compress_options = "gzip, $compress_options"; }
    $req->push_header("Accept-Encoding" => $compress_options);

    return $req;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        my $resp = $ua->request($req);
        die "Error mailing document: ".$resp->message()."\n" if $resp->is_error;
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=item B<--urllist>

Use another file for the url list, by default it is "url.list".

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 165
Prop-content-length: 210
Content-length: 210

K 7
svn:log
V 108
Add file: to the url part so that Mozilla/FireFox will be able to open it.
Fixes bug #287353 in Debian BTS.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2005-01-01T22:17:34.038295Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 21932
Text-content-md5: 7298786a5ac0fd30c92ca5bfde8ff07f
Content-length: 21932

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
use URI;
eval { require Compress::Zlib; } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters

$urllist = "url.list";
GetOptions("urllist=s" => \$urllist);

if ( -e $urllist ) {
    $base = ".";
}
else {
    $base = $ENV{HOME} . "/.websec";
}
$help = 0;
$man  = 0;

# Parse command line options
GetOptions(
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);
pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$page_current = "$base/retrieve.html";

# Red Hat has sendmail in /usr/sbin/sendmail, but LWP::Protocol::mailto
# expects it in /usr/lib/sendmail.  Revert to the Red Hat location if there
# is no executable at the expected location.
use vars qw($LWP::Protocol::mailto::SENDMAIL);
$LWP::Protocol::mailto::SENDMAIL = "/usr/sbin/sendmail"
  unless -x "/usr/lib/sendmail";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

$htmldiffbin = "$FindBin::Bin/htmldiff";
if ( !-e $htmldiffbin ) {
  $htmldiffbin = "./htmldiff";
  if ( !-e $htmldiffbin ) {
      $htmldiffbin = "htmldiff";
  }
}

# prepare digest
@digest = ();
@htmldigest = ();

# Set default values
local %defaults = (
    url        => "",
    auth       => "none",
    name       => "",
    prefix     => "",
    diff       => "webdiff",
    hicolor    => "blue",
    asciimarker => 0,
    ignore     => "none",
    ignoreurl  => "none",
    email      => "",
    emaillink  => "",
    emailerror => 1,
    program    => "",
    programdigest => "",
    proxy      => "",
    proxyauth  => "none",
    randomwait => 0,
    retry      => 3,
    retrywait  => 0,
    timeout    => 20,
    tmin       => 0,
    tmax       => 99999,
    addsubject => "",
    digest     => "false",
    useragent  => "WebSec/1.7",
    datefmt    => " - %d %B %Y (%a)",
    mailfrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/" . "$urllist" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{url} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{mailfrom} );
}

if (@htmldigest) {
    open( OUTPAGE, "> $base/index.html" ) or die "Cannot open $base/index.html: $!\n";
    print OUTPAGE "<HTML><BODY>The contents of the following URLs has changed:<P><P>";
    foreach (@htmldigest) { print OUTPAGE "$_<P>\n"; }
    print OUTPAGE "<P></BODY></HTML>";
    close(OUTPAGE);

    &ShowDocument( $program, $base."/index.html");
}


# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $keyword =~ tr/A-Z/a-z/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{url};
    $name       = $siteinfo{name};
    $prefix     = $siteinfo{prefix};
    $diff       = $siteinfo{diff};
    $hicolor    = $siteinfo{hicolor};
    $ignore     = $siteinfo{ignore};
    $ignoreurl  = $siteinfo{ignoreurl};
    $email      = $siteinfo{email};
    $emailLink  = $siteinfo{emaillink};
    $program    = $siteinfo{program};
    $programdigest = $siteinfo{programdigest};
    $proxy      = $siteinfo{proxy};
    $randomwait = $siteinfo{randomwait};
    $retry      = $siteinfo{retry};
    $retrywait  = $siteinfo{retrywait};
    $timeout    = $siteinfo{timeout};
    $tmin       = $siteinfo{tmin};
    $tmax       = $siteinfo{tmax};
    $addsubject = $siteinfo{addsubject};
    $digest     = $siteinfo{digest};
    $useragent  = $siteinfo{useragent};
    $datefmt    = $siteinfo{datefmt};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $outgoing     = "$pagebase.diff.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";
     $htmldiff = "$htmldiffbin $page_previous $page_current > $outgoing";

    if ($siteinfo{asciimarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = PrepareRequest($url);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);

        if ( ! $resp->is_success ) {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
            next;
        }

        # Leave if there is no refresh header
        if (!$resp->header("Refresh")) { last; }

        # Handle it if the refresh is for zero seconds
        ( $time, $refresh_url ) = split(/;/, $resp->header("Refresh"), 2);
        if ($time > 0) { last; }
        
        # Convert to absolute URL and refetch the page
        ( undef, $refresh_to ) = split(/=/, $refresh_url, 2);
        $newurl = URI->new_abs($refresh_to, $url)->as_string();

        $req = PrepareRequest($newurl);
        # Don't reset the counter, we still want to protect from endless loops
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{mailfrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{mailfrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        elsif ( $diff eq "htmldiff" )
        {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage using htmldiff...\n";
                $rc = system($htmldiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{mailfrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{emailerror} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{mailfrom} );
        }
        if ( $emailLink ne "" && $siteinfo{emailerror} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{mailfrom} );
            }
        }
    }

    return 0;
}

sub PrepareRequest() {
    my $url = shift (@_);

    $req = new HTTP::Request( 'GET', $url );

    my $auth = $siteinfo{auth};
    if ( $auth ne "none" ) { $req->authorization_basic( split ( /:/, $auth, 2 ) ); }

    my $proxyAuth = $siteinfo{proxyauth};
    if ( $proxyAuth ne "none" ) { $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) ); }
    
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    
    my $compress_options = "identity";
    if ($feature_compress) { $compress_options = "gzip, $compress_options"; }
    $req->push_header("Accept-Encoding" => $compress_options);

    return $req;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        my $resp = $ua->request($req);
        die "Error mailing document: ".$resp->message()."\n" if $resp->is_error;
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(file:" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=item B<--urllist>

Use another file for the url list, by default it is "url.list".

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 166
Prop-content-length: 210
Content-length: 210

K 7
svn:log
V 108
Apply patch #2538 from Savannah to fix handling of parameters that was
introduced with the urllist feature.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2005-01-01T22:33:30.045610Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 21964
Text-content-md5: 8e7b041607cfef207175032073c2bfb2
Content-length: 21964

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
use URI;
eval { require Compress::Zlib; } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters

$help = 0;
$man  = 0;
$urllist = "url.list";

# Parse command line options
GetOptions(
    "urllist=s" => \$urllist,
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);

pod2usage(1) if $help;
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

if ($base eq "") {
    if ( -e $urllist ) {
        $base = ".";
    } else {
        $base = $ENV{HOME} . "/.websec";
    }
}

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$page_current = "$base/retrieve.html";

# Red Hat has sendmail in /usr/sbin/sendmail, but LWP::Protocol::mailto
# expects it in /usr/lib/sendmail.  Revert to the Red Hat location if there
# is no executable at the expected location.
use vars qw($LWP::Protocol::mailto::SENDMAIL);
$LWP::Protocol::mailto::SENDMAIL = "/usr/sbin/sendmail"
  unless -x "/usr/lib/sendmail";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

$htmldiffbin = "$FindBin::Bin/htmldiff";
if ( !-e $htmldiffbin ) {
  $htmldiffbin = "./htmldiff";
  if ( !-e $htmldiffbin ) {
      $htmldiffbin = "htmldiff";
  }
}

# prepare digest
@digest = ();
@htmldigest = ();

# Set default values
local %defaults = (
    url        => "",
    auth       => "none",
    name       => "",
    prefix     => "",
    diff       => "webdiff",
    hicolor    => "blue",
    asciimarker => 0,
    ignore     => "none",
    ignoreurl  => "none",
    email      => "",
    emaillink  => "",
    emailerror => 1,
    program    => "",
    programdigest => "",
    proxy      => "",
    proxyauth  => "none",
    randomwait => 0,
    retry      => 3,
    retrywait  => 0,
    timeout    => 20,
    tmin       => 0,
    tmax       => 99999,
    addsubject => "",
    digest     => "false",
    useragent  => "WebSec/1.7",
    datefmt    => " - %d %B %Y (%a)",
    mailfrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/" . "$urllist" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{url} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{mailfrom} );
}

if (@htmldigest) {
    open( OUTPAGE, "> $base/index.html" ) or die "Cannot open $base/index.html: $!\n";
    print OUTPAGE "<HTML><BODY>The contents of the following URLs has changed:<P><P>";
    foreach (@htmldigest) { print OUTPAGE "$_<P>\n"; }
    print OUTPAGE "<P></BODY></HTML>";
    close(OUTPAGE);

    &ShowDocument( $program, $base."/index.html");
}


# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $keyword =~ tr/A-Z/a-z/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{url};
    $name       = $siteinfo{name};
    $prefix     = $siteinfo{prefix};
    $diff       = $siteinfo{diff};
    $hicolor    = $siteinfo{hicolor};
    $ignore     = $siteinfo{ignore};
    $ignoreurl  = $siteinfo{ignoreurl};
    $email      = $siteinfo{email};
    $emailLink  = $siteinfo{emaillink};
    $program    = $siteinfo{program};
    $programdigest = $siteinfo{programdigest};
    $proxy      = $siteinfo{proxy};
    $randomwait = $siteinfo{randomwait};
    $retry      = $siteinfo{retry};
    $retrywait  = $siteinfo{retrywait};
    $timeout    = $siteinfo{timeout};
    $tmin       = $siteinfo{tmin};
    $tmax       = $siteinfo{tmax};
    $addsubject = $siteinfo{addsubject};
    $digest     = $siteinfo{digest};
    $useragent  = $siteinfo{useragent};
    $datefmt    = $siteinfo{datefmt};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $outgoing     = "$pagebase.diff.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";
     $htmldiff = "$htmldiffbin $page_previous $page_current > $outgoing";

    if ($siteinfo{asciimarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = PrepareRequest($url);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);

        if ( ! $resp->is_success ) {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
            next;
        }

        # Leave if there is no refresh header
        if (!$resp->header("Refresh")) { last; }

        # Handle it if the refresh is for zero seconds
        ( $time, $refresh_url ) = split(/;/, $resp->header("Refresh"), 2);
        if ($time > 0) { last; }
        
        # Convert to absolute URL and refetch the page
        ( undef, $refresh_to ) = split(/=/, $refresh_url, 2);
        $newurl = URI->new_abs($refresh_to, $url)->as_string();

        $req = PrepareRequest($newurl);
        # Don't reset the counter, we still want to protect from endless loops
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{mailfrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{mailfrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        elsif ( $diff eq "htmldiff" )
        {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage using htmldiff...\n";
                $rc = system($htmldiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{mailfrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{emailerror} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{mailfrom} );
        }
        if ( $emailLink ne "" && $siteinfo{emailerror} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{mailfrom} );
            }
        }
    }

    return 0;
}

sub PrepareRequest() {
    my $url = shift (@_);

    $req = new HTTP::Request( 'GET', $url );

    my $auth = $siteinfo{auth};
    if ( $auth ne "none" ) { $req->authorization_basic( split ( /:/, $auth, 2 ) ); }

    my $proxyAuth = $siteinfo{proxyauth};
    if ( $proxyAuth ne "none" ) { $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) ); }
    
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    
    my $compress_options = "identity";
    if ($feature_compress) { $compress_options = "gzip, $compress_options"; }
    $req->push_header("Accept-Encoding" => $compress_options);

    return $req;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        my $resp = $ua->request($req);
        die "Error mailing document: ".$resp->message()."\n" if $resp->is_error;
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(file:" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=item B<--urllist>

Use another file for the url list, by default it is "url.list".

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 167
Prop-content-length: 129
Content-length: 129

K 7
svn:log
V 28
Give a default value to base
K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2005-01-02T00:39:58.969183Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 21976
Text-content-md5: 9d5b12556ce63c37427ebcb303fb21a9
Content-length: 21976

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
use URI;
eval { require Compress::Zlib; } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters

$help = 0;
$man  = 0;
$urllist = "url.list";
$base = "";

# Parse command line options
GetOptions(
    "urllist=s" => \$urllist,
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);

pod2usage(1) if $help;
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

if ($base eq "") {
    if ( -e $urllist ) {
        $base = ".";
    } else {
        $base = $ENV{HOME} . "/.websec";
    }
}

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
$page_current = "$base/retrieve.html";

# Red Hat has sendmail in /usr/sbin/sendmail, but LWP::Protocol::mailto
# expects it in /usr/lib/sendmail.  Revert to the Red Hat location if there
# is no executable at the expected location.
use vars qw($LWP::Protocol::mailto::SENDMAIL);
$LWP::Protocol::mailto::SENDMAIL = "/usr/sbin/sendmail"
  unless -x "/usr/lib/sendmail";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

$htmldiffbin = "$FindBin::Bin/htmldiff";
if ( !-e $htmldiffbin ) {
  $htmldiffbin = "./htmldiff";
  if ( !-e $htmldiffbin ) {
      $htmldiffbin = "htmldiff";
  }
}

# prepare digest
@digest = ();
@htmldigest = ();

# Set default values
local %defaults = (
    url        => "",
    auth       => "none",
    name       => "",
    prefix     => "",
    diff       => "webdiff",
    hicolor    => "blue",
    asciimarker => 0,
    ignore     => "none",
    ignoreurl  => "none",
    email      => "",
    emaillink  => "",
    emailerror => 1,
    program    => "",
    programdigest => "",
    proxy      => "",
    proxyauth  => "none",
    randomwait => 0,
    retry      => 3,
    retrywait  => 0,
    timeout    => 20,
    tmin       => 0,
    tmax       => 99999,
    addsubject => "",
    digest     => "false",
    useragent  => "WebSec/1.7",
    datefmt    => " - %d %B %Y (%a)",
    mailfrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/" . "$urllist" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{url} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{mailfrom} );
}

if (@htmldigest) {
    open( OUTPAGE, "> $base/index.html" ) or die "Cannot open $base/index.html: $!\n";
    print OUTPAGE "<HTML><BODY>The contents of the following URLs has changed:<P><P>";
    foreach (@htmldigest) { print OUTPAGE "$_<P>\n"; }
    print OUTPAGE "<P></BODY></HTML>";
    close(OUTPAGE);

    &ShowDocument( $program, $base."/index.html");
}


# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $keyword =~ tr/A-Z/a-z/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{url};
    $name       = $siteinfo{name};
    $prefix     = $siteinfo{prefix};
    $diff       = $siteinfo{diff};
    $hicolor    = $siteinfo{hicolor};
    $ignore     = $siteinfo{ignore};
    $ignoreurl  = $siteinfo{ignoreurl};
    $email      = $siteinfo{email};
    $emailLink  = $siteinfo{emaillink};
    $program    = $siteinfo{program};
    $programdigest = $siteinfo{programdigest};
    $proxy      = $siteinfo{proxy};
    $randomwait = $siteinfo{randomwait};
    $retry      = $siteinfo{retry};
    $retrywait  = $siteinfo{retrywait};
    $timeout    = $siteinfo{timeout};
    $tmin       = $siteinfo{tmin};
    $tmax       = $siteinfo{tmax};
    $addsubject = $siteinfo{addsubject};
    $digest     = $siteinfo{digest};
    $useragent  = $siteinfo{useragent};
    $datefmt    = $siteinfo{datefmt};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $outgoing     = "$pagebase.diff.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";
     $htmldiff = "$htmldiffbin $page_previous $page_current > $outgoing";

    if ($siteinfo{asciimarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = PrepareRequest($url);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);

        if ( ! $resp->is_success ) {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
            next;
        }

        # Leave if there is no refresh header
        if (!$resp->header("Refresh")) { last; }

        # Handle it if the refresh is for zero seconds
        ( $time, $refresh_url ) = split(/;/, $resp->header("Refresh"), 2);
        if ($time > 0) { last; }
        
        # Convert to absolute URL and refetch the page
        ( undef, $refresh_to ) = split(/=/, $refresh_url, 2);
        $newurl = URI->new_abs($refresh_to, $url)->as_string();

        $req = PrepareRequest($newurl);
        # Don't reset the counter, we still want to protect from endless loops
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{mailfrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{mailfrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        elsif ( $diff eq "htmldiff" )
        {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage using htmldiff...\n";
                $rc = system($htmldiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{mailfrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{emailerror} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{mailfrom} );
        }
        if ( $emailLink ne "" && $siteinfo{emailerror} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{mailfrom} );
            }
        }
    }

    return 0;
}

sub PrepareRequest() {
    my $url = shift (@_);

    $req = new HTTP::Request( 'GET', $url );

    my $auth = $siteinfo{auth};
    if ( $auth ne "none" ) { $req->authorization_basic( split ( /:/, $auth, 2 ) ); }

    my $proxyAuth = $siteinfo{proxyauth};
    if ( $proxyAuth ne "none" ) { $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) ); }
    
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    
    my $compress_options = "identity";
    if ($feature_compress) { $compress_options = "gzip, $compress_options"; }
    $req->push_header("Accept-Encoding" => $compress_options);

    return $req;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        my $resp = $ua->request($req);
        die "Error mailing document: ".$resp->message()."\n" if $resp->is_error;
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(file:" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=item B<--urllist>

Use another file for the url list, by default it is "url.list".

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 168
Prop-content-length: 244
Content-length: 244

K 7
svn:log
V 142
Use temporary file names from File::Temp so that multiple websecs for different
url.list's will work correctly. Fixes bug #11474 in Savannah.

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2005-01-02T01:43:28.389852Z
PROPS-END

Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 22048
Text-content-md5: 7cabe7858a08c19e5f7afaa78ff10925
Content-length: 22048

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.7.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
use URI;
eval { require Compress::Zlib; } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;
use File::Temp qw/tempfile/;

# Print introduction
print "Web Secretary Ver 1.7.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters

$help = 0;
$man  = 0;
$urllist = "url.list";
$base = "";

# Parse command line options
GetOptions(
    "urllist=s" => \$urllist,
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);

pod2usage(1) if $help;
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

if ($base eq "") {
    if ( -e $urllist ) {
        $base = ".";
    } else {
        $base = $ENV{HOME} . "/.websec";
    }
}

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
($current_fh, $page_current) = tempfile(DIR=>"$base", UNLINK=>1, SUFFIX=>".html");
$current_fh=-1;

# Red Hat has sendmail in /usr/sbin/sendmail, but LWP::Protocol::mailto
# expects it in /usr/lib/sendmail.  Revert to the Red Hat location if there
# is no executable at the expected location.
use vars qw($LWP::Protocol::mailto::SENDMAIL);
$LWP::Protocol::mailto::SENDMAIL = "/usr/sbin/sendmail"
  unless -x "/usr/lib/sendmail";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

$htmldiffbin = "$FindBin::Bin/htmldiff";
if ( !-e $htmldiffbin ) {
  $htmldiffbin = "./htmldiff";
  if ( !-e $htmldiffbin ) {
      $htmldiffbin = "htmldiff";
  }
}

# prepare digest
@digest = ();
@htmldigest = ();

# Set default values
local %defaults = (
    url        => "",
    auth       => "none",
    name       => "",
    prefix     => "",
    diff       => "webdiff",
    hicolor    => "blue",
    asciimarker => 0,
    ignore     => "none",
    ignoreurl  => "none",
    email      => "",
    emaillink  => "",
    emailerror => 1,
    program    => "",
    programdigest => "",
    proxy      => "",
    proxyauth  => "none",
    randomwait => 0,
    retry      => 3,
    retrywait  => 0,
    timeout    => 20,
    tmin       => 0,
    tmax       => 99999,
    addsubject => "",
    digest     => "false",
    useragent  => "WebSec/1.7",
    datefmt    => " - %d %B %Y (%a)",
    mailfrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/" . "$urllist" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{url} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{mailfrom} );
}

if (@htmldigest) {
    ($OUTPAGE, $pagename) = tempfile(DIR=>"$base", UNLINK=>1, SUFFIX=>".html");
    print OUTPAGE "<HTML><BODY>The contents of the following URLs has changed:<P><P>";
    foreach (@htmldigest) { print OUTPAGE "$_<P>\n"; }
    print OUTPAGE "<P></BODY></HTML>";
    close(OUTPAGE);

    &ShowDocument( $program, $pagename);
}


# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $keyword =~ tr/A-Z/a-z/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{url};
    $name       = $siteinfo{name};
    $prefix     = $siteinfo{prefix};
    $diff       = $siteinfo{diff};
    $hicolor    = $siteinfo{hicolor};
    $ignore     = $siteinfo{ignore};
    $ignoreurl  = $siteinfo{ignoreurl};
    $email      = $siteinfo{email};
    $emailLink  = $siteinfo{emaillink};
    $program    = $siteinfo{program};
    $programdigest = $siteinfo{programdigest};
    $proxy      = $siteinfo{proxy};
    $randomwait = $siteinfo{randomwait};
    $retry      = $siteinfo{retry};
    $retrywait  = $siteinfo{retrywait};
    $timeout    = $siteinfo{timeout};
    $tmin       = $siteinfo{tmin};
    $tmax       = $siteinfo{tmax};
    $addsubject = $siteinfo{addsubject};
    $digest     = $siteinfo{digest};
    $useragent  = $siteinfo{useragent};
    $datefmt    = $siteinfo{datefmt};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $outgoing     = "$pagebase.diff.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";
     $htmldiff = "$htmldiffbin $page_previous $page_current > $outgoing";

    if ($siteinfo{asciimarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = PrepareRequest($url);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);

        if ( ! $resp->is_success ) {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
            next;
        }

        # Leave if there is no refresh header
        if (!$resp->header("Refresh")) { last; }

        # Handle it if the refresh is for zero seconds
        ( $time, $refresh_url ) = split(/;/, $resp->header("Refresh"), 2);
        if ($time > 0) { last; }
        
        # Convert to absolute URL and refetch the page
        ( undef, $refresh_to ) = split(/=/, $refresh_url, 2);
        $newurl = URI->new_abs($refresh_to, $url)->as_string();

        $req = PrepareRequest($newurl);
        # Don't reset the counter, we still want to protect from endless loops
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{mailfrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{mailfrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        elsif ( $diff eq "htmldiff" )
        {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage using htmldiff...\n";
                $rc = system($htmldiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{mailfrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{emailerror} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{mailfrom} );
        }
        if ( $emailLink ne "" && $siteinfo{emailerror} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{mailfrom} );
            }
        }
    }

    return 0;
}

sub PrepareRequest() {
    my $url = shift (@_);

    $req = new HTTP::Request( 'GET', $url );

    my $auth = $siteinfo{auth};
    if ( $auth ne "none" ) { $req->authorization_basic( split ( /:/, $auth, 2 ) ); }

    my $proxyAuth = $siteinfo{proxyauth};
    if ( $proxyAuth ne "none" ) { $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) ); }
    
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    
    my $compress_options = "identity";
    if ($feature_compress) { $compress_options = "gzip, $compress_options"; }
    $req->push_header("Accept-Encoding" => $compress_options);

    return $req;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        my $resp = $ua->request($req);
        die "Error mailing document: ".$resp->message()."\n" if $resp->is_error;
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(file:" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=item B<--urllist>

Use another file for the url list, by default it is "url.list".

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 169
Prop-content-length: 143
Content-length: 143

K 7
svn:log
V 42
Update versions and NEWS to release 1.8.0

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2005-01-02T01:52:36.781248Z
PROPS-END

Node-path: trunk/websec/NEWS
Node-kind: file
Node-action: change
Text-content-length: 9546
Text-content-md5: f95e3ff10f63190cb300d6f343b753c8
Content-length: 9546

1.8.0 - Released on 02-Jan-2005

    * Support for diffing with HTML::Diff from CPAN, this has support to show
      the removed text in addition to the changed text. (not working yet)

    * Special support for konqueror for Program directive.

    * Replace TABLE tags with SPAN tags to highlight differences, this is
      reported to work better for some users.

    * Accept keywords for url.list in all cases.
    
    * Use sendmail from /usr/sbin/ if it's no available in /usr/lib/, this
      fixes a problem with RedHat 9. (Thanks to Michael Wittman)

    * Follow refresh links, this is useful to pass automatic forwarding pages.
      (Thanks to Adam Stanley)

    * Allow using a URL list file other than url.list (Thanks to Sacha
      Fournier)

    * Fix bug when having several websec instances running at the same time.

    * Multiple small bug fixes

1.7.0 - Released on 31-May-2003

    * Add MailFrom, to set the address from which the mail will appear to come.
      (Thanks to Adam Stanley)

    * Added URL to the mail subject, to simplify jumping to a changed page from
      the mail folder.

    * Added an EmailError option to disable sending error messages. This was
      requested numerous times (Bug#3498 on Savannah), this fix is based on a
      patch from Peter Bieringer.

    * Add man pages for url.list and ignore.list, removed their documentation
      from the README file and moved it to the files themselves for easier
      reference.

    * Add "ascii" color highlighting from Javier M. Mora. Instead of using color
      to highlight it uses ascii characters around the change.

    * Add Emacs mode to highlight url.list, it is likely to be in error, a user
      help with it would be most helpful as I'm not an Emacs user.

    * Add ViM syntax file to highlight url.list.

    * Fixed usage of environment variables in definitions, also allowed using
      multiple environment variables in a single line.

    * Handle Content-Encoding: gzip, but if the Compress::Gzip module is not
      available don't just die.

1.6.0 - Released on 05 May 2003

    * Added DateFMT feature to set Date Formats in mail subjects.

    * Add to EmailLink the local file system link so that a user can open it in
      his browser.

    * Add Program option to open a program on changed link instead of mailing
      notification.

    * Multiple bug fixes.

1.5.0 - Released on 02 May 2003

    * Added UserAgent option to control the user agent sent by the web client.
      This enables access to websites which lockdown unknown browsers.

    * Enabled the user to run websec from the current directory without the
      multi-user ~/.websec/ setup, the multi-user setup is the default unless
      you run websec from a directory with the url.list file.

    * Allow using environment variables in config file, this enables things
      like: Email = ${USER}@example.com

    * Moved the manpages to pod and into the files, this enables fancy options
      such as -help and -man to work and reduces duplication of effort.

1.4.0 - Released on 31 Oct 2002

    * Various changes that came from the Debian package

1.3.4 - Released on 6 Jun 2000

    * Added tip on how to install LWP in READE. Thanks to Jeff for contributing
      this info!

    * Fixed a bug with $emailLink and added digest format. Thanks to Matti Airas
      for contributing this!

1.3.3 - Released on 31 Jan 2000

    * Added MIME-Version header to email sent. Thanks to Joe Rumsey for pointing
      this out. This fixes problems under certain mailers, where messages are
      treated as plain text instead of HTML.

1.3.2 - Released on 2 Dec 1999

    * Added <CODE> as a recognized tag.

    * Added the ability to send updated page to multiple recipients.

    * Added the ability to send URL link instead of entire page to recipients.
      Check "EmailLink" tag.

    * "Alexander Lazic" <al@eunet.at> suggested using strftime instead of the
      Unix command "date". This has been fixed.

1.31 - Released on 17 Apr 1999

    * Volker Stampa contributed some code to allow websec to work with proxies
      that require authentication.

1.3  - Released on 20 Mar 1999

    * Trevor Boicey suggested allowing the use of arbitrary HTML colors in the
      "-hicolor" parameter of webdiff. This feature has been included.

    * Webdiff had some problems with a tag of this nature:
      <A HREF="xxx <yyy>">, first found in the ZDNET series of web sites.  This
      has been fixed.

    * A new "ignore URL" feature has been included. This allows certain
      hyperlinks sections in a web page to be skipped during webdiff
      processing.

    * All ignore keywords and URLs have been consolidated into one file.

1.22 - Released on 13 Jan 1999

    * A small shell script has been included to "rollback" the files in the
      archive directory for one session.

    * Proxy settings can now be supplied via the "http_proxy" environment
      variable. However, the "Proxy" parameter will take precedence over the
      environment variable.

    * When checking for short and long tokens (based on the Tmin and Tmax
      parameters), any mangled HTML tags are first stripped from the token
      before word count is done.  Therefore, word count is done on the "plain
      text" version of the token.

    * When checking for ignore keywords, the token which possibly contains
      mangled HTML tags is first checked. Then it is stripped of any mangled
      HTML tags and checked again. This is cater for cases where the mangled
      HTML tag precedes or follows an actual word without any spacing. Hence
      the entire string is treated as one word, and will fail to match any of
      the ignore keywords.

1.21 - Released on 1 Jan 1999 (Happy New Year!)

    * Made minor modification to try downloading any URL up to 3 times before
      giving up. I did not find it necessary to include this as a parameter, so
      it was hard-coded.

1.2  - Released on 25 Dec 1998 (Merry Christmas!)

    * Rewrote Web Secretary to use the LWP module in Perl for HTTP retrieval and
      email transmission. Hence, it is no longer necessary to have 'lynx' and
      'metasend' installed on your system in order to use Web Secretary.

    * Since Web Secretary was rewritten to use the LWP module (instead of lynx),
      for HTTP retrieval, I had to add a 'Proxy' parameter for folks (like
      myself) who are behind a firewall.

    * Added the 'Tmin' parameter to ignore short tokens when highlighting
      differences. This is useful because certain sites have tokens containing
      one or two words which change constantly but are uninteresting to track.

    * Added the 'Tmax' parameter to prevent long tokens from being processed by
      the ignore keywords filter. This was included because certain sites have
      tokens containing the current day/month etc. which I want to filter off.
      But at the same time, I do not want to filter off long paragraphs that
      contain these words.

1.11 - Released on 10 Oct 1998

    * Minor modification to the comparison algorithm so that it won't be fooled
      by extra spaces in the tokens.

1.1  - Released on 25 Sep 1998

    * Improved the detection algorithm for multiple consecutive mangled HTML
      tags so that they will not be incorrectly highlighted.

    * Support for Javascript and stylesheet tags so that they will not be
      incorrectly highlighted.

1.0  - First released on 4 Sep 1998.

    The idea for this tool originated from a software package called Tierra
    Highlights for the PC (http://www.tierra.com). I tried it out for a while
    and found it to be extremely useful.  However, like most PC tools, it was
    closely tied to the PC that you installed the software on. If you are
    working on some other computer, you will not be unable to access the pages
    being monitored. At that time, I was already convinced that email is the
    best "push" platform the world has ever seen, so why not deliver the changed
    pages via email?

    I bounced the idea around for a while amongst friends and colleagues, and
    when I could not find any sucker to write this for me :-), I wrote the first
    version in a crazy moment of unrest using shell script. However, this first
    version was not very configurable, so I quickly wrote the second version in
    Perl.

    So far, however, the program does nothing but retrieve pages and email them
    to me. I quickly added a quick hack to do a diff between an archive page
    and the current page before deciding whether to email the page, but the
    scheme proved too brittle for detecting changes in most cases.

    I lived with this scheme for a while. Finally, lunacy got the better of me.
    I figured out a quick and dirty way of doing what Tierra Highlights does,
    and actually thought I could implement the whole idea in one day. It took
    two days instead, and the initial version sucked like hell and failed
    miserably on many pages. However, you should have seen the grin on my face
    when it highlighted PC Magazine and PC Week properly. :-)

    Like most programmers who are crazy enough to think that they can "do this
    thing in one day", I spent the next two weeks feverishly debugging the
    project. Everyday, I will add new pages to the URL list, and debug those
    that failed to be highlighted. Finally, I have something which I use on a
    daily basis now and is prepared to share with the rest of the world.

vim:set et ts=4:


Node-path: trunk/websec/webdiff
Node-kind: file
Node-action: change
Text-content-length: 12916
Text-content-md5: 6c40335133f5e49d1a1c309bcf108aa8
Content-length: 12916

#!/usr/bin/perl -w

#################################################################################
#
# Webdiff Ver 1.8.0
#
# Compares two HTML pages (current and archive) and outputs a new page based
# on the current page but with the differences between the two pages highlighted.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

use Getopt::Long;
use Pod::Usage;

# Initialize parameters
$oldpage    = "";
$curpage    = "";
$outpage    = "";
$hicolor    = "blue";
$asciimarker = 0;
$ignore     = "none";
$ignoreurl  = "none";
$tmin       = 0;
$tmax       = 99999;
$debug      = 0;
$ignoreFile = "ignore.list";
$basedir    = $ENV{HOME} . "/.websec/";

# Parse options
$help = 0;
$man  = 0;
GetOptions(
    "help|?"       => \$help,
    "man"          => \$man,
    "basedir=s"    => \$basedir,
    "archive=s"    => \$oldpage,
    "current=s"    => \$curpage,
    "out=s"        => \$outpage,
    "hicolor=s"    => \$hicolor,
    "asciimarker"  => \$asciimarker,
    "ignore=s"     => \$ignore,
    "ignoreurl=s"  => \$ignoreurl,
    "tmin=i"       => \$tmin,
    "tmax=i"       => \$tmax,
    "debug"        => \$debug,
    "ignorefile=s" => \$ignoreFile
  )
  or pod2usage(0);

pod2usage(1) if ($help);
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

# Remove trailing slash from basedir, we will add it ourself everywhere needed
$basedir =~ s/\/$//;

# Make sure some essential option values are supplied
if ( $oldpage eq "" ) {
    print
      "You did not supply the archive HTML file via the --archive option.\n";
    exit -1;
}
if ( $curpage eq "" ) {
    print
      "You did not supply the current HTML file via the --current option.\n";
    exit -1;
}
if ( $outpage eq "" ) {
    print "You did not supply the output HTML file via the --out option.\n";
    exit -1;
}

# Choose highlighting color
%colorList = (
    yellow => "#ffff99",
    blue   => "#66ccff",
    pink   => "#ffcccc",
    grey   => "#4c4c4c"
);
if ( defined $colorList{$hicolor} ) { $hicolor = $colorList{$hicolor}; }
if ( $hicolor eq "" ) { $hicolor = $colorList{"blue"}; }

# Other global variables
$changeStatus = 0;
@tags         = (
    "CODE",     "B",   "I",   "U",     "TT",     "EM",
    "FONT*",    "SUP", "SUB", "SMALL", "STRIKE", "STRONG",
    "CAPTION*", "A*"
);

# Read ignore keywords
if ( $ignore ne "none" ) {
    $ignore          = "," . $ignore . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignore =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignore = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignore) { print "Ignore: $_\n"; }
}

# Read ignore urls
if ( $ignoreurl ne "none" ) {
    $ignoreurl       = "," . $ignoreurl . ",";
    $ignorelist      = "";
    $ignoreStartRead = 0;
    open( IGNORE, "< $basedir/$ignoreFile" )
      or die "Cannot open $basedir/$ignoreFile: $!\n";
    while (<IGNORE>) {
        chomp;
        s/^\s*//;
        s/\s*$//;

        # Ignore comments
        next if (m/^#/);
        # Stop with a finish marker
        last if (m/^__END__/);

        if ( $ignoreStartRead && $_ eq "" ) { $ignoreStartRead = 0; next; }
        if ($ignoreStartRead) { $ignorelist .= $_ . "\r"; next; }
        ( $section = $_ ) =~ s:\[\s*(.*?)\s*\]:$1:sig;
        if ( $ignoreurl =~ m:,$section,:i ) { $ignoreStartRead = 1; }
    }
    close( IGNORE );
    @ignoreurl = split /[\r\n]/, $ignorelist;
}
if ($debug) {
    foreach (@ignoreurl) { print "IgnoreURL: $_\n"; }
}

# Undefine line separator so that we can read entire file at one go from now on
undef $/;

# Open input pages for comparing
open( OLDPAGE, "< $oldpage" ) or die "Cannot open $oldpage: $!\n";
open( CURPAGE, "< $curpage" ) or die "Cannot open $curpage: $!\n";

# Read input pages
$oldpage = <OLDPAGE>;
$newpage = <CURPAGE>;

# Close input pages
close(OLDPAGE);
close(CURPAGE);

# Mangle some HTML tags to a form suitable for analysis
$oldpage = &MangleHTML($oldpage, @tags);
$newpage = &MangleHTML($newpage, @tags);

# Parse old and new page
&TokenizePage($oldpage);
@oldtokens = @tokens;
$#tokens   = -1;
if ($debug) {
    foreach (@oldtokens) { print ">>>> $_\n"; }
}
&TokenizePage($newpage);
@newtokens = @tokens;
$#tokens   = -1;

# Parse new page
&PerformDiff();

# Restore tags which we have previously mangled
foreach $token (@newtokens) {
    $token =~ s/\@\@\@\@&nbsp;~~~~/&nbsp;/sig;
    foreach $tag (@tags) { $token =~ s/~~~~(\/*.*?)\@\@\@\@/<$1>/sig; }
}

# Open output file for writing
open( OUTPAGE, "> $outpage" ) or die "Cannot open $outpage: $!\n";
foreach (@newtokens) { print OUTPAGE "$_\n"; }
close(OUTPAGE);

# End of program
if ( !$changeStatus ) {
    if ($debug) { print "No changes were detected.\n"; }
}
exit $changeStatus;

# Convert page to tokens
sub TokenizePage() {
    my $page = shift (@_);
    @tokens = split /(<.*?>)/s, $page;
    foreach (@tokens) { s/^\s+//sig; }
    foreach (@tokens) { s/\s+$//sig; }
}

# Perform diff between two pages
sub PerformDiff() {
    my $commentOn   = 0;
    my $scriptOn    = 0;
    my $styleOn     = 0;
    my $titleOn     = 0;
    my $ignoreUrlOn = 0;

    foreach $token (@newtokens) {
        if ( $token eq "" ) { next; }
        if ($debug) { print "<<<< $token\n"; }

        if ( $token =~ m|^.*?<!-.*?$| ) { $commentOn = 1; }
        if ( $token =~ m|^.*?->.*?| )   { $commentOn = 0; next; }

        if ( $token =~ m|^.*?<TITLE.*?>$|i )  { $titleOn = 1; }
        if ( $token =~ m|^.*?</TITLE.*?>$|i ) { $titleOn = 0; next; }

        if ( $token =~ m|^.*?<SCRIPT.*?>$|i )  { $scriptOn = 1; }
        if ( $token =~ m|^.*?</SCRIPT.*?>$|i ) { $scriptOn = 0; next; }

        if ( $token =~ m|^.*?<STYLE.*?>$|i )  { $styleOn = 1; }
        if ( $token =~ m|^.*?</STYLE.*?>$|i ) { $styleOn = 0; next; }

        if ( TokenContainsIgnoreURL($token) ) { $ignoreUrlOn = 1; }
        if ( $ignoreUrlOn && TokenContainsHlinkEnd($token) ) {
            $ignoreUrlOn = 0;
            next;
        }

        if ($commentOn) {
            if ($debug) { print "#### Token is within comment block.\n"; }
        }
        elsif ($titleOn) {
            if ($debug) { print "#### Token is within title block.\n"; }
        }
        elsif ($scriptOn) {
            if ($debug) { print "#### Token is within Javascript block.\n"; }
        }
        elsif ($styleOn) {
            if ($debug) { print "#### Token is within stylesheet block.\n"; }
        }
        elsif ($ignoreUrlOn) {
            if ($debug) {
                print "#### Token contains ignore URL - $lastIgnoreURL\n";
            }
        }
        elsif ( $token =~ m/<.*?>/sig ) {
            if ($debug) { print "#### Token is a HTML tag.\n"; }
        }
        elsif ( TokenIsMangledHTMLTag($token) ) {
            if ($debug) { print "#### Token is a mangled HTML tag.\n"; }
        }
        elsif ( TokenContainsIgnoreKeyword($token) ) {
            if ($debug) {
                print
                  "#### Token contains ignore keyword - $lastIgnoreKeyword\n";
            }
        }
        elsif ( TokenExists($token) ) {
            if ($debug) { print "#### Token exists in old page.\n"; }
        }
        else {
            if ($debug) { print "#### Token has been highlighted!\n"; }
            if ($asciimarker) {
                $token = "###>>>". $token ."<<<###";
            }
            $token =
                    "<span style=\"background-color: $hicolor\">"
                    . $token . "</span>";
            $changeStatus = 1;
        }
    }
}

# Check if token is a mangled HTML tag
sub TokenIsMangledHTMLTag() {
    my $token = shift (@_);

    while ( $token ne "" ) {
        if ( $token =~ m/^\s*(.*?)\s*~~~~.*?\@\@\@\@\s*(.*?)\s*$/i ) {
            $token = $2;
            if ( !$1 =~ m/^\s*$/ ) { return 0; }
        }
        else { return 0; }
    }
    return 1;
}

# Check if token contains any keyword in ignore list
sub TokenContainsIgnoreKeyword() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    # If this token contains >= tmax no. of words, do not ignore
    $tokdup = &ReduceSpaces($token);
    @words = split /\s/, $tokdup;
    if ($debug) { print "#### C" . ( $#words + 1 ) . ": $tokdup\n"; }
    if ( $#words + 1 > $tmax ) { return 0; }

    foreach $keyword (@ignore) {
        if ( $token =~ m/^.*?(\b$keyword\b).*?$/i
            || $tokdup =~ m/^.*?(\b$keyword\b).*?$/i )
        {
            $lastIgnoreKeyword = $keyword;
            return 1;
        }
    }
    return 0;
}

# Check if token already exists
sub TokenExists() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    # If this token contains <= tmin no. of words, don't check
    $tokdup = &ReduceSpaces($token);
    @words = split /\s/, $tokdup;
    if ( $#words + 1 <= $tmin ) { return 1; }

    foreach $oldtok (@oldtokens) {
        $oldtok =~ s/\s{2,}/ /sig;
        if ( $token eq $oldtok ) { return 1; }
    }
    return 0;
}

# Check if token contains ignore URL
sub TokenContainsIgnoreURL() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;

    foreach $url (@ignoreurl) {
        if ( $token =~ m/~~~~A.*?HREF=.*?$url.*?\@\@\@\@/i ) {
            $lastIgnoreURL = $url;
            return 1;
        }
    }
    return 0;
}

# Check if token contains end of hyperlink
sub TokenContainsHlinkEnd() {
    my $token = shift (@_);
    $token =~ s/\s{2,}/ /sig;
    return 1 if $token =~ m/~~~~\/A\@\@\@\@/i;
    return 0;
}

sub MangleHTML() {
    my $page = shift(@_);
    my @tags = shift(@_);

    $page =~ s/[\r\n]|\s\s/ /sig;    # Handle MSDOS-style line separators
    $page =~ s/&nbsp;/\@\@\@\@&nbsp;~~~~/sig;   # Handle non-breaking white space
    $page =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>/~~~~A$1~~~~$2\@\@\@\@$3\@\@\@\@/sig;    # Handle nested brackets
    foreach (@tags) {
        $tag = $_;
        $page =~ s/<(\/*$tag)>/~~~~$1\@\@\@\@/sig;
        if ( $tag =~ s/\*/ / ) { # XXX WTF is going here with the re?
            $page =~ s/<(\/*$tag.*?)>/~~~~$1\@\@\@\@/sig;
        }
    }

    return $page;
}

sub ReduceSpaces() {
    my $token = shift(@_);
    
    $token =~ s/\@\@\@\@&nbsp;~~~~/ /sig;
    $token =~ s/~~~~/</sig;
    $token =~ s/\@\@\@\@/>/sig;
    $token =~ s/<A(\s+[^>]*)<([^>]*)>([^>])*>//sig;
    $token =~ s/<[^>]*>//sig;
    $token =~ s/^\s*//sig;
    $token =~ s/\s*$//sig;
    $token =~ s/\s+/ /sig;

    return $token;
}

__END__

=head1 NAME

webdiff - Find and Highlight Differences Between Webpages

=head1 SYNOPSIS

webdiff [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--archive>=<pathname>

Archive HTML file

=item B<--current>=<pathname>

Current HTML file

=item B<--out>=<pathname>

Output HTML file (with highlighting)

=item B<--basedir>=<pathname>

Base directory for files

=item B<--hicolor>=<color>

Highlight color (Def: blue, yellow, pink, grey or #rrggbb)

=item B<--ignore>=<filelist>

Comma-delimited list of named sections containing ignore keywords

=item B<--ignoreurl>=<filelist>

Comma-delimited list of named sections containing ignore urls

=item B<--tmin>=<number>

Don't check if token contains <= given no. of words

=item B<--tmax>=<number>

Don't ignore if token contains >= given no. of words

=item B<--debug>

Debug messages

=back

=head1 DESCRIPTION

B<webdiff> will compare two webpages and create an output file with the changesw
highlighted.


B<webdiff> is internal to B<websec> and isn't well documented.


=head1 SEE ALSO

L<websec(1)>


=head1 AUTHOR

Victor Chew is the original author of this software and
Baruch Even is continuing the maintenance.

=cut

vim:set et ts=4:


Node-path: trunk/websec/websec
Node-kind: file
Node-action: change
Text-content-length: 22048
Text-content-md5: 0442f465d5e684f99afeed53f490bd6f
Content-length: 22048

#!/usr/bin/perl -w

#################################################################################
# 
# Web Secretary Ver 1.8.0
#
# Retrieves a list of web pages and send the pages via email to
# a designated recipient. It can optionally compare the page with a
# previously retrieved page, highlight the differences and send the
# modified page to the recipient instead.
#
# Copyright (C) 1998  Chew Wei Yih
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
#################################################################################

my $feature_compress = 1;

use LWP::UserAgent;
use URI;
eval { require Compress::Zlib; } or $feature_compress=0;
use POSIX qw(strftime);
use File::Spec;
use Getopt::Long;
use Pod::Usage;
use File::Temp qw/tempfile/;

# Print introduction
print "Web Secretary Ver 1.8.0\n";
print "By Chew Wei Yih Copyleft (c) 1998\n\n";

# Initialize parameters

$help = 0;
$man  = 0;
$urllist = "url.list";
$base = "";

# Parse command line options
GetOptions(
    "urllist=s" => \$urllist,
    "help|?" => \$help,
    "man"    => \$man,
    "base=s" => \$base
);

pod2usage(1) if $help;
pod2usage( -exitstatus => 0, -verbose => 2 ) if $man;

if ($base eq "") {
    if ( -e $urllist ) {
        $base = ".";
    } else {
        $base = $ENV{HOME} . "/.websec";
    }
}

# Remove trailing slash from base, we will add it ourself everywhere needed
$base =~ s/\/$//;

# Prepare pathnames.
$archive = "$base/archive";
mkdir $base,    0750 if !-d $base;
mkdir $archive, 0750 if !-d $archive;
($current_fh, $page_current) = tempfile(DIR=>"$base", UNLINK=>1, SUFFIX=>".html");
$current_fh=-1;

# Red Hat has sendmail in /usr/sbin/sendmail, but LWP::Protocol::mailto
# expects it in /usr/lib/sendmail.  Revert to the Red Hat location if there
# is no executable at the expected location.
use vars qw($LWP::Protocol::mailto::SENDMAIL);
$LWP::Protocol::mailto::SENDMAIL = "/usr/sbin/sendmail"
  unless -x "/usr/lib/sendmail";

# Location of webdiff, if it's in the same directory as websec, use it,
# this enables simply opening the archive and using the program inplace.
use FindBin;
$webdiffbin = "$FindBin::Bin/webdiff";
if ( !-e $webdiffbin ) {
    $webdiffbin = "webdiff";
}

$htmldiffbin = "$FindBin::Bin/htmldiff";
if ( !-e $htmldiffbin ) {
  $htmldiffbin = "./htmldiff";
  if ( !-e $htmldiffbin ) {
      $htmldiffbin = "htmldiff";
  }
}

# prepare digest
@digest = ();
@htmldigest = ();

# Set default values
local %defaults = (
    url        => "",
    auth       => "none",
    name       => "",
    prefix     => "",
    diff       => "webdiff",
    hicolor    => "blue",
    asciimarker => 0,
    ignore     => "none",
    ignoreurl  => "none",
    email      => "",
    emaillink  => "",
    emailerror => 1,
    program    => "",
    programdigest => "",
    proxy      => "",
    proxyauth  => "none",
    randomwait => 0,
    retry      => 3,
    retrywait  => 0,
    timeout    => 20,
    tmin       => 0,
    tmax       => 99999,
    addsubject => "",
    digest     => "false",
    useragent  => "WebSec/1.7",
    datefmt    => " - %d %B %Y (%a)",
    mailfrom   => "",
);
%siteinfo = %defaults;

# Default return code
$rc = 0;

open ARGV, $base . "/" . "$urllist" unless exists $ARGV[0];

# Loop through input file and process all sites listed
while (<>) {
    chop $_;
    s/^\s*//;
    s/\s*$//;

    # Ignore comments
    next if (m/^#/);
    # Stop with a finish marker
    last if (m/^__END__/);

    # Handle non-empty lines
    if ( length != 0 ) {
        $rc = &HandleInput();
        if ( $rc != 0 ) { last; }
        next;
    }

    # Handle line separators
    $rc = &HandleSite();
    if ( $rc != 0 ) { last; }
    %siteinfo = %defaults;
}

# Process last site if available
if ( $rc == 0 && $siteinfo{url} ne "" ) { $rc = &HandleSite(); }

# Delete temp files
unlink($page_current);

if (@digest) {
    $linkmsg =
      "The contents of the following URLs have changed:\n\n"
      . join ( "\n", @digest ) . "\n";
    $subj = "$addsubject$today";
    &MailMessage( $linkmsg, $subj, $digestEmail, $siteinfo{mailfrom} );
}

if (@htmldigest) {
    ($OUTPAGE, $pagename) = tempfile(DIR=>"$base", UNLINK=>1, SUFFIX=>".html");
    print OUTPAGE "<HTML><BODY>The contents of the following URLs has changed:<P><P>";
    foreach (@htmldigest) { print OUTPAGE "$_<P>\n"; }
    print OUTPAGE "<P></BODY></HTML>";
    close(OUTPAGE);

    &ShowDocument( $program, $pagename);
}


# End of main program
exit $rc;

# Handle setting of parameters
# Params: none
sub HandleInput() {

    # Get keyword, value pair
    ( $keyword, $value ) = split ( /=/, $_, 2 );
    $keyword =~ s/^\s*(.*?)\s*$/$1/;
    $keyword =~ tr/A-Z/a-z/;
    $value   =~ s/^\s*\"?(.*?)\"?\s*$/$1/;

    # Check if valid keyword
    if ( not defined $defaults{$keyword} ) {
        print qq(Unrecognized keyword in line $.: "$_". Keyword="$keyword".\n);
        return -1;
    }

    # Allow values from the environment
    while ($value =~ m/\${([^}]+)}/) {
        if (not exists $ENV{$1}) {
            print STDERR "Used environment variable '$1' but it is not defined, aborting.\n";
            exit 1;
        }
        $value =~ s/\${([^}]+)}/$ENV{$1}/;
    }

    $siteinfo{$keyword} = $value;
    return 0;
}

# Handle downloading, highlighting and mailing of each site.
# Params: none
# Returns: 0 => OK, -1 => Error
sub HandleSite() {

    # Get parameter values for this page
    $url        = $siteinfo{url};
    $name       = $siteinfo{name};
    $prefix     = $siteinfo{prefix};
    $diff       = $siteinfo{diff};
    $hicolor    = $siteinfo{hicolor};
    $ignore     = $siteinfo{ignore};
    $ignoreurl  = $siteinfo{ignoreurl};
    $email      = $siteinfo{email};
    $emailLink  = $siteinfo{emaillink};
    $program    = $siteinfo{program};
    $programdigest = $siteinfo{programdigest};
    $proxy      = $siteinfo{proxy};
    $randomwait = $siteinfo{randomwait};
    $retry      = $siteinfo{retry};
    $retrywait  = $siteinfo{retrywait};
    $timeout    = $siteinfo{timeout};
    $tmin       = $siteinfo{tmin};
    $tmax       = $siteinfo{tmax};
    $addsubject = $siteinfo{addsubject};
    $digest     = $siteinfo{digest};
    $useragent  = $siteinfo{useragent};
    $datefmt    = $siteinfo{datefmt};

    # Get today's date in the format we want.
    $today = strftime $datefmt, localtime;

    # If block without URL, assume parameter setting block and update default
    # values
    if ( $url eq "" ) {
        %defaults = %siteinfo;
        return 0;
    }

    # If essential parameters are not present, abort with error
    if ( $name eq ""
        || $prefix eq ""
        || ( $email eq "" && $emailLink eq "" && $program eq "" ) )
    {
        print "Name, prefix, program or email info missing from URL: $url.\n";
        return -1;
    }

    # Prepare for downloading this page
    print "Processing => $url ($name) ...\n";
    $pagebase            = "$archive/$prefix";
    $page_previous       = "$pagebase.html";
    $page_archive        = "$pagebase.old.html";
    $outgoing     = "$pagebase.diff.html";
    $page_previousExists = 1;
    open( FILE, $page_previous ) or $page_previousExists = 0;
    close(FILE);
    $subj    = "$addsubject $name$today - $url";
    $webdiff =
"$webdiffbin --basedir=$base --archive=$page_previous --current=$page_current --out=$outgoing "
      . "--hicolor=$hicolor --ignore=$ignore --ignoreurl=$ignoreurl --tmin=$tmin --tmax=$tmax";
     $htmldiff = "$htmldiffbin $page_previous $page_current > $outgoing";

    if ($siteinfo{asciimarker}) {
        $webdiff .= " --asciimarker";
    }

    # Download URL using LWP
    $ua = new LWP::UserAgent;
    $ua->agent($useragent);
    $ua->timeout($timeout);
    $ua->env_proxy;
    if ( $proxy ne "" ) { $ua->proxy( http => $proxy ); }
    $req = PrepareRequest($url);

    # Try up to '$retry' times to download URL
    $counter = 0;
    srand;
    while ( $counter < $retry ) {
        $resp = $ua->request($req);

        if ( ! $resp->is_success ) {
            $counter++;
            if ( $randomwait > 0 ) {
                $random = int( rand $randomwait ) + 1;
                sleep $random;
            }
            else { sleep $retrywait; }
            next;
        }

        # Leave if there is no refresh header
        if (!$resp->header("Refresh")) { last; }

        # Handle it if the refresh is for zero seconds
        ( $time, $refresh_url ) = split(/;/, $resp->header("Refresh"), 2);
        if ($time > 0) { last; }
        
        # Convert to absolute URL and refetch the page
        ( undef, $refresh_to ) = split(/=/, $refresh_url, 2);
        $newurl = URI->new_abs($refresh_to, $url)->as_string();

        $req = PrepareRequest($newurl);
        # Don't reset the counter, we still want to protect from endless loops
    }

    # If URL is successfully downloaded
    if ( $resp->is_success ) {
        # Check if the data is gzip compressed, decompress if it is.
        if (($resp->content_encoding || "") =~ /gzip/) {
            my $new_content;

            if ($feature_compress) {
                $new_content = Compress::Zlib::memGunzip($resp->content);
            } else {
                $new_content = "Server sent gzip compressed data, and we are missing Compress::Gzip";
            }
            if (defined $new_content) {
                $resp->content($new_content);
                $resp->content_length(length $new_content);
                $resp->content_encoding("");
            }
        }
    
        open( HTML_FILE, ">$page_current" );
        print HTML_FILE "<!-- X-URL: ", $resp->base, " -->\n";
        print HTML_FILE "<BASE HREF= \"", $resp->base . "\">\n";
        print HTML_FILE $resp->content;
        close HTML_FILE;

        if ( $diff eq "webdiff" ) {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage ...\n";
                $rc = system($webdiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email,
                            $siteinfo{mailfrom} );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage(
                                $linkmsg,   $subj,
                                $emailLink, $siteinfo{mailfrom}
                            );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        elsif ( $diff eq "htmldiff" )
        {
            if ( $page_previousExists == 1 ) {
                print
"Highlighting differences from previous version of webpage using htmldiff...\n";
                $rc = system($htmldiff);
                if ( $rc != 0 ) {
                    if ( $email ne "" ) {
                        print "Sending highlighted page to $email ...\n";
                        MailDocument( $outgoing, $subj, $email );
                    }
                    if ( $emailLink ne "" ) {
                        print "Sending link to $emailLink ...\n";
                        if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                            push @digest, $url;
                            ($digestEmail) or ( $digestEmail = $emailLink );
                        }
                        else {
                            my $filepath = File::Spec->rel2abs($page_previous);
                            $linkmsg =
"The contents of the following URL has changed:\n$url\n\nIt can also be found at:\nfile://$filepath\n";
                            MailMessage( $linkmsg, $subj, $emailLink );
                        }
                    }
                    if ( $program ne "" ) {
                        if ( $programdigest ne "true" ) {
                            ShowDocument( $program, $outgoing );
                        }
                        else {
                            push @htmldigest, "<A HREF=\"".$outgoing."\">Changes for ".$name."</A>".
                                    "&nbsp;<A HREF=\"".$page_archive."\">previous page</A>".
                                    "&nbsp;<A HREF=\"".$page_previous."\">current page</A>".
                                    "&nbsp;<A HREF=\"".$url."\">current page on the net</A><P><P>";
                        }
                    }
                }
                else {
                    print "No changes were detected.\n";
                }
                rename $page_previous, $page_archive;
                rename $page_current,  $page_previous;
            }
            else {
                print
                  "No previous version for this page. Storing in archive ...\n";
                rename $page_current, $page_previous;
            }
        }
        else {
            if ( $email ne "" ) {
                MailDocument( $page_current, $subj, $email,
                    $siteinfo{mailfrom} );
            }
            if ($page_previousExists) { rename $page_previous, $page_archive; }
            rename $page_current, $page_previous;
        }
    }

    # If unable to download URL
    else {
        print "Unable to retrieve page.\n";
        $errmsg =
          "Unable to retrieve $name ($url).\n\n"
          . "Detailed error as follows:\n"
          . $resp->error_as_HTML;

        if ( $email ne "" && $siteinfo{emailerror} ) {
            MailMessage( $errmsg, $subj, $email, $siteinfo{mailfrom} );
        }
        if ( $emailLink ne "" && $siteinfo{emailerror} ) {
            if ( ( $digest ne "no" ) && ( $digest ne "false" ) ) {
                push @digest, "Unable to retrieve: $url";
                ($digestEmail) or ( $digestEmail = $emailLink );
            }
            else {
                MailMessage( $errmsg, $subj, $emailLink, $siteinfo{mailfrom} );
            }
        }
    }

    return 0;
}

sub PrepareRequest() {
    my $url = shift (@_);

    $req = new HTTP::Request( 'GET', $url );

    my $auth = $siteinfo{auth};
    if ( $auth ne "none" ) { $req->authorization_basic( split ( /:/, $auth, 2 ) ); }

    my $proxyAuth = $siteinfo{proxyauth};
    if ( $proxyAuth ne "none" ) { $req->proxy_authorization_basic( split ( /:/, $proxyAuth, 2 ) ); }
    
    #$req->push_header("Accept" => "text/html, text/plain, text/*, */*");
    
    my $compress_options = "identity";
    if ($feature_compress) { $compress_options = "gzip, $compress_options"; }
    $req->push_header("Accept-Encoding" => $compress_options);

    return $req;
}

# Mail message
# Params: message, subject, recipient
# Returns: none
sub MailMessage() {
    my $message    = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",      $subject );
        $req->header( "Content-type", "text/plain; charset=us-ascii" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($message);

        $ua = new LWP::UserAgent;
        $ua->request($req);
    }
}

# Mail HTML document.
# Params: filename, subject, recipient
# Returns: none
sub MailDocument() {
    my $filename   = shift (@_);
    my $subject    = shift (@_);
    my @recipients = split /,/, shift (@_);
    my $from       = shift (@_);
    my $tmpstr     = $/;

    undef $/;
    open( FILE, "$filename" ) or die "Cannot open $filename: $!\n";
    my $content = <FILE>;
    close(FILE);

    foreach $email (@recipients) {
        $req = HTTP::Request->new( POST => "mailto:" . $email );
        if ( $from ne "" ) {
            $req->header( "From",   $from );
            $req->header( "Sender", $from );
        }
        $req->header( "Subject",                   $subject );
        $req->header( "Content-type",              "text/html" );
        $req->header( "Content-Transfer-Encoding", "7bit" );
        $req->header( "MIME-Version",              "1.0" );
        $req->content($content);

        $ua = new LWP::UserAgent;
        my $resp = $ua->request($req);
        die "Error mailing document: ".$resp->message()."\n" if $resp->is_error;
    }

    $/ = $tmpstr;
}

sub ShowDocument() {
    my ( $program, $outgoing ) = @_;
    my $status;

    # special handling for mozilla, try to use remoting...
    if ( $program eq "mozilla" ) {
        $status = system("mozilla -remote \"ping()\"");

        # print "Status after ping: ".$status."\n";

# if ping() returns ne 0, mozilla is not running, we cannot use openurl()
        if ( $status ne 0 ) {
            $status = system( "mozilla", $outgoing );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
        else {
            $status =
              system(
                "mozilla -remote \"openurl(file:" . $outgoing . ",new-tab)\"" );
            if ( $status ne 0 ) {
                print "Running mozilla returned status: " . $status . "\n";
            }
        }
    }
    elsif ($program eq "konqueror") {
        # konqueror from KDE has a small client application that helps with opening urls
        # run 'kfmclient --commands' for help about the available commandline options
        $status = system( "kfmclient openURL ".$outgoing." text/html" );
        if ( $status ne 0 ) {
            print "Displaying URL in konqueror returned status: " . $status . "\n";
        }
    }
    else {

        # other applications are currently just started
        $status = system( $program, $outgoing );
        if ( $status ne 0 ) {
            print "Application " . $program
              . " returned status: " . $status . "\n";
        }
    }
}

__END__

=head1 NAME

websec - Web Secretary

=head1 SYNOPSIS

websec [options]


=head1 OPTIONS

=over 8

=item B<--help>

Print a brief help message and exits.

=item B<--man>

Prints the manual page and exits.

=item B<--base>

Base directory for configuration (~/.websec by default)

=item B<--urllist>

Use another file for the url list, by default it is "url.list".

=back

=head1 DESCRIPTION

B<websec> is a web page monitoring software.  It will send you a changed web
page with the contents highlighted.

The base directory is the place from which B<websec> will read the config files
and in which it will store its data.

When called without an argument, B<websec> will look for a base directory.
If the current directory has url.list it will use it, otherwise it will try to
use I<$HOME/.websec/>. You can also override this process with the I<--base>
option.

You can add a line like I<AddSubject = [websec]> to url.list, websec will add
I<[websec]> to every subject as a first word when mail is sent. You can then
easily detect this line by a mail filter.

The keywords I<Retry>, I<Retrywait>, and I<Timeout> in url.list lets you specify
the number of times to retry, time to wait between retries, and a timeout
setting.

B<Websec> waits for a random number of seconds between retries up to the value
specified by the I<Randomwait> keyword. This is to prevent websec from being
blocked by websites that perform log analysis to find time similarities between
requests.


=head1 SEE ALSO

/usr/share/doc/websec/README.gz, L<url.list(5)>, L<ignore.list(5)>, L<webdiff(1)>.


=head1 AUTHOR

Victor Chew is the original author of this software,
Baruch Even is continuing the maintenance and
Joop Stakenborg <pa3aba@debian.org> provided this man page, 

=cut

vim:set et ts=4:


Revision-number: 170
Prop-content-length: 131
Content-length: 131

K 7
svn:log
V 30
Tag version 1.8.0 for release

K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2005-01-02T01:53:31.055354Z
PROPS-END

Node-path: tags/websec/version-1.8.0
Node-kind: dir
Node-action: add
Node-copyfrom-rev: 169
Node-copyfrom-path: trunk/websec


Revision-number: 171
Prop-content-length: 122
Content-length: 122

K 7
svn:log
V 21
Release version 1.8.0
K 10
svn:author
V 6
baruch
K 8
svn:date
V 27
2005-01-02T02:06:16.387745Z
PROPS-END

Node-path: trunk/site/index.html.wml
Node-kind: file
Node-action: change
Text-content-length: 6203
Text-content-md5: 1dcc11f12dc885a19c588ce572cf172b
Content-length: 6203

#use wml::templates::be-tmpl
<subject "WebSec - A Web Secretary"/>

#<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />

<topic "Summary">
<p>
You can download from the 
<a href="https://savannah.nongnu.org/files/?group=websec">Savannah project file section</a>.

The latest versions are:
</p>
<ul>
#<:listversions():>
<li><a href="websec-1.8.0.tar.gz">1.8.0</a> - <a href="websec-1.8.0.tar.gz.sig">GPG signature</a></li>
</ul>

<p>Contacts:</p>
<ul>
 <li>Mailing List (Preferred): <a href="http://savannah.nongnu.org/mail/?group=websec">mailing list</a></li>
 <li>Baruch Even (Author): <a href="mailto:websec@ev-en.org">websec@ev-en.org</a></li>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
 <li><a href="http://savannah.nongnu.org/projects/websec/">Project Page</a></li>
</ul>
</topic>

<topic "What is WebSec?">
<p>
Web Secretary is a web page monitoring software. However, it goes beyond the
normal functionalities offered by such software. It will detect changes based on
content analysis, making sure that it's not just HTML that changed, but actual
content. You can tell it what to ignore in the page (hit counters and such), and
it can mail you the document with the changes highlighted or load the
highlighted page in a browser.
</p>
<p>
Web Secretary is actually a suite of two Perl scripts called websec and
webdiff. websec retrieves web pages and email them to you based on a URL
list that you provide. webdiff compares two web pages (current and archive)
and creates a new page based on the current page but with all the
differences highlighted using a predefined color.
</p>
<p>
For example you can look at the Web Secretary page as it was monitored:
</p>
<ul>
 <li><a href="websec-homepage.old.html">Old Page</a> - The original page before the change.</li>
 <li><a href="websec-homepage.new.html">Modified Page</a> - The page after the change.</li>
 <li><a href="websec-homepage.highlighted.html">Highlighted Page</a> - The page as WebSec sends, highlighted.</li>
</ul>
<p>
Personally, I put Web Secretary on crontab to monitor a large number of web
pages. When the highlighted pages are delivered to me, I use procmail to
sort them out and file them into another folder. Sometimes, when I am busy,
I will not have time to accessing the web for a few days. However, with Web
Secretary, I can always access the "archive" that it has created for me at
my own leisure.
</p>

<p>
 The man pages can be found online: 
 <a href="websec(1).html">websec(1)</a>,
 <a href="url.list(5).html">url.list(5)</a>,
 <a href="ignore.list(5).html">ignore.list(5)</a>,
 <a href="webdiff(1).html">webdiff(1)</a>.
</p>
</topic>

<topic "Installation">

<h3>Are there any dependencies?</h3>
<p>
Only Perl 5 and LWP module which should be standard with all Perl distributions.
</p>

<h3>How to install?</h3>
<p>
Simply unpack the archive and modify the configuration file to your hearts content.
There is no GUI to configure this program, it's all in the text files.
</p>

<h3>How to use?</h3>
<p>
Just run the program and it will do its magic, the best mode would be to put it in
a cron job for automatic daily work, this is great if you are connected all the
time.
</p>
<p>
If you are connected by dial-up, you may want to make it run automatically upon
connection, how to do this is different between OSes &amp; Distributions so
exact instructions you will need to find on your own.
</p>
<p>
If you want to have different sites checked at different intervals you can
check the way Jani Uusitalo made <a
href="http://www.mummila.net/varasto/tekstit/websec-cron.html">an advanced
setup for websec and cron</a>.
</p>
</topic>

<topic "How do I get help? How do I help?">
<p>
You can subscribe to the <a href="http://mail.nongnu.org/mailman/listinfo/websec-users">mailing list</a>.
Post messages to request help and offer help. You can suggest ideas and even provide patches to implement
them :-)
</p>

<p>
<em>Please share with us the web pages that you have monitored using Web Secretary,
as well as tips and tricks for maximizing the signal-to-noise ratio.</em>
</p>

<p>
There are several other facilities to help each other:
</p>
<ul>
 <li><a href="http://savannah.nongnu.org/bugs/?group=websec">Bug Tracker</a> - List known bugs</li>
 <li><a href="http://savannah.nongnu.org/patch/?group=websec">Patch Manager</a> - Upload improvements for the benefit of everyone</li>
</ul>
</topic>

<topic "Authors">
<p>
The original author is Chew Wei Yih (also known as Victor Chew), with the help
of several contributors (see the README file).
</p>
<p>
<a href="http://baruch.ev-en.org/">Baruch Even</a> picked up the program when
it was mostly unmaintained to give it a new public home on 
<a href="http://savannah.gnu.org/">Savannah</a>.
</p>
</topic>

<topic '"Competitors"'>
<p>
  Maybe you are not happy with Web Secretary (please tell me why!), or maybe you want to look at
  other options, it's a Free-Software world(!) after-all. Following is a list of programs I've found
  that are somewhat related to what WebSec does.
</p>

<dl>
 <dt><a href="http://kwebwatch.sourceforge.net/">KWebWatch</a></dt>
 <dd>
  Based on Web Secretary, but written in C++ and with a KDE GUI. Has
  renewed development and offers a KDE3 user interface.
 </dd>
 
 <dt><a href="http://www.antbear.org/urlchange/urlprj.html">urlchange</a></dt>
 <dd>
    Checks only timestamps and not actual page changes. It's main advantages
    is being written in Python.
 </dd>

 <dt><a href="http://sourceforge.net/projects/wrep/">wrep</a></dt>
 <dd>
   A program to extract with regular expression interesting parts of pages,
   doesn't check for changes, it simply extracts interesting parts.
   <br/>
   There appears to be two wrep's, from two different authors, with totally
   different methods of work.
 </dd>

 <dt><a href="http://lab.madscience.nl/wrep/">WebReporter</a></dt>
 <dd>
   A language to munge webpages into something else, can be used to create
 a summary page out of changes from other sites.
 </dd>
</dl>
</topic>


Node-path: trunk/site/websec-1.8.0.tar.gz
Node-kind: file
Node-action: add
Prop-content-length: 59
Text-content-length: 30778
Text-content-md5: 54d6b6d1fcaec2147b8ea257c6bc6658
Content-length: 30837

K 13
svn:mime-type
V 24
application/octet-stream
PROPS-END
 eTA [sGrbS) E((H%\v]x?BW=.HSKVx4D`{}9<n~Ol<}=y'y~R:	/Yto+pogw(.Lbtabj{NNG[uY-Y^IN&6-g5>R<=UlfPT1Ff`J--v>t4+2Oj&:7Yser4s7f0DGF]zJ=-.q+d,&LOhpsJjllnTU7>"(Y<$pHx/>F&&N[Uocn);Q(T)}81\JIx[S(y&~Kum_Ar3CD}e9i7YdK|bk*]`OyL+.4Pw<aRYnUt^&.9iz'<&oao/L
19\A#U^P-q*)"s;^gz0{$h="+,-(DRqYNu`QB8|JT<<>up"S+{fIl>0	="{lqW:70QWn]0j0
B6_U?>v/igOWq~p.La@uqyz4Vvf{TfDLKVT\h`~Qo$7~gx5iM."kp/E8@eS8P,F@F<s:A#
i5`b\b66zp7gf+P%1g" n3H i?3U`[g`TIol86 :)2BywYyFo%8KK?I2R/vL4q6_/FU_10=^&KYQ"86<	L+uk]7gWjbtY&?}9QX5KAH!"|E@_Jk% K) M	@h0G^HkLlh'65`V_"r{~44"Fs%9
oTX|_4xF!\fK|rT|gK$:^ieNd XXT9r*WY%i}Y[Z\hOi$N tCLOT^# 
kW=W?^[iH<QpoJ04}@>An%
rMDJ 
E`DS{k3k+T#j5*
/L\s?PwUrD5qg:Yl}a"Alq,)TtYZudwYx*QB=p4dqe>2*l)wHqzR.~iD w2SaX#+0qviwO^pIijzz
gK OE X.yX
W.g&V:-uD%qQ{F]N^*5@/e2EhS
b
'<r MSh,hnK%y&$h@5EexRnDRwa.Unn2.$P3:2 r:}$1~U5zOHj54JX4^7u4E$'owoooEEK*u9prWQ/L<p:~RT]Wap,wli?,Z\jtI*q48 .-HbK)(TJyF:/?M3C'"Y]x2\_SHyEy_<F)#u[U<Yp+E*xo]vKjo}\3zztK5_kzLN><[f+#"I1%6f]
S9\m]{
:]LI
%2A(|&_jl&tzJCa$u|]IJgQn_?5/yR`1
Pb&6ymyhDFFi~l>=lgy(]@=}|`*@ b#Z" kc#@kd+1RO$705K+gjw4b^C]mJ} Mu
4JdLKB036u
S]z/!1!Xq	@XL=dp}fWw3cYdfg(rSzYTP.i3709LB%2jZC7guI387UzZm$V(C`nk)Q cR9`_9fEQ'=_Ben'Y
lnvyq\Ua]&763f%   pau[M9gvGHSR^,W,'	!P:Nr`Xebp;0VcX`:>0Uc+nSQzsBjL*NE*V/j
WT.?),NNLsB/jP{P-7FAg@le'13=22&cW
3j^z .bM9,:1@(eKU]LTm-1YQUi1;}=3d*
0m*Z3^+zul#YvjSI]Udr	@EbpWo_cI9Z.Rf	SQOr<_
I>qw0*VsScO<'EIVH:?	pX<N]+|}ss*y''1;*}VtVa#.S_.beOd]<P| ^Igq=V1ye.aY(M%D'-f#\3zV#jD#MpvY0l\Ql	|ym=ZoV9+<U?W652E2H#w	Hq.B?ld=B=YwIjk-Y\P /3i@#Md=JL.^G`;)IDV+nFy],P8~px&iJp<e
I/gVmy "W@D^RE?|DFfO#uvIr rD9km*heRO{X8];C"z598IOYS^^"bVnqw"_%qcdi.]bA_v-}J83_lgcimvqp4DyhVRy[lgtq(KL(A);(!l8-fElti<U@0L*U_p0#rAdg	Sq;J`z[w1Zq@aPW$	6`?BG$(:\?{ph`pgw1~t<,J\}qM! 
O?i*b:!F1=7AiT#%x2e	v.C $e\,u-S`{JD,|` a kF8% }ir#o*P~P2Zr8^%x~^5Xf72:2vW{5lgx]b/9! 9K0/Vd @Y9OhuU}rIbJs8LXL:#?v)"iQ2~L=wBnd	3Z$J~=U%e'LEn\[LP(m/7xK1]p}tm%L5"]V0,K5J}ED8+-wngn*;noCp/~7 PPS<5/(Snzvot*WysX`-|)snaG'3YypT0|?	:=e%p1wE)$HVl2[(MxH RzCTtfq&n-iQ?_Vy=q0eLgvCz\`=|}Jce~xnrQ\sm!.grIbt%O%EJeO'Q_8x3`'[$JD~xW%jPGgkSVyF2^6&{/:'|/dtU6|)s\gbwIp8v!|wy/vbL^^W[`euaK739R9 XFZ- M%p..G
T 
"!I/t9YYm-^u^{$" yJ'Z;@]);Kcs#*Kq@$Ix%[J fU2yK*F]oz%Q~BD@,"t-"A+o>TwoP`US	iLH7b%oDo,[[h~WqC{O{.?9mg]vz{J~#RL	~]M{[:S(=yPs; r0O5Z`xr&V)?_yo=Cv>>8p~\{_G7~|~/b"`mb'd ZBf$6yg4x1{WWWWW}k^,]VMSX_CN"\%Im5,"u,gC:$-4CM:DtoT: H$aK]_IC92\i[(|T[+-4Q6m>PW,{rn=iFq]D|jq.~zl3nd|O6Mmre&h#G@-?z{R}?>6#!S^>}i'68sFlo]msda,/jjD,dsZ0CJx0O;):N\BCQ^wAATEM<,Uxa&R#E.j3qwG,or	QO:v-T%QE|ks!-I*I^%~8auuqWU$AY73l@|yqFGK-q
cuS\)m:s?%ums/l.8|}bzSIuR\Us&~kg(ND	3Y{ig}jdT8(R]^$OKGzmU@1E~Gj[p[4%K>@lMES
mY}`o6qk6eP,mkyuA>Y  @k10hIa.-E/SWBZop+;$3dfYoVX?-4"L5''tk47w-
RV_WCVn*n~:,=foq}?=|+!*6HJUQE?_"&Vl?	(TjL~3`$Qmbec7tN4{tF.-!XW5*_&w;7>FIY%lEQ5NP f_2.-uSS?}B |9=NSOQ9^gl9f1qVqMRmu{!N56m6V<9.d:/s|a&|{f@GH](s9|.<{frsjgs?|jAIth9zb= ^)Sq'y~yDoO	Sd/0{GIlb eX6h1J(:?/67g36|wbzhPQRGNhYfZ&BS.X|Tx;5mW-0
't)@q/'tNR}dU?{joGG~4:J~JP]|{f2$o(q?`A<7Jw2+xTz%NeC_	QguQGr<^RtdJ`Pt[Tw28#ZPya{T(a7&ld'G^5$?--=ZZ|o?y~o:{_A~	6PM3'
}.r-svk9' SF&+KF~958uKXu>jA
D" 3OZ=EsHpNya{3o:X\xR-57"pfSW52w:-eMSQ+[Ukfj;$0k0(s	k &x	3H;KX>a%		iQXpn8e]u$;8H.=;cGhM3j4~<M/A't^Pt3V[w<4,46yp$~Xpw_x1NZnV}P$v#)Ya9v?i(3ZP}c[8m3s	y0L@u&+4<xP	m2DfY'~psggK'*W&:YH^Q
uFNF]3,|okwjoO2>cQ[d 9-Fz=41-j;>4K6mo8?tG dl/`t1{L3QQQ NgsdIZ<hSh({1loind{2? oQ4f"x2 *Ooj|Z393'$b?zI}4V.u{+\I`
4N3"cMO1VkrL`j}+ m[Jh<:M0o<kAh= r50&pBm*r Nm$:QAGAT<j!`SJ4"B./=PkGNS<cWgyn1.p	&;T39'DodWCwA)XleU`''["'GQ{h| x63L"v$T\vv
em0 a
['S  PSkpNqio?k <Bvib+G2S4XqaiPOxy|	13xQDw)&\8$ jg	8(3bv1
@YlhsUn),`@iuOHJh2n:a@)m$dCvHnDd.;Ln'(!T_GHsyV28vHZ2x<~$Vf{[@=~<2~c+aGc~S	r7Uk,=G`yYol`Ap7kQ\Hb	BO8T2H\b}H+m%hVQV&$SZsP~G$VpNVOj|p}]Uz}b`.,khq4<M~ A{{XB`
8X89o]Fpf{oQ_P.7x-|t(`"5<!kn;ziZX +&+(2FccS*ZW?HTH'Q[x
H8@FuojZU9|uvOW>j+Zgey{{`P\.>1Io5N6GYjiZ{I_%n_.%M_k'VJ3WxZWo[sH<x	wK@3$ <+{3"qo^tpgn~=e"Q-@l&$`!r#F_/CS9j M|km1: R}," 6=v5^	c7$&!w?3EjY	=qz
@ s4[E6WsC	VosHH|B97;K"xaG
_+esP p&	}E&d	9$hHT81q)w"|ED SPLz!@eIEmZ`pNg1 =Ji y*7Hma=&H7haq;du<dtC S9RV|8>K$H1SV	140toHj`ggUc!v++y	c=q/{tN
-&8|M_FYsK=Q{ fs~J2otdad~;Q.n(JeNhYj6Cv1 6??u_^z1O%?#apD)X}#rn/Sr	pFyyo#(7(;w`ghj*]nVW,<\;>vrZ@sPoU;l^%sn|Y.YU$LDap2f:}\}R>B'r)b9-u-|pH>JBGtV++TZp ='A@Ru3:2vRVjBM	:-t6iW)!0`^@e(\;]YeCk`c]u/Hr/pe1Y\.sOQ"4w4[MxQc#<p@/2e
	mF
R*;Jz}[*
-:T@/@6\(Ea~BH8/L9d9K0/
FAL?KX>"eT2+jc=|/l8[@XBEH9;w1+87Sr5FQrM/z#{urBY
 sg%F'3[:5KrIl,CSdD@&}yrEu\_|`!?;nT#G"d 7ACg~wS[=VWMz%v]~%qh5J9E$
J``'?WO[&~:>-_\^,!]W/XD.9*Jtg0!F0l5?8]V&^?uclZK1:B
GudZ&?(d]A{#*8H.ZVV8\ ;T>b,-vL.n82&h'7i1r	u!i_VGOymPzgw+uG5i.4"VL,N5oK-[I&@@Ud&jm5zv\FAVGPRh]=[ws}WU!\j>M9~=\-Z|wX:g{K5F>sH ay+3Kh8US n-n<yJJrfPBw{sp4B[Wru^\R<bwOZmN':.bsN05bAL),y{W"-Id<h1z [3/~+bX
|~)k()8Q'bJgU!0:fO'&QHeemq3z!'>@wgm@) xolRi*Fd+}_EbuCTUGV#GqcS5d8PsKF>^`zDA_#i}ZW,<Cy50IsOZVS{2ZPlpb0=;\Il>/YaJ;\A^8NVEt9_.%U
:xd=~h-s$74$@}VoT&R-DY)]+7P,]}vYV=j9@O`e(}2-yA	N[~FpwZHWK#4s8D3D_z<\
.`wG7kX?_z<##U|rNU!Oow^dQat)'>\R!p=nkp}JoI^I3o2=9F8sAW@Rv]U>
q<Oenlw^aQ!Fe?=tD
(R5nM+j,j$qw|:CK+rig`yCBC4vw%sYJT$hAX|^$Z=WDB<
mfd)zr.87hrE RX.k>?J^#|0[@jiUJ|fAQPBVpd{|g:,ru3"j3t6C:EiN4dSii
-d3abshlF9yXqi6i~0\m7dH7-tj2S-f8'Tr6Xgv~mh+SLj[r:-"@C1`Iv
d
!|P~^wX:eS>P/$:Uj*kT2+wb!H3b#*(vT!xp*[Cx;g/7'*lIBBq1)-|)yt@x7*!=,hT-,o}:*g6[~76|,DbXL=K-4	{WIkN+=iK&ijgk(4au[I9HT6'{xP>DEY*ij%1|j_ t} LH7vE6Sl9`xx>mFb>fXgq6mw	0&&^IH>"I7b]%AS/	fI9?+A2vx?/NA$!LnM{18m4@ZNL'2Q:S\Axe>8GNg&!oxlqINuQ#vkH`hW9@qSd*;2{[7AIRV"p"fBGc`+1C1!+\P Y@#6wCkAQI.@/'+%:Gi7?/V8aG:O^WfMKXN.K=$B
6q`\rsw&HwR Nr-}`\sbr8`u?#DN(tE`><4Tx6fiLahu!ghf*;dZ!.:Lx5_VDguZ=iD.Mf6?/yl$]|w>N'K|C{=soy!i>yJ{K;fNAiz'0#'aYa/4=wP8qTwcsz%|h[Rj[X _{(ps`/vPam#K~\a
;L<oZ;$+8NAa)fH=9O>g_~u?!jS0d9Dj}H(cP@4Z@pIX+)tli@Rp3X5Q3^$)HURr23,dn3~|>O1I[IGG'&>NS6i|\M
^DI{<OA./QkpeVLyu {TWb{w7\sc~cyq!Z?Sx:j4E,.,?_R_?7?(SPvwowtTy$69Khd(6Z78PUw^x!+C#WJ]y(*7A&g/)5kv9'wa~;km`raID 	z+?]Fu,'_Q L@HrXQdq~]#< gcAc+o3#SbCdMj`%)[,fHtk@b%O==Z# n6yr/NNy)F1C_M:l%R#:Zwb gWpjM)tpsd1(Y 5 zOkk}6 6cz;bGO/'/(@Yt<~B13J)X	+:p^	J!+pQj8(+A7C/=4[ut<#j~dH|>l!_tkq$qeI#BKbp_J[HWx#0YTI<"c]._2'gVgNyNH$^zC05-5elcjbYfb7\BRSAgzvDl,`P@f|tn F
6BDHf4!6O'[_1p/8"+IwE&t$balMapqv":8 [mGVp3:1J!vC{sl}H[$q&a|/z>;as].:/o/i?)u\ZZ\z-=x|x
aQ{:1j+||#7C<S~_%rKlO"c2<9rE=CY%{yy||nkqZ @K|gx:og{B=J1	P
GlDK+-G+4BD}zR++E~Rs\B`lRK%JzI[Z$\
++B@_wV]niWU"njp++"	QM~(7Z/sqo}WN}$O.s[U\5~= X830$*Z\gzk"_FIq4&{gN2FxdyT[
~O?5JqoyA._&VaK s\%C187:lx()n7>}!3uxXKa[z$_3XO *
T7_o-sQ	N7X07iY`Ou4EuI4O=Dc}o@muT;
_7O;?7\{%7VezH"afv|RH>VLqw<XY1DG":\fVJT':`5Y(mq$HaThsw'"AW} ajJtu1`c}I\Sy6hCsNqhQIV6q*ae2U^Djped>;Ji!Ui+devd-AIT8x0H/rp+qJEpl|eNb^{ s6gY?2`V&ojM/C=onRx+7v!6VF>1caQTB{/9bd^Ilo=,w!X:^:=;r3sr."1~'o-5[VO(_>	|Vj
}`6R-QQT&ha&#V?IW-[0v
kyXHO>==+N|8'`lh8c[]:i3]r!	aow[UMz@q
zwLzyol.f*=9IGOj{\V$|
]
>biJ6`XeWW4X>##X"'Yp(+,k[A(rJ)U24\1j(5$`e/{E>%_5P:T%V9r_{&S^w.)-BU6`?5|m'YtdB!Oe5,n=~qt+k/7uI.[xKY0JP-nZwtj2[:8_lkx\qb4FPeN,rcAEIW5p)]tG}8BiAa[UyK	naXm~m9Rk6$im65a0+R6+G. rs^
cN_rbsU_<kk_.4W@fW^*:u)&7_@-!sytp`[GV}hGOQp ]X'wpG4\]xoapPJ!!aoja\eD4kg+7>fi]X5_3.)HXMEE*{/l5t^Drn-}>K^
$%DxmI|K-/{u>3"_a!l<@u!;3$/^1KK^K~BQQ/tA4T:KAaI"G;)K,;8RvyG	0ya_k,kS\s4iH{Ez_WM]"9]&^`(Mim6?np-M!"(+ sHwR&XCA@s 7Gf-G
>zaVrc/3|ei=.0u$]#QcX{$yD0
YaU]cB:,#2ge2`e[C"%ft5|1i=kr/Us/R.rGxzl[j=
F^B+=Nch=Ui4t[w`!xZmpZ~YgjD|<|k7(G{fhe/](u!"k+p_sjk7OH\/wFJ~Ye?=^f/,F*H)"9Tdo3?yhhJ)vU_ ).5&.!im:nu}u^VF&xl8YsWt\Z/?;0BL'LN]E
M $/tS/I+5_EDpeE"gju w"`!"~jk
3.sT/5St0`s@nk+ew#<DWzKUWkN4E(z ZFdtN3u#qaF^@'SkSAP4P?_1?UGUPoR%	7tmVqvDT4f(]^4+ya<|6Qv
kx=SEax%~A5e\^o.gu~0Ow?*qR)cA~[*z%P
C%Cr}#'fy[/w?K_
 GXFK\/un\Py+x|{>f!N2S6f>_O2e:UJab&EJ2N)M8?!jYZoz$L #Q/Kk]:uVJUrFMIjWHuh)U<s}I37es+n0fR+^8F`EJZ-+oueBrp}7C^W)jJK:#]Ks/E7k@;ABoZUit*U.\7snrvCV,Fn:+{cy'Kyl	_Dz^I6[N.Mt;G,9#jn}`,7k} ~eh5_/v3R_*JYRUb;Vy>[V	<>l/lV\1dVY&gP(6}	K
Te0[.I))rm\ ]+aD1(9(}>paL;tHx2U23+.F=W93aW!'WeY0u++7}xORHr|z2@8Jjf>$a+\,w`	3l3gi_)=DFQ7|`:hH~\\egV}SB(BCPZ@=T}S_:>k9'eK`&:|MwN]V)^q+`U[[|Y]+!:Fg,$m)eu
vTXMy
fe!*0rf>cKN;SJV!h&i<YbSA9I4JO$R*qrU"Y,5KX%FFbF6[)bomq>j8_2VxX<$`oWq_O`+".m_8M3]J=sO[.@#4H
9K*UV
"mwk,dnRY;N~%?GD%|[86gQ	"Q14yJ[q\.1*lu(.]0")5GQ(H+#zC!qZ+Ym<]A/v}_5:MJ~-wBCc3+CArP5"LS zXy;^.VTtq$NV.Gp&hR`BEj4&qc8u0@yMfff3"a9~t$*D*Olj2XcFYG4;LX6;oMoW'cy(wov^t\D~`/#zgZO`.HFM?QeJC@D]| L}C5"Q/IA0 F#!H0d(7<+CE3)48+GefP: xp 'wa!e8aH5kMYKT 5_ !BoneKNYrQM/nE~_EF- HQNHtlIFOECEhW k*Q*i%$m{=J:ke~BfmW1dA RD7 J+Bt1uhM &
 JKa9vK7DF!KM,F9,//Gu#eGn#^YDb:#VC/<d1~btVjYTXkliVTAz9(0tu2	89q]/>GqM@b\0\nA/-=zP~-A&y-cN9	kfjb
^r{]O'7b}6<I8m:u6C*zs2-J-J-J'!!^zt-@$C2qF[qosp(?a\DzIO%/`+l\N]xT*GgW6dmekJp	i4'x	t4(x	t4 J~ICVX}em	4J)U;J;%	4sWk%%(@NU/AZWy^kj}-4#qAht7S!T:#tmXW~*E\46+`~/8e>	V<!Z8>;c1|||&~Uo\Is+^lW	9l:!v^et|{B7(RbG=g@.G?&}xU\]~5vAl\40tn8dvt:#H+7N'tTn5i	Q?~CRy{zWO;;~mkoed)rW6tdmfK?R{U\?j0x,m.+E,$C!-fNGf%9r	+l3:zd<)S
+Ys0/[FZT
&^n:JmHR-w21JEryJ?-( rnWz;j2?%G`MuXlF[+2DMoCq%-sRXXHpn'%61','5]+
E%X|uj$}FQ\A4__fHHF\G$GAf]-H[Mp Kgog[ vTk"^gE8p%kgv^}tV[fW~k
,8 A'%%@	?I	[uZ:Kw=Vr/],8XfAN2wxWqBD]	ng~m9?oPYr"G_/b.ya<NrU9}''NqKOKl9Rq<'` ^)W=j}LIKl-,mp)y9]C4vlc&2,E7K~'G97auDY
"r0Ga.Mw=WhZ6j-Z-y6R5IR-rsT+Z4	Fa,lMar9}?) d7TTdB@v_5z4R@io9x{rprM6e7H	uHoV\7CLyppM9|S6$_@.fN,J5_~3.iD\vO/2ox:bs!8y8+s1Y#{db`:TnZW}$N!OHUzpeCY<B1G75.ooJ]Z	d"0:nT
m<kX!7kIl?[CmG#/n<h11WcH3
\9uS)-W@GJ^5!++L-{l|
cO\7XGJi	YcJ+]3!Y>}'LYRth#6 %#Ba%K?Z+K\$XX*Xh~WL"^rc"l.A2{FO\7~\
Hm
X qpP{c6{SYi?MyQ
%K4wj{
PrCCg2jWW2 9rB2[e Z'KXN Ke)'g,h&ak2f)CTGg\23(z*1	3>x'9u5ILtXVgL1DH-b:wbvtt=w}2
scuytEyrV,H+%)*RZ#RYAEYI],;j`DE`m@*.Y=Jo?}U[o%l.w8QqBTOLZRtEk-6JQ$ICrWb~_>5h5/!\]
r\t.gd5K|QZ+QjZ*[9CSNz kP:
j(?QkA<M
_BX_p$#CM96U~OR|ZOB?rodS)Edj5Ug[>QIrzz|kT,jTo-x7zse^WgW06TBxC
D>}/_\lqZly	JXlh):).#,=X]*GA/qtd] RrR[GgqHc/5o}7gC=_WKkK-=Xz_K>~xG<w@vaWn{s'|Ng+w${s7K$M?@R$=?n,!X0@pdpN
V!>dX@=y6v.uLZR>DplHrIF%{\p6}adUx4%${Q`LzR4;w]Uf[_ryL!s?Dy|m*F6JK"13@c"`2}l$<u=>I(3A&6kaVzmqJ387A5vt hL`4'T?p/oY:C{_VGQvd/*$L
q'~zVs
&]v_0xYF)5Gfk*Ddca+F)afy^7IRA8"V'f<HIh8H:}'$v	hX.N #9}eDG@u*S8RK-Eiq)$(t(?g1]p
0`N8AtW@_F-8o)%/#5Q0\VzF$AiFtrcI`S5QJtR p3?5?ZJ&$#A8cKy-84q7)U`>*VxcD+ Tp!6 1x*jK64<.@ES5WFbb{S%l24y/MPq3xI< w S~VV=J@BA:W'9I{%9GTdoe~iXRs=H3Aw;':)H%N&2&j+ma4\wFAiiCTC&<.]Yds[?5
	(Bm)&a{AnnwXzo=yOvigkST$*LNhE'3$04{dll<
^\yQ\R8jXxaUHz
XfqmZo0E
+lYi0pO#M}q)bE`QCZOU,9<'>E"H(= I&5fsy)3#8L{(*dV{g\WPQVh &" u\!X[N0	3:VT6 TJny(2hn4T%S9%	W '#3XUb9DJcql!YZhm6Bd6JQ;q,;a	js/L/\ClF,Z!C$y0P*ch8Z<^NKo``A
G;DCxZ 0|N('?v>NqQ{z,DfCO]#'s
P$'^eJct-kRxZJeEe>'y6_dnB3SrF9*q`c*gQ)Ci\V\T422dR-KD"Hco_4Ec[X#YvTH>s	z8m)IxL W7^yG[(6gX26.*@X.=>!('Q1aEj'=W
lwk&wQX]Qjza">t	e(AQe EOVXA1%=ip>=1t.h4wauyQ8^X".@_?T! ]),X+>(HGK(.F/awd^PQ+|oF&T.{odEK0y]XYof%DT0Rbf$wp(9cW3[|$"	@d<@D04wiy_1&*Qb&w 871~s@IskPf/QgsL[k>/Cb}G8j!(]PXSXM%(n*	iIv(\8S9O T:s5&dNyu9Q -il;<CQ$ MM#Oj:	|`]tFct!xMq&!0	sh38ngNbX~`T

3Hu+;
S(OGZ'o^m2MRZT/vz!e35gJl:p^KVa9d)jE *b@L~MRp6%SpH+$6s4RR5o/	^ON^J8SSeJBl:"@3lg[O,rC"/D7e%|Q+X@bpYW\%X  z,@Z]X^,
#ONnXfs0'0yxr$2,${jN!,s0cV'>#bXfF'F~>_~r9%Nx0=XFD'Se:nD"\62|h-9k*>f]*N-tL%tB */V4aPBjUO&Pt("_pzD|N.;	EqAqBcdc> /d<b*=DM?iUbj	7hT#[@HW(+ x@QZJ =:tZ2X:*\=X\!ubhe3qQ	# 4(+s\R1f3\04.a @EuGxD 
|'6
&Iq{%Abf]{4+p"tGWtV<v$4O'rd7atxp/$*Zvjex6#Y8}(%rIG-C(f)_JX>h>'Ne	_h^rhsz*L`AhxN.(`dr]G5RJs8\I] 8j)(YBDkz<
H?'>,-pE,EduLp5
eO}Tsy2M*gpgL~/tMe i(WZ(LPY+Jcq3ltjw<R9$b-@_VYnNV<*da	SUtJZEK$t\3N]}~L&**
k.m]@z8?\:J)vp:ee\`rrei.Yef!zaY{z?k:FmGQb#&K:&\6[`wf8cwW,+2EH*E<|POIy47	6-I"y>3
),@0,F(cd"ADtph/DD)i)(XHH(j'r;&Jtn43Sz$*'RTPOuu8s,.r5_9P&DdZh|jIN&ZD0vnE;j	yQ91`!<He-\GqG\'|	&YIQx$
tR@252%FTx$ljg`FWZ2fg/lPwBag>U)}6pC1C^q<[Nf3"VO=	\q(8NA+_f7[9]xt&++ccA ">os]^h_@JaD3b!`HG aT+{Gr9dwx1	cZ"I{MuNTi
F~<fXx0sM1Rf}(H)QFcI:2'	 '"
1 o9=-: R _\s!	$OB 3P;[93	aA0\J=;m6Sj3Bkx)LX6KKg269lzM4!'CL&<:XV?a[8Yz[b8c=$}B,].ix?pJY<61wD$q4k/yod'b4 <|"qH<qG"O"
NbF\S'iwi_jO[vrs`Vv;{n=+o#_ld(np=__t'o/';pg5fn_w=_w;@w=?lZEsF>wlP+.b5Cvvax>TvM;[;	wHD=vXZ+C@WA+B*|sp W ]*OTa"+m$,n;RJ!Nn	w0~f6(eoWxrGT4jwo+O	%gDmLo3T:g!2_l7JLg
":7aP{:,jSbY8"lou}'QU0B	7i:5N hmh.jVw@;{ 6p3OHjR -.OJjozqnvv^GC"La)U\g/7s'mJ}G	h-ttLGfl|J%Gb&QO/\

G`|nQ};JMt<etDd?r7P&NGz$:+,6yJv2)1I`8?E'Zr-1(x`zgE{bZfl&<(cm?'THc>)[qGrirmlqb{)ihB[~$D$7{%`ggCj]1(Us,<eIJ.&(KcCQF"~_8J^tGOIF7%Qy"P	eqP-CH?dgX#gZ7h>%1q{5lH5P3s1E7~Ml1LyJ6W$%a{lUANPq.ANc~u{~Cbrvy3_qe>nkN9r	EHVPp^B6#+<(ypLE6wi| s4$Y'2'B_R9;Kd=q'A@9Fb{fez9?	FOYG05*Td*o(%U$AYx@<Qlq7xt~wcl.H1o4T#5O0\+}^J6	jI$XC`i'
RT,zi7@?b#`aK|V~6zQ%~&0pvd-QZDJS$%M$L,DG9 ]@z5s>s>s>s>s>s>s>s>3g  

Node-path: trunk/site/websec-1.8.0.tar.gz.sig
Node-kind: file
Node-action: add
Prop-content-length: 59
Text-content-length: 65
Text-content-md5: 7a9dd0dd298bba41a2e15f9da66dea7d
Content-length: 124

K 13
svn:mime-type
V 24
application/octet-stream
PROPS-END
? ATv&GE a8Ee>@dPAhB5 ~D{wevw

